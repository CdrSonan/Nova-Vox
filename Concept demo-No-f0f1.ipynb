{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "monetary-bulgaria",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jokla\\anaconda3\\lib\\site-packages\\torchaudio\\extension\\extension.py:13: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "torchaudio.set_audio_backend(\"soundfile\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "occasional-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioSample:\n",
    "    def __init__(self, filepath):\n",
    "        loadedData = torchaudio.load(filepath)\n",
    "        self.waveform = loadedData[0][0]\n",
    "        self.sampleRate = loadedData[1]\n",
    "        del loadedData\n",
    "        self.pitchDeltas = torch.tensor([], dtype = int)\n",
    "        self.pitchBorders = torch.tensor([], dtype = int)\n",
    "        self.pitch = torch.tensor([0], dtype = int)\n",
    "        self.spectra = torch.tensor([[]], dtype = float)\n",
    "        self.spectrum = torch.tensor([], dtype = float)\n",
    "        self.excitation = torch.tensor([], dtype = float)\n",
    "        self.voicedExcitation = torch.tensor([], dtype = float)\n",
    "        self.VoicedExcitations = torch.tensor([], dtype = float)\n",
    "        \n",
    "    def CalculatePitch(self, expectedPitch, searchRange = 0.2):\n",
    "        batchSize = math.floor((1. + searchRange) * self.sampleRate / expectedPitch)\n",
    "        lowerSearchLimit = math.floor((1. - searchRange) * self.sampleRate / expectedPitch)\n",
    "        batchStart = 0\n",
    "        while batchStart + batchSize <= self.waveform.size()[0] - batchSize:\n",
    "            sample = torch.index_select(self.waveform, 0, torch.linspace(batchStart, batchStart + batchSize, batchSize, dtype = int))\n",
    "            zeroTransitions = torch.tensor([], dtype = int)\n",
    "            for i in range(lowerSearchLimit, batchSize):\n",
    "                if (sample[i-1] < 0) and (sample[i] > 0):\n",
    "                    zeroTransitions = torch.cat([zeroTransitions, torch.tensor([i])], 0)\n",
    "            error = math.inf\n",
    "            delta = math.floor(self.sampleRate / expectedPitch)\n",
    "            for i in zeroTransitions:\n",
    "                shiftedSample = torch.index_select(self.waveform, 0, torch.linspace(batchStart + i.item(), batchStart + batchSize + i.item(), batchSize, dtype = int))\n",
    "                newError = torch.sum(torch.pow(sample - shiftedSample, 2))\n",
    "                if error > newError:\n",
    "                    delta = i.item()\n",
    "                    error = newError\n",
    "            self.pitchDeltas = torch.cat([self.pitchDeltas, torch.tensor([delta])])\n",
    "            batchStart += delta\n",
    "        nBatches = self.pitchDeltas.size()[0]\n",
    "        self.pitchBorders = torch.zeros(nBatches + 1, dtype = int)\n",
    "        for i in range(nBatches):\n",
    "            self.pitchBorders[i+1] = self.pitchBorders[i] + self.pitchDeltas[i]\n",
    "        self.pitch = torch.mean(self.pitchDeltas.float()).int()\n",
    "        del batchSize\n",
    "        del lowerSearchLimit\n",
    "        del batchStart\n",
    "        del sample\n",
    "        del zeroTransitions\n",
    "        del error\n",
    "        del delta\n",
    "        del shiftedSample\n",
    "        del newError\n",
    "        del nBatches\n",
    "        \n",
    "    def CalculateSpectra(self, iterations = 10, filterWidth = 10, preIterations = 2):\n",
    "        tripleBatchSize = int(self.sampleRate / 25)\n",
    "        BatchSize = int(self.sampleRate / 75)\n",
    "        Window = torch.hann_window(tripleBatchSize)\n",
    "        signals = torch.stft(self.waveform, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, return_complex = True, onesided = True)\n",
    "        signals = torch.transpose(signals, 0, 1)\n",
    "        signalsAbs = signals.abs()\n",
    "        \n",
    "        workingSpectra = torch.sqrt(signalsAbs)\n",
    "        \n",
    "        workingSpectra = torch.max(workingSpectra, torch.tensor([-100]))\n",
    "        self.spectra = torch.full_like(workingSpectra, -float(\"inf\"), dtype=torch.float)\n",
    "        \n",
    "        for j in range(preIterations):\n",
    "            workingSpectra = torch.max(workingSpectra, self.spectra)\n",
    "            self.spectra = workingSpectra\n",
    "            for i in range(filterWidth):\n",
    "                self.spectra = torch.roll(workingSpectra, -i, dims = 1) + self.spectra + torch.roll(workingSpectra, i, dims = 1)\n",
    "            self.spectra = self.spectra / (2 * filterWidth + 1)\n",
    "        \n",
    "        self.VoicedExcitations = torch.zeros_like(signals)\n",
    "        for i in range(signals.size()[0]):\n",
    "            for j in range(signals.size()[1]):\n",
    "                if torch.sqrt(signalsAbs[i][j]) > self.spectra[i][j]:\n",
    "                    self.VoicedExcitations[i][j] = signals[i][j]\n",
    "                \n",
    "        for j in range(iterations):\n",
    "            workingSpectra = torch.max(workingSpectra, self.spectra)\n",
    "            self.spectra = workingSpectra\n",
    "            for i in range(filterWidth):\n",
    "                self.spectra = torch.roll(workingSpectra, -i, dims = 1) + self.spectra + torch.roll(workingSpectra, i, dims = 1)\n",
    "            self.spectra = self.spectra / (2 * filterWidth + 1)\n",
    "        \n",
    "        self.spectrum = torch.mean(self.spectra, 0)\n",
    "        for i in range(self.spectra.size()[0]):\n",
    "            self.spectra[i] = self.spectra[i] - self.spectrum\n",
    "        #return torch.log(signalsAbs)\n",
    "        del Window\n",
    "        del signals\n",
    "        del workingSpectra\n",
    "        \n",
    "    def CalculateExcitation(self, filterWidth = 10):\n",
    "        tripleBatchSize = int(self.sampleRate / 25)\n",
    "        BatchSize = int(self.sampleRate / 75)\n",
    "        Window = torch.hann_window(tripleBatchSize)\n",
    "        signals = torch.stft(self.waveform, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, return_complex = True, onesided = True)\n",
    "        signals = torch.transpose(signals, 0, 1)\n",
    "        excitations = torch.empty_like(signals)\n",
    "        for i in range(excitations.size()[0]):\n",
    "            excitations[i] = signals[i] / torch.square(self.spectrum + self.spectra[i])\n",
    "            self.VoicedExcitations[i] = self.VoicedExcitations[i] / torch.square(self.spectrum + self.spectra[i])\n",
    "        \n",
    "        VoicedExcitations = torch.transpose(self.VoicedExcitations, 0, 1)\n",
    "        excitations = torch.transpose(excitations, 0, 1)\n",
    "        self.excitation = torch.istft(excitations, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, onesided = True)\n",
    "        self.voicedExcitation = torch.istft(VoicedExcitations, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, onesided = True)\n",
    "        \n",
    "        self.excitation = self.excitation - self.voicedExcitation\n",
    "        self.excitation = torch.stft(self.excitation, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, return_complex = True, onesided = True)\n",
    "        self.voicedExcitation = torch.stft(self.voicedExcitation, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, return_complex = True, onesided = True)\n",
    "        self.excitation = torch.transpose(self.excitation, 0, 1)\n",
    "        #self.voicedExcitation = torch.transpose(self.voicedExcitation, 0, 1)\n",
    "        \n",
    "        del Window\n",
    "        del signals\n",
    "        del excitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "expired-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(RelLoss, self).__init__()\n",
    " \n",
    "    def forward(self, inputs, targets):    \n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        differences = torch.abs(inputs - targets)\n",
    "        sums = torch.abs(inputs + targets)\n",
    "        out = (differences / sums).sum() / inputs.size()[0]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "configured-craps",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecCrfAi(nn.Module):\n",
    "    def __init__(self, learningRate=1e-4):\n",
    "        super(SpecCrfAi, self).__init__()\n",
    "        \n",
    "        self.layer1 = torch.nn.Linear(3843, 3843)\n",
    "        self.ReLu1 = nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(3843, 5763)\n",
    "        self.ReLu2 = nn.ReLU()\n",
    "        self.layer3 = torch.nn.Linear(5763, 3842)\n",
    "        self.ReLu3 = nn.ReLU()\n",
    "        self.layer4 = torch.nn.Linear(3842, 1921)\n",
    "        \n",
    "        self.learningRate = learningRate\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learningRate, weight_decay=0.)\n",
    "        self.criterion = nn.L1Loss()\n",
    "        #self.criterion = RelLoss()\n",
    "        \n",
    "    def forward(self, spectrum1, spectrum2, factor):\n",
    "        fac = torch.tensor([factor])\n",
    "        x = torch.cat((spectrum1, spectrum2, fac), dim = 0)\n",
    "        x = x.float()#.unsqueeze(0).unsqueeze(0)\n",
    "        x = self.layer1(x)\n",
    "        x = self.ReLu1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.ReLu2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.ReLu3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "    \n",
    "    def processData(self, spectrum1, spectrum2, factor):\n",
    "        output = torch.square(torch.squeeze(self(torch.sqrt(spectrum1), torch.sqrt(spectrum2), factor)))\n",
    "        return output\n",
    "    \n",
    "    def train(self, indata, epochs=1):\n",
    "        for epoch in range(epochs):\n",
    "            for data in self.dataLoader(indata):\n",
    "                spectrum1 = data[0]\n",
    "                spectrum2 = data[-1]\n",
    "                indexList = np.arange(0, data.size()[0], 1)\n",
    "                np.random.shuffle(indexList)\n",
    "                for i in indexList:\n",
    "                    factor = i / float(data.size()[0])\n",
    "                    spectrumTarget = data[i]\n",
    "                    output = torch.squeeze(self(spectrum1, spectrum2, factor))\n",
    "                    loss = self.criterion(output, spectrumTarget)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "            print('epoch [{}/{}], loss:{:.4f}'\n",
    "                  .format(epoch + 1, epochs, loss.data))\n",
    "            \n",
    "    def dataLoader(self, data):\n",
    "        return torch.utils.data.DataLoader(dataset=data, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "black-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocalSegment:\n",
    "    def __init__(self, start1, start2, start3, end1, end2, end3, startCap, endCap, phonemeKey, vb, offset, repetititionSpacing, pitch, steadiness):\n",
    "        self.start1 = start1\n",
    "        self.start2 = start2\n",
    "        self.start3 = start3\n",
    "        self.end1 = end1\n",
    "        self.end2 = end2\n",
    "        self.end3 = end3\n",
    "        self.startCap = startCap\n",
    "        self.endCap = endCap\n",
    "        self.phonemeKey = phonemeKey\n",
    "        self.vb = vb\n",
    "        self.offset = offset\n",
    "        self.repetititionSpacing = repetititionSpacing\n",
    "        self.pitch = pitch\n",
    "        self.steadiness = steadiness\n",
    "        \n",
    "    def phaseShift(self, inputTensor, pitch, phase):\n",
    "        absolutes = inputTensor.abs()\n",
    "        phases = inputTensor.angle()\n",
    "        phaseOffsets = torch.full(phases.size(), phase / pitch)\n",
    "        phaseOffsets *= torch.unsqueeze(torch.arange(phases.size()[0]), 1)\n",
    "        phases += phaseOffsets\n",
    "        phases = torch.fmod(phases, 2 * math.pi)\n",
    "        return torch.polar(absolutes, phases)\n",
    "        \n",
    "    def loopSamplerVoicedExcitation(self, inputTensor, targetSize, repetititionSpacing, pitch):\n",
    "        batchRS = repetititionSpacing\n",
    "        BatchSize = int(self.vb.sampleRate / 75)\n",
    "        tripleBatchSize = int(self.vb.sampleRate / 25)\n",
    "        repetititionSpacing *= BatchSize\n",
    "        window = torch.hann_window(tripleBatchSize)\n",
    "        alignPhase = inputTensor[batchRS][pitch].angle()\n",
    "        finalPhase = inputTensor[1][pitch].angle()\n",
    "        phaseDiff = finalPhase - alignPhase\n",
    "        requiredTensors = math.ceil((targetSize/BatchSize + batchRS) / inputTensor.size()[1])\n",
    "        \n",
    "        if requiredTensors == 1:\n",
    "            outputTensor = inputTensor.clone()\n",
    "            outputTensor = torch.istft(outputTensor, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = window, onesided = True, length = inputTensor.size()[1]*BatchSize)\n",
    "        else:\n",
    "            outputTensor = torch.zeros(requiredTensors * (inputTensor.size()[1] * BatchSize - repetititionSpacing) + repetititionSpacing)\n",
    "            \n",
    "            workingTensor = inputTensor.clone()\n",
    "            workingTensor = torch.istft(workingTensor, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = window, onesided = True, length = inputTensor.size()[1]*BatchSize)\n",
    "            workingTensor[-repetititionSpacing:] *= torch.linspace(1, 0, repetititionSpacing)\n",
    "            outputTensor[0:inputTensor.size()[1] * BatchSize] += workingTensor\n",
    "            \n",
    "            for i in range(1, requiredTensors - 1):\n",
    "                workingTensor = inputTensor.clone()\n",
    "                workingTensor = self.phaseShift(workingTensor, pitch, i * phaseDiff)\n",
    "                workingTensor = torch.istft(workingTensor, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = window, onesided = True, length = inputTensor.size()[1]*BatchSize)\n",
    "                workingTensor[0:repetititionSpacing] *= torch.linspace(0, 1, repetititionSpacing)\n",
    "                workingTensor[-repetititionSpacing:] *= torch.linspace(1, 0, repetititionSpacing)\n",
    "                outputTensor[i * (inputTensor.size()[1] * BatchSize - repetititionSpacing):i * (inputTensor.size()[1] * BatchSize - repetititionSpacing) + inputTensor.size()[1] * BatchSize] += workingTensor\n",
    "            \n",
    "            workingTensor = inputTensor.clone()\n",
    "            workingTensor = self.phaseShift(workingTensor, pitch, (requiredTensors - 1) * phaseDiff)\n",
    "            workingTensor = torch.istft(workingTensor, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = window, onesided = True, length = inputTensor.size()[1]*BatchSize)\n",
    "            workingTensor[0:repetititionSpacing] *= torch.linspace(0, 1, repetititionSpacing)\n",
    "            outputTensor[(requiredTensors - 1) * (inputTensor.size()[1] * BatchSize - repetititionSpacing):] += workingTensor\n",
    "        return outputTensor[0:targetSize * BatchSize]\n",
    "    \n",
    "    def loopSamplerSpectrum(self, inputTensor, targetSize, repetititionSpacing):\n",
    "        #repetititionSpacing = math.ceil(repetititionSpacing / int(self.vb.sampleRate / 75))\n",
    "        requiredTensors = math.ceil((targetSize + repetititionSpacing) / inputTensor.size()[0])\n",
    "        if requiredTensors == 1:\n",
    "            outputTensor = inputTensor.clone()\n",
    "        else:\n",
    "            outputTensor = torch.zeros(requiredTensors * (inputTensor.size()[0] - repetititionSpacing) + repetititionSpacing, inputTensor.size()[1])\n",
    "            \n",
    "            workingTensor = inputTensor.clone()\n",
    "            workingTensor[-repetititionSpacing:] *= torch.unsqueeze(torch.linspace(1, 0, repetititionSpacing), 1)\n",
    "            outputTensor[0:inputTensor.size()[0]] += workingTensor\n",
    "            \n",
    "            for i in range(1, requiredTensors - 1):\n",
    "                workingTensor = inputTensor.clone()\n",
    "                workingTensor[0:repetititionSpacing] *= torch.unsqueeze(torch.linspace(0, 1, repetititionSpacing), 1)\n",
    "                workingTensor[-repetititionSpacing:] *= torch.unsqueeze(torch.linspace(1, 0, repetititionSpacing), 1)\n",
    "                outputTensor[i * (inputTensor.size()[0] - repetititionSpacing):i * (inputTensor.size()[0] - repetititionSpacing) + inputTensor.size()[0]] += workingTensor\n",
    "            \n",
    "            workingTensor = inputTensor.clone()\n",
    "            workingTensor[0:repetititionSpacing] *= torch.unsqueeze(torch.linspace(0, 1, repetititionSpacing), 1)\n",
    "            outputTensor[(requiredTensors - 1) * (inputTensor.size()[0] - repetititionSpacing):] += workingTensor\n",
    "        return outputTensor[0:targetSize]\n",
    "    \n",
    "    def getSpectrum(self):\n",
    "        if self.startCap:\n",
    "            windowStart = self.offset\n",
    "        else:\n",
    "            windowStart = self.start3 - self.start1 + self.offset\n",
    "        if self.endCap:\n",
    "            windowEnd = self.end3 - self.start1 + self.offset\n",
    "        else:\n",
    "            windowEnd = self.end1 - self.start1 + self.offset\n",
    "        spectrum =  self.vb.phonemeDict[self.phonemeKey].spectrum#implement looping\n",
    "        #spectra =  self.vb.phonemeDict[self.phonemeKey].spectra[windowStart:windowEnd]\n",
    "        spectra = self.loopSamplerSpectrum(self.vb.phonemeDict[self.phonemeKey].spectra, windowEnd, self.repetititionSpacing)[windowStart:windowEnd]\n",
    "        return torch.square(spectrum + (math.pow(1 - self.steadiness, 2) * spectra))\n",
    "    \n",
    "    def getExcitation(self):\n",
    "        BatchSize = int(self.vb.sampleRate / 75)\n",
    "        tripleBatchSize = int(self.vb.sampleRate / 25)\n",
    "        premul = self.vb.phonemeDict[self.phonemeKey].excitation.size()[0] / (self.end3 - self.start1 + 1)\n",
    "        #premul = 1\n",
    "        if self.startCap:\n",
    "            windowStart = 0\n",
    "            length = -self.start1\n",
    "        else:\n",
    "            windowStart = math.floor((self.start2 - self.start1) * premul)\n",
    "            length = -self.start2\n",
    "        if self.endCap:\n",
    "            windowEnd = math.ceil((self.end3 - self.start1) * premul)\n",
    "            length += self.end3\n",
    "        else:\n",
    "            windowEnd = math.ceil((self.end2 - self.start1) * premul)\n",
    "            length += self.end2\n",
    "        excitation = self.vb.phonemeDict[self.phonemeKey].excitation[windowStart:windowEnd]\n",
    "        excitation = torch.transpose(excitation, 0, 1)\n",
    "        transform = torchaudio.transforms.TimeStretch(hop_length = int(self.vb.sampleRate / 75),\n",
    "                                                      n_freq = int(self.vb.sampleRate / 25 / 2) + 1, \n",
    "                                                      fixed_rate = premul)\n",
    "        excitation = transform(torch.view_as_real(excitation))\n",
    "        excitation = torch.view_as_complex(excitation)\n",
    "        window = torch.hann_window(int(self.vb.sampleRate / 25))\n",
    "        excitation = torch.istft(excitation, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = window, onesided = True, length = length*int(self.vb.sampleRate / 75))\n",
    "        \n",
    "        return excitation[0:length*int(self.vb.sampleRate / 75)]\n",
    "    \n",
    "    def getVoicedExcitation(self):\n",
    "        BatchSize = int(self.vb.sampleRate / 75)\n",
    "        nativePitch = self.vb.phonemeDict[self.phonemeKey].pitch\n",
    "        #nativePitch = self.vb.phonemeDict[self.phonemeKey].pitches[...]\n",
    "        #pitch = nativePitch + self.vb.phonemeDict[phonemeKey].pitches...\n",
    "        premul = self.pitch / nativePitch * self.vb.sampleRate / 75\n",
    "        windowStart = math.floor(self.offset * self.vb.sampleRate / 75)\n",
    "        windowEnd = math.ceil((self.end3 - self.start1) * premul + (self.offset * self.vb.sampleRate / 75))\n",
    "        #windowStart = math.floor(self.offset)\n",
    "        #windowEnd = math.ceil((self.end3 - self.start1) * premul + self.offset)\n",
    "        #voicedExcitation = self.vb.phonemeDict[self.phonemeKey].voicedExcitation[windowStart:windowEnd]\n",
    "        voicedExcitation = self.loopSamplerVoicedExcitation(self.vb.phonemeDict[self.phonemeKey].voicedExcitation, windowEnd, self.repetititionSpacing, math.ceil(nativePitch / 75.))[windowStart:windowEnd]\n",
    "        transform = torchaudio.transforms.Resample(orig_freq = nativePitch,\n",
    "                                                   new_freq = self.pitch,\n",
    "                                                   resampling_method = 'sinc_interpolation')\n",
    "        voicedExcitation = transform(voicedExcitation)\n",
    "        if self.startCap == False:\n",
    "            slope = torch.linspace(0, 1, (self.start3 - self.start1) * int(self.vb.sampleRate / 75))\n",
    "            voicedExcitation[0:(self.start3 - self.start1) * int(self.vb.sampleRate / 75)] *= slope\n",
    "        if self.endCap == False:\n",
    "            slope = torch.linspace(1, 0, (self.end3 - self.end1) * int(self.vb.sampleRate / 75))\n",
    "            voicedExcitation[(self.end1 - self.start1) * int(self.vb.sampleRate / 75):(self.end3 - self.start1) * int(self.vb.sampleRate / 75)] *= slope\n",
    "        return voicedExcitation[0:(self.end3 - self.start1) * int(self.vb.sampleRate / 75)]\n",
    "        #resample segments\n",
    "        #individual fourier transform\n",
    "        #istft\n",
    "        #windowing adaptive to borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "emotional-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocalSequence:\n",
    "    def __init__(self, start, end, vb, borders, phonemes, offsets):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.vb = vb\n",
    "        self.synth = Synthesizer(self.vb.sampleRate)\n",
    "        \n",
    "        self.spectrum = torch.zeros((self.end - self.start, int(self.vb.sampleRate / 25 / 2) + 1))\n",
    "        self.excitation = torch.zeros((self.end - self.start) * int(self.vb.sampleRate / 75))\n",
    "        self.voicedExcitation = torch.zeros((self.end - self.start) * int(self.vb.sampleRate / 75))\n",
    "        \n",
    "        self.segments = []\n",
    "        if len(phonemes)== 1:#rewrite border system to use tensor, implement pitch and steadiness\n",
    "            self.segments.append(VocalSegment(borders[0], borders[1], borders[2], borders[3], borders[4], borders[5],\n",
    "                                             True, True, phonemes[0], vb, offsets[0], 12, 386, 0))\n",
    "        else:\n",
    "            self.segments.append(VocalSegment(borders[0], borders[1], borders[2], borders[3], borders[4], borders[5],\n",
    "                                             True, False, phonemes[0], vb, offsets[0], 12, 386, 0))\n",
    "            for i in range(1, len(phonemes)-1):\n",
    "                self.segments.append(VocalSegment(borders[3*i], borders[3*i+1], borders[3*i+2], borders[3*i+3], borders[3*i+4], borders[3*i+5],\n",
    "                                                  False, False, phonemes[i], vb, offsets[i], 12, 386, 0))\n",
    "            endpoint = len(phonemes)-1\n",
    "            self.segments.append(VocalSegment(borders[3*endpoint], borders[3*endpoint+1], borders[3*endpoint+2], borders[3*endpoint+3], borders[3*endpoint+4], borders[3*endpoint+5],\n",
    "                                             False, True, phonemes[-1], vb, offsets[-1], 12, 386, 0))\n",
    "\n",
    "        self.requiresUpdate = np.ones(len(phonemes))\n",
    "        self.update()\n",
    "    def update(self):\n",
    "        for i in range(self.requiresUpdate.size):\n",
    "            if self.requiresUpdate[i] == 1:\n",
    "                print(i)\n",
    "                segment = self.segments[i]\n",
    "                spectrum = torch.zeros((segment.end3 - segment.start1, int(self.vb.sampleRate / 25 / 2) + 1))\n",
    "                excitation = torch.zeros((segment.end3 - segment.start1) * int(self.vb.sampleRate / 75))\n",
    "                voicedExcitation = torch.zeros((segment.end3 - segment.start1) * int(self.vb.sampleRate / 75))\n",
    "                if segment.startCap:\n",
    "                    windowStart = 0\n",
    "                else:\n",
    "                    windowStart = segment.start3 - segment.start1\n",
    "                    previousSpectrum = self.segments[i-1].getSpectrum()[-1]\n",
    "                    previousVoicedExcitation = self.segments[i-1].getVoicedExcitation()[(self.segments[i-1].end1-self.segments[i-1].end3)*int(self.vb.sampleRate/75):]\n",
    "                if segment.endCap:\n",
    "                    windowEnd = segment.end3 - segment.start1\n",
    "                else:\n",
    "                    windowEnd = segment.end1 - segment.start1\n",
    "                    nextSpectrum = self.segments[i+1].getSpectrum()[0]\n",
    "                    nextVoicedExcitation = self.segments[i+1].getVoicedExcitation()[0:(self.segments[i+1].start3-self.segments[i+1].start1)*int(self.vb.sampleRate/75)]\n",
    "                \n",
    "                spectrum[windowStart:windowEnd] = segment.getSpectrum()\n",
    "                voicedExcitation = segment.getVoicedExcitation()\n",
    "                if segment.startCap == False:\n",
    "                    for j in range(segment.start3 - segment.start1):\n",
    "                        spectrum[j] = self.vb.crfAi.processData(previousSpectrum, spectrum[windowStart], j / (segment.start3 - segment.start1))\n",
    "                    voicedExcitation[0:(segment.start3-segment.start1)*int(self.vb.sampleRate/75)] += previousVoicedExcitation\n",
    "                if segment.endCap == False:\n",
    "                    for j in range(segment.end1 - segment.start1, segment.end3 - segment.start1):\n",
    "                        spectrum[j] = self.vb.crfAi.processData(spectrum[windowEnd], nextSpectrum, (j - segment.start1) / (segment.end3 - segment.end1))\n",
    "                    voicedExcitation[(segment.end1-segment.end3)*int(self.vb.sampleRate/75):] += nextVoicedExcitation\n",
    "                if segment.startCap:\n",
    "                    windowStart = 0\n",
    "                else:\n",
    "                    windowStart = (segment.start2 - segment.start1) * int(self.vb.sampleRate / 75)\n",
    "                    previousExcitation = self.segments[i-1].getExcitation()[(segment.start1-segment.start2)*int(self.vb.sampleRate/75):]\n",
    "                    excitation[0:windowStart] = previousExcitation\n",
    "                if segment.endCap:\n",
    "                    windowEnd = (segment.end3 - segment.start1) * int(self.vb.sampleRate / 75)\n",
    "                else:\n",
    "                    windowEnd = (segment.end2 - segment.start1) * int(self.vb.sampleRate / 75)\n",
    "                    nextExcitation = self.segments[i+1].getExcitation()[0:(segment.end3-segment.end2)*int(self.vb.sampleRate/75)]\n",
    "                    excitation[windowEnd:] = nextExcitation\n",
    "                excitation[windowStart:windowEnd] = segment.getExcitation()\n",
    "                \n",
    "                self.spectrum[segment.start1:segment.end3] = spectrum\n",
    "                self.excitation[segment.start1*int(self.vb.sampleRate/75):segment.end3*int(self.vb.sampleRate/75)] = excitation\n",
    "                self.voicedExcitation[segment.start1*int(self.vb.sampleRate/75):segment.end3*int(self.vb.sampleRate/75)] = voicedExcitation\n",
    "                \n",
    "                skipPrevious = True#implement skipPrevious\n",
    "            else:\n",
    "                skipPrevious = False\n",
    "            \n",
    "        self.synth.Synthesize(0, self.spectrum, self.excitation, self.voicedExcitation)\n",
    "    def save(self):\n",
    "        self.synth.save(\"Output_Demo.wav\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "valid-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TempVB:\n",
    "    def __init__(self):\n",
    "        self.sampleRate = 48000\n",
    "        self.phonemeDict = dict([])\n",
    "        phonemeKeys = [\"A\", \"E\", \"I\", \"O\", \"U\", \"G\", \"K\", \"N\", \"S\", \"T\"]\n",
    "        for key in phonemeKeys:\n",
    "            self.phonemeDict[key] = AudioSample(\"Samples_rip/\"+key+\".wav\")\n",
    "            self.sampleRate = self.phonemeDict[key].sampleRate\n",
    "            self.phonemeDict[key].CalculatePitch(249.)\n",
    "            self.phonemeDict[key].CalculateSpectra(iterations = 15)\n",
    "            self.phonemeDict[key].CalculateExcitation()\n",
    "        self.crfAi = SpecCrfAi(learningRate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "expected-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Voicebank:\n",
    "    def __init__(self, vbKey):\n",
    "        self.phonemeDict = dict()\n",
    "        loaded_weights = 0#(vbKey)\n",
    "        self.crfAi = 0#SpecCrfAi(loaded_weights)\n",
    "        #load additional parameters\n",
    "        self.sampleRate = 48000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "polished-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Synthesizer:\n",
    "    def __init__(self, sampleRate):\n",
    "        self.sampleRate = sampleRate\n",
    "        self.returnSignal = torch.tensor([], dtype = float)\n",
    "        \n",
    "    def Synthesize(self, steadiness, spectrum, Excitation, VoicedExcitation):\n",
    "        tripleBatchSize = int(self.sampleRate / 25)\n",
    "        BatchSize = int(self.sampleRate / 75)\n",
    "        Window = torch.hann_window(tripleBatchSize)\n",
    "        \n",
    "        #HERE + VoicedExcitation\n",
    "        \n",
    "        self.returnSignal = torch.stft(Excitation + VoicedExcitation , tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, return_complex = True, onesided = True)\n",
    "        self.returnSignal = torch.transpose(self.returnSignal, 0, 1)[0:-1]\n",
    "        self.returnSignal = self.returnSignal * spectrum\n",
    "        self.returnSignal = torch.transpose(self.returnSignal, 0, 1)\n",
    "        self.returnSignal = torch.istft(self.returnSignal, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, onesided=True)\n",
    "        del Window\n",
    "        \n",
    "    def save(self, filepath):\n",
    "        torchaudio.save(filepath, torch.unsqueeze(self.returnSignal.detach(), 0), self.sampleRate, format=\"wav\", encoding=\"PCM_S\", bits_per_sample=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "latin-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingSamples = dict([])\n",
    "TrainingKeys = [\"A_E\", \"A_G\", \"A_I\", \"A_K\", \"A_N\", \"A_O\", \"A_S\", \"A_T\", \"A_U\",\n",
    "                   \"E_A\", \"E_G\", \"E_I\", \"E_K\", \"E_N\", \"E_O\", \"E_S\", \"E_T\", \"E_U\",\n",
    "                   \"I_A\", \"I_E\", \"I_G\", \"I_K\", \"I_N\", \"I_O\", \"I_S\", \"I_T\", \"I_U\",\n",
    "                   \"O_A\", \"O_E\", \"O_G\", \"O_I\", \"O_K\", \"O_N\", \"O_S\", \"O_T\", \"O_U\",\n",
    "                   \"U_A\", \"U_E\", \"U_G\", \"U_I\", \"U_K\", \"U_N\", \"U_O\", \"U_S\", \"U_T\",\n",
    "                   \"G_A\", \"G_E\", \"G_I\", \"G_O\", \"G_U\",\n",
    "                   \"K_A\", \"K_E\", \"K_I\", \"K_O\", \"K_U\",\n",
    "                   \"N_A\", \"N_E\", \"N_I\", \"N_O\", \"N_U\",\n",
    "                   \"S_A\", \"S_E\", \"S_I\", \"S_O\", \"S_U\",\n",
    "                   \"T_A\", \"T_E\", \"T_I\", \"T_O\", \"T_U\"\n",
    "                  ]\n",
    "\n",
    "for key in TrainingKeys:\n",
    "    TrainingSamples[key] = AudioSample(\"Samples_rip/\"+key+\".wav\")\n",
    "    TrainingSamples[key].CalculatePitch(249.)\n",
    "    TrainingSamples[key].CalculateSpectra(iterations = 25)\n",
    "    TrainingSamples[key].CalculateExcitation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "former-economics",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/1], loss:0.6301\n",
      "epoch [1/1], loss:0.2667\n",
      "epoch [1/1], loss:0.2157\n",
      "epoch [1/1], loss:0.2265\n",
      "epoch [1/1], loss:0.3138\n",
      "epoch [1/1], loss:0.1329\n",
      "epoch [1/1], loss:0.2458\n",
      "epoch [1/1], loss:0.1141\n",
      "epoch [1/1], loss:0.1104\n",
      "epoch [1/1], loss:0.1651\n",
      "epoch [1/1], loss:0.1013\n",
      "epoch [1/1], loss:0.1597\n",
      "epoch [1/1], loss:0.1619\n",
      "epoch [1/1], loss:0.0646\n",
      "epoch [1/1], loss:0.1048\n",
      "epoch [1/1], loss:0.2248\n",
      "epoch [1/1], loss:0.1683\n",
      "epoch [1/1], loss:0.0777\n",
      "epoch [1/1], loss:0.1732\n",
      "epoch [1/1], loss:0.1719\n",
      "epoch [1/1], loss:0.0566\n",
      "epoch [1/1], loss:0.0881\n",
      "epoch [1/1], loss:0.0701\n",
      "epoch [1/1], loss:0.0922\n",
      "epoch [1/1], loss:0.2805\n",
      "epoch [1/1], loss:0.1003\n",
      "epoch [1/1], loss:0.0610\n",
      "epoch [1/1], loss:0.3569\n",
      "epoch [1/1], loss:0.1372\n",
      "epoch [1/1], loss:0.2070\n",
      "epoch [1/1], loss:0.1401\n",
      "epoch [1/1], loss:0.0126\n",
      "epoch [1/1], loss:0.0954\n",
      "epoch [1/1], loss:0.1104\n",
      "epoch [1/1], loss:0.0186\n",
      "epoch [1/1], loss:0.1260\n",
      "epoch [1/1], loss:0.0870\n",
      "epoch [1/1], loss:0.2044\n",
      "epoch [1/1], loss:0.0682\n",
      "epoch [1/1], loss:0.0996\n",
      "epoch [1/1], loss:0.0767\n",
      "epoch [1/1], loss:0.0718\n",
      "epoch [1/1], loss:0.0856\n",
      "epoch [1/1], loss:0.1553\n",
      "epoch [1/1], loss:0.0932\n",
      "epoch [1/1], loss:0.2900\n",
      "epoch [1/1], loss:0.1524\n",
      "epoch [1/1], loss:0.2005\n",
      "epoch [1/1], loss:0.2081\n",
      "epoch [1/1], loss:0.1764\n",
      "epoch [1/1], loss:0.1877\n",
      "epoch [1/1], loss:0.2019\n",
      "epoch [1/1], loss:0.1742\n",
      "epoch [1/1], loss:0.1962\n",
      "epoch [1/1], loss:0.1820\n",
      "epoch [1/1], loss:0.0872\n",
      "epoch [1/1], loss:0.1029\n",
      "epoch [1/1], loss:0.1278\n",
      "epoch [1/1], loss:0.0663\n",
      "epoch [1/1], loss:0.0617\n",
      "epoch [1/1], loss:0.3642\n",
      "epoch [1/1], loss:0.1418\n",
      "epoch [1/1], loss:0.1702\n",
      "epoch [1/1], loss:0.2004\n",
      "epoch [1/1], loss:0.2401\n",
      "epoch [1/1], loss:0.2342\n",
      "epoch [1/1], loss:0.1916\n",
      "epoch [1/1], loss:0.1338\n",
      "epoch [1/1], loss:0.1214\n",
      "epoch [1/1], loss:0.1166\n"
     ]
    }
   ],
   "source": [
    "trainSpectra = []\n",
    "i = 0\n",
    "for key in TrainingKeys:\n",
    "    trainSpectra.append(torch.empty_like(TrainingSamples[key].spectra))\n",
    "    for j in range(TrainingSamples[key].spectra.size()[0]):\n",
    "        trainSpectra[i][j] = TrainingSamples[key].spectrum + TrainingSamples[key].spectra[j]\n",
    "    i += 1\n",
    "\n",
    "vb = TempVB()\n",
    "for i in range(70):\n",
    "    vb.crfAi.train(trainSpectra[i], epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "directed-nirvana",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor(2.3690) tensor([2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690,\n",
      "        2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690,\n",
      "        2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690,\n",
      "        2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690,\n",
      "        2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690,\n",
      "        2.3690, 2.3690, 2.3690])\n",
      "1\n",
      "tensor(3.2876) tensor([3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876])\n",
      "tensor(2.3690) tensor([2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690,\n",
      "        2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690,\n",
      "        2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690,\n",
      "        2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690,\n",
      "        2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690,\n",
      "        2.3690, 2.3690, 2.3690])\n",
      "2\n",
      "tensor(2.3690) tensor([2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690,\n",
      "        2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690,\n",
      "        2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690,\n",
      "        2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690,\n",
      "        2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690, 2.3690,\n",
      "        2.3690, 2.3690, 2.3690])\n",
      "tensor(3.3008) tensor([3.3008, 3.3008, 3.3008, 3.3008, 3.3008, 3.3008, 3.3008, 3.3008, 3.3008,\n",
      "        3.3008, 3.3008, 3.3008, 3.3008])\n",
      "tensor(3.2876) tensor([3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876])\n",
      "3\n",
      "tensor(3.2876) tensor([3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876])\n",
      "tensor(3.2876) tensor([3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876])\n",
      "tensor(6.5752) tensor([6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752,\n",
      "        6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752,\n",
      "        6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752,\n",
      "        6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752,\n",
      "        6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752,\n",
      "        6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752,\n",
      "        6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752])\n",
      "tensor(9.8629) tensor([9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629,\n",
      "        9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629,\n",
      "        9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629,\n",
      "        9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629,\n",
      "        9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629,\n",
      "        9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629,\n",
      "        9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629])\n",
      "tensor(13.1505) tensor([13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505,\n",
      "        13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505,\n",
      "        13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505,\n",
      "        13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505,\n",
      "        13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505,\n",
      "        13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505,\n",
      "        13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505,\n",
      "        13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505])\n",
      "tensor(3.3008) tensor([3.3008, 3.3008, 3.3008, 3.3008, 3.3008, 3.3008, 3.3008, 3.3008, 3.3008,\n",
      "        3.3008, 3.3008, 3.3008, 3.3008])\n",
      "4\n",
      "tensor(3.3008) tensor([3.3008, 3.3008, 3.3008, 3.3008, 3.3008, 3.3008, 3.3008, 3.3008, 3.3008,\n",
      "        3.3008, 3.3008, 3.3008, 3.3008])\n",
      "tensor(3.2876) tensor([3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876,\n",
      "        3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876, 3.2876])\n",
      "tensor(6.5752) tensor([6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752,\n",
      "        6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752,\n",
      "        6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752,\n",
      "        6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752,\n",
      "        6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752,\n",
      "        6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752,\n",
      "        6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752, 6.5752])\n",
      "tensor(9.8629) tensor([9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629,\n",
      "        9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629,\n",
      "        9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629,\n",
      "        9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629,\n",
      "        9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629,\n",
      "        9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629,\n",
      "        9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629, 9.8629])\n",
      "tensor(13.1505) tensor([13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505,\n",
      "        13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505,\n",
      "        13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505,\n",
      "        13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505,\n",
      "        13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505,\n",
      "        13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505,\n",
      "        13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505,\n",
      "        13.1505, 13.1505, 13.1505, 13.1505, 13.1505, 13.1505])\n"
     ]
    }
   ],
   "source": [
    "borders = [0, 1, 2,\n",
    "           35, 36, 37,\n",
    "           40, 51, 52,\n",
    "           75, 76, 79,\n",
    "           82, 83, 86,\n",
    "           328,329, 330\n",
    "          ]\n",
    "phonemes = [\"A\", \"N\", \"A\", \"T\", \"A\"]\n",
    "#offsets = [0, 5, 1, 1, 1]\n",
    "offsets = [0, 20, 20, 0, 13]\n",
    "\n",
    "sequence = VocalSequence(0, 400, vb, borders, phonemes, offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "velvet-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "varied-egyptian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1800b485490>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(sequence.synth.returnSignal.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "residential-crime",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(vb.crfAi.state_dict(), \"CrossfadeWeights.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "divine-middle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model = TheModelClass(*args, **kwargs)\\nmodel.load_state_dict(torch.load(PATH))\\nmodel.eval()'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"model = TheModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

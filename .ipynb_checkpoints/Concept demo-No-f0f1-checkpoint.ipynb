{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "monetary-bulgaria",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jokla\\anaconda3\\lib\\site-packages\\torchaudio\\extension\\extension.py:13: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "torchaudio.set_audio_backend(\"soundfile\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "occasional-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioSample:\n",
    "    def __init__(self, filepath):\n",
    "        loadedData = torchaudio.load(filepath)\n",
    "        self.waveform = loadedData[0][0]\n",
    "        self.sampleRate = loadedData[1]\n",
    "        del loadedData\n",
    "        self.pitchDeltas = torch.tensor([], dtype = int)\n",
    "        self.pitchBorders = torch.tensor([], dtype = int)\n",
    "        self.pitch = torch.tensor([0], dtype = int)\n",
    "        self.spectra = torch.tensor([[]], dtype = float)\n",
    "        self.spectrum = torch.tensor([], dtype = float)\n",
    "        self.excitation = torch.tensor([], dtype = float)\n",
    "        self.voicedExcitation = torch.tensor([], dtype = float)\n",
    "        self.VoicedExcitations = torch.tensor([], dtype = float)\n",
    "        \n",
    "    def CalculatePitch(self, expectedPitch, searchRange = 0.2):\n",
    "        batchSize = math.floor((1. + searchRange) * self.sampleRate / expectedPitch)\n",
    "        lowerSearchLimit = math.floor((1. - searchRange) * self.sampleRate / expectedPitch)\n",
    "        batchStart = 0\n",
    "        while batchStart + batchSize <= self.waveform.size()[0] - batchSize:\n",
    "            sample = torch.index_select(self.waveform, 0, torch.linspace(batchStart, batchStart + batchSize, batchSize, dtype = int))\n",
    "            zeroTransitions = torch.tensor([], dtype = int)\n",
    "            for i in range(lowerSearchLimit, batchSize):\n",
    "                if (sample[i-1] < 0) and (sample[i] > 0):\n",
    "                    zeroTransitions = torch.cat([zeroTransitions, torch.tensor([i])], 0)\n",
    "            error = math.inf\n",
    "            delta = math.floor(self.sampleRate / expectedPitch)\n",
    "            for i in zeroTransitions:\n",
    "                shiftedSample = torch.index_select(self.waveform, 0, torch.linspace(batchStart + i.item(), batchStart + batchSize + i.item(), batchSize, dtype = int))\n",
    "                newError = torch.sum(torch.pow(sample - shiftedSample, 2))\n",
    "                if error > newError:\n",
    "                    delta = i.item()\n",
    "                    error = newError\n",
    "            self.pitchDeltas = torch.cat([self.pitchDeltas, torch.tensor([delta])])\n",
    "            batchStart += delta\n",
    "        nBatches = self.pitchDeltas.size()[0]\n",
    "        self.pitchBorders = torch.zeros(nBatches + 1, dtype = int)\n",
    "        for i in range(nBatches):\n",
    "            self.pitchBorders[i+1] = self.pitchBorders[i] + self.pitchDeltas[i]\n",
    "        self.pitch = torch.mean(self.pitchDeltas.float()).int()\n",
    "        del batchSize\n",
    "        del lowerSearchLimit\n",
    "        del batchStart\n",
    "        del sample\n",
    "        del zeroTransitions\n",
    "        del error\n",
    "        del delta\n",
    "        del shiftedSample\n",
    "        del newError\n",
    "        del nBatches\n",
    "        \n",
    "    def CalculateSpectra(self, iterations = 10, filterWidth = 10, preIterations = 2):\n",
    "        tripleBatchSize = int(self.sampleRate / 25)\n",
    "        BatchSize = int(self.sampleRate / 75)\n",
    "        Window = torch.hann_window(tripleBatchSize)\n",
    "        signals = torch.stft(self.waveform, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, return_complex = True, onesided = True)\n",
    "        signals = torch.transpose(signals, 0, 1)\n",
    "        signalsAbs = signals.abs()\n",
    "        \n",
    "        workingSpectra = torch.sqrt(signalsAbs)\n",
    "        \n",
    "        workingSpectra = torch.max(workingSpectra, torch.tensor([-100]))\n",
    "        self.spectra = torch.full_like(workingSpectra, -float(\"inf\"), dtype=torch.float)\n",
    "        \n",
    "        for j in range(preIterations):\n",
    "            workingSpectra = torch.max(workingSpectra, self.spectra)\n",
    "            self.spectra = workingSpectra\n",
    "            for i in range(filterWidth):\n",
    "                self.spectra = torch.roll(workingSpectra, -i, dims = 1) + self.spectra + torch.roll(workingSpectra, i, dims = 1)\n",
    "            self.spectra = self.spectra / (2 * filterWidth + 1)\n",
    "        \n",
    "        self.VoicedExcitations = torch.zeros_like(signals)\n",
    "        for i in range(signals.size()[0]):\n",
    "            for j in range(signals.size()[1]):\n",
    "                if torch.sqrt(signalsAbs[i][j]) > self.spectra[i][j]:\n",
    "                    self.VoicedExcitations[i][j] = signals[i][j]\n",
    "                \n",
    "        for j in range(iterations):\n",
    "            workingSpectra = torch.max(workingSpectra, self.spectra)\n",
    "            self.spectra = workingSpectra\n",
    "            for i in range(filterWidth):\n",
    "                self.spectra = torch.roll(workingSpectra, -i, dims = 1) + self.spectra + torch.roll(workingSpectra, i, dims = 1)\n",
    "            self.spectra = self.spectra / (2 * filterWidth + 1)\n",
    "        \n",
    "        self.spectrum = torch.mean(self.spectra, 0)\n",
    "        for i in range(self.spectra.size()[0]):\n",
    "            self.spectra[i] = self.spectra[i] - self.spectrum\n",
    "        #return torch.log(signalsAbs)\n",
    "        del Window\n",
    "        del signals\n",
    "        del workingSpectra\n",
    "        \n",
    "    def CalculateExcitation(self, filterWidth = 10):\n",
    "        tripleBatchSize = int(self.sampleRate / 25)\n",
    "        BatchSize = int(self.sampleRate / 75)\n",
    "        Window = torch.hann_window(tripleBatchSize)\n",
    "        signals = torch.stft(self.waveform, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, return_complex = True, onesided = True)\n",
    "        signals = torch.transpose(signals, 0, 1)\n",
    "        excitations = torch.empty_like(signals)\n",
    "        for i in range(excitations.size()[0]):\n",
    "            excitations[i] = signals[i] / torch.square(self.spectrum + self.spectra[i])\n",
    "            self.VoicedExcitations[i] = self.VoicedExcitations[i] / torch.square(self.spectrum + self.spectra[i])\n",
    "        \n",
    "        VoicedExcitations = torch.transpose(self.VoicedExcitations, 0, 1)\n",
    "        excitations = torch.transpose(excitations, 0, 1)\n",
    "        self.excitation = torch.istft(excitations, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, onesided = True)\n",
    "        self.voicedExcitation = torch.istft(VoicedExcitations, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, onesided = True)\n",
    "        \n",
    "        self.excitation = self.excitation - self.voicedExcitation\n",
    "        self.excitation = torch.stft(self.excitation, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, return_complex = True, onesided = True)\n",
    "        self.voicedExcitation = torch.stft(self.voicedExcitation, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, return_complex = True, onesided = True)\n",
    "        self.excitation = torch.transpose(self.excitation, 0, 1)\n",
    "        #self.voicedExcitation = torch.transpose(self.voicedExcitation, 0, 1)\n",
    "        \n",
    "        del Window\n",
    "        del signals\n",
    "        del excitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "expired-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(RelLoss, self).__init__()\n",
    " \n",
    "    def forward(self, inputs, targets):    \n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        differences = torch.abs(inputs - targets)\n",
    "        sums = torch.abs(inputs + targets)\n",
    "        out = (differences / sums).sum() / inputs.size()[0]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "configured-craps",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecCrfAi(nn.Module):\n",
    "    def __init__(self, learningRate=1e-4):\n",
    "        super(SpecCrfAi, self).__init__()\n",
    "        \n",
    "        self.layer1 = torch.nn.Linear(3843, 3843)\n",
    "        self.ReLu1 = nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(3843, 5763)\n",
    "        self.ReLu2 = nn.ReLU()\n",
    "        self.layer3 = torch.nn.Linear(5763, 3842)\n",
    "        self.ReLu3 = nn.ReLU()\n",
    "        self.layer4 = torch.nn.Linear(3842, 1921)\n",
    "        \n",
    "        self.learningRate = learningRate\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learningRate, weight_decay=0.)\n",
    "        self.criterion = nn.L1Loss()\n",
    "        #self.criterion = RelLoss()\n",
    "        \n",
    "    def forward(self, spectrum1, spectrum2, factor):\n",
    "        fac = torch.tensor([factor])\n",
    "        x = torch.cat((spectrum1, spectrum2, fac), dim = 0)\n",
    "        x = x.float()#.unsqueeze(0).unsqueeze(0)\n",
    "        x = self.layer1(x)\n",
    "        x = self.ReLu1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.ReLu2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.ReLu3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "    \n",
    "    def processData(self, spectrum1, spectrum2, factor):\n",
    "        output = torch.square(torch.squeeze(self(torch.sqrt(spectrum1), torch.sqrt(spectrum2), factor)))\n",
    "        return output\n",
    "    \n",
    "    def train(self, indata, epochs=1):\n",
    "        for epoch in range(epochs):\n",
    "            for data in self.dataLoader(indata):\n",
    "                spectrum1 = data[0]\n",
    "                spectrum2 = data[-1]\n",
    "                indexList = np.arange(0, data.size()[0], 1)\n",
    "                np.random.shuffle(indexList)\n",
    "                for i in indexList:\n",
    "                    factor = i / float(data.size()[0])\n",
    "                    spectrumTarget = data[i]\n",
    "                    output = torch.squeeze(self(spectrum1, spectrum2, factor))\n",
    "                    loss = self.criterion(output, spectrumTarget)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "            print('epoch [{}/{}], loss:{:.4f}'\n",
    "                  .format(epoch + 1, epochs, loss.data))\n",
    "            \n",
    "    def dataLoader(self, data):\n",
    "        return torch.utils.data.DataLoader(dataset=data, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "black-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocalSegment:\n",
    "    def __init__(self, start1, start2, start3, end1, end2, end3, startCap, endCap, phonemeKey, vb, offset, repetititionSpacing, pitch, steadiness):\n",
    "        self.start1 = start1\n",
    "        self.start2 = start2\n",
    "        self.start3 = start3\n",
    "        self.end1 = end1\n",
    "        self.end2 = end2\n",
    "        self.end3 = end3\n",
    "        self.startCap = startCap\n",
    "        self.endCap = endCap\n",
    "        self.phonemeKey = phonemeKey\n",
    "        self.vb = vb\n",
    "        self.offset = offset\n",
    "        self.repetititionSpacing = repetititionSpacing\n",
    "        self.pitch = pitch\n",
    "        self.steadiness = steadiness\n",
    "        \n",
    "    def phaseShift(self, inputTensor, pitch, phase):\n",
    "        #factor = torch.exp(-j * phase / pitch * torch.unsqueeze(torch.arange(inputTensor.size()[0]), 1))\n",
    "        #return factor * inputTensor\n",
    "        absolutes = inputTensor.abs()\n",
    "        phases = inputTensor.angle()\n",
    "        phaseOffsets = torch.full(phases.size(), phase)# / pitch)\n",
    "        #phaseOffsets *= torch.unsqueeze(torch.arange(phases.size()[0]), 1)\n",
    "        phases += phaseOffsets\n",
    "        #phases = torch.fmod(phases, 2 * math.pi)\n",
    "        return torch.polar(absolutes, phases)\n",
    "        \n",
    "    def loopSamplerVoicedExcitation(self, inputTensor, targetSize, repetititionSpacing, pitch):\n",
    "        batchRS = math.ceil(repetititionSpacing * inputTensor.size()[1] / 2)\n",
    "        BatchSize = int(self.vb.sampleRate / 75)\n",
    "        tripleBatchSize = int(self.vb.sampleRate / 25)\n",
    "        repetititionSpacing = int(repetititionSpacing * BatchSize * math.ceil(inputTensor.size()[1] / 2))\n",
    "        window = torch.hann_window(tripleBatchSize)\n",
    "        alignPhase = inputTensor[batchRS][pitch].angle()\n",
    "        finalPhase = inputTensor[1][pitch].angle()\n",
    "        phaseDiff = (finalPhase - alignPhase)\n",
    "        requiredTensors = math.ceil((targetSize/BatchSize - batchRS) / (inputTensor.size()[1] - batchRS))\n",
    "        \n",
    "        if requiredTensors == 1:\n",
    "            outputTensor = inputTensor.clone()\n",
    "            outputTensor = torch.istft(outputTensor, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = window, onesided = True, length = inputTensor.size()[1]*BatchSize)\n",
    "        else:\n",
    "            outputTensor = torch.zeros(requiredTensors * (inputTensor.size()[1] * BatchSize - repetititionSpacing) + repetititionSpacing)\n",
    "            \n",
    "            workingTensor = inputTensor.clone()\n",
    "            workingTensor = torch.istft(workingTensor, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = window, onesided = True, length = inputTensor.size()[1]*BatchSize)\n",
    "            workingTensor[-repetititionSpacing:] *= torch.linspace(1, 0, repetititionSpacing)\n",
    "            outputTensor[0:inputTensor.size()[1] * BatchSize] += workingTensor\n",
    "            \n",
    "            for i in range(1, requiredTensors - 1):\n",
    "                workingTensor = inputTensor.clone()\n",
    "                workingTensor = self.phaseShift(workingTensor, pitch, i * phaseDiff)\n",
    "                workingTensor = torch.istft(workingTensor, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = window, onesided = True, length = inputTensor.size()[1]*BatchSize)\n",
    "                workingTensor[0:repetititionSpacing] *= torch.linspace(0, 1, repetititionSpacing)\n",
    "                workingTensor[-repetititionSpacing:] *= torch.linspace(1, 0, repetititionSpacing)\n",
    "                outputTensor[i * (inputTensor.size()[1] * BatchSize - repetititionSpacing):i * (inputTensor.size()[1] * BatchSize - repetititionSpacing) + inputTensor.size()[1] * BatchSize] += workingTensor\n",
    "            \n",
    "            workingTensor = inputTensor.clone()\n",
    "            workingTensor = self.phaseShift(workingTensor, pitch, (requiredTensors - 1) * phaseDiff)\n",
    "            workingTensor = torch.istft(workingTensor, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = window, onesided = True, length = inputTensor.size()[1]*BatchSize)\n",
    "            workingTensor[0:repetititionSpacing] *= torch.linspace(0, 1, repetititionSpacing)\n",
    "            outputTensor[(requiredTensors - 1) * (inputTensor.size()[1] * BatchSize - repetititionSpacing):] += workingTensor\n",
    "        return outputTensor[0:targetSize * BatchSize]\n",
    "    \n",
    "    def loopSamplerSpectrum(self, inputTensor, targetSize, repetititionSpacing):\n",
    "        repetititionSpacing = math.ceil(repetititionSpacing * inputTensor.size()[0] / 2)\n",
    "        requiredTensors = math.ceil((targetSize - repetititionSpacing) / (inputTensor.size()[0] - repetititionSpacing))\n",
    "        if requiredTensors == 1:\n",
    "            outputTensor = inputTensor.clone()\n",
    "        else:\n",
    "            outputTensor = torch.zeros(requiredTensors * (inputTensor.size()[0] - repetititionSpacing) + repetititionSpacing, inputTensor.size()[1])\n",
    "            \n",
    "            workingTensor = inputTensor.clone()\n",
    "            workingTensor[-repetititionSpacing:] *= torch.unsqueeze(torch.linspace(1, 0, repetititionSpacing), 1)\n",
    "            outputTensor[0:inputTensor.size()[0]] += workingTensor\n",
    "            \n",
    "            for i in range(1, requiredTensors - 1):\n",
    "                workingTensor = inputTensor.clone()\n",
    "                workingTensor[0:repetititionSpacing] *= torch.unsqueeze(torch.linspace(0, 1, repetititionSpacing), 1)\n",
    "                workingTensor[-repetititionSpacing:] *= torch.unsqueeze(torch.linspace(1, 0, repetititionSpacing), 1)\n",
    "                outputTensor[i * (inputTensor.size()[0] - repetititionSpacing):i * (inputTensor.size()[0] - repetititionSpacing) + inputTensor.size()[0]] += workingTensor\n",
    "            \n",
    "            workingTensor = inputTensor.clone()\n",
    "            workingTensor[0:repetititionSpacing] *= torch.unsqueeze(torch.linspace(0, 1, repetititionSpacing), 1)\n",
    "            outputTensor[(requiredTensors - 1) * (inputTensor.size()[0] - repetititionSpacing):] += workingTensor\n",
    "        return outputTensor[0:targetSize]\n",
    "    \n",
    "    def getSpectrum(self):\n",
    "        if self.startCap:\n",
    "            windowStart = self.offset\n",
    "        else:\n",
    "            windowStart = self.start3 - self.start1 + self.offset\n",
    "        if self.endCap:\n",
    "            windowEnd = self.end3 - self.start1 + self.offset\n",
    "        else:\n",
    "            windowEnd = self.end1 - self.start1 + self.offset\n",
    "        spectrum =  self.vb.phonemeDict[self.phonemeKey].spectrum#implement looping\n",
    "        #spectra =  self.vb.phonemeDict[self.phonemeKey].spectra[windowStart:windowEnd]\n",
    "        spectra = self.loopSamplerSpectrum(self.vb.phonemeDict[self.phonemeKey].spectra, windowEnd, self.repetititionSpacing)[windowStart:windowEnd]\n",
    "        return torch.square(spectrum + (math.pow(1 - self.steadiness, 2) * spectra))\n",
    "    \n",
    "    def getExcitation(self):\n",
    "        BatchSize = int(self.vb.sampleRate / 75)\n",
    "        tripleBatchSize = int(self.vb.sampleRate / 25)\n",
    "        premul = self.vb.phonemeDict[self.phonemeKey].excitation.size()[0] / (self.end3 - self.start1 + 1)\n",
    "        #premul = 1\n",
    "        if self.startCap:\n",
    "            windowStart = 0\n",
    "            length = -self.start1\n",
    "        else:\n",
    "            windowStart = math.floor((self.start2 - self.start1) * premul)\n",
    "            length = -self.start2\n",
    "        if self.endCap:\n",
    "            windowEnd = math.ceil((self.end3 - self.start1) * premul)\n",
    "            length += self.end3\n",
    "        else:\n",
    "            windowEnd = math.ceil((self.end2 - self.start1) * premul)\n",
    "            length += self.end2\n",
    "        excitation = self.vb.phonemeDict[self.phonemeKey].excitation[windowStart:windowEnd]\n",
    "        excitation = torch.transpose(excitation, 0, 1)\n",
    "        transform = torchaudio.transforms.TimeStretch(hop_length = int(self.vb.sampleRate / 75),\n",
    "                                                      n_freq = int(self.vb.sampleRate / 25 / 2) + 1, \n",
    "                                                      fixed_rate = premul)\n",
    "        excitation = transform(torch.view_as_real(excitation))\n",
    "        excitation = torch.view_as_complex(excitation)\n",
    "        window = torch.hann_window(int(self.vb.sampleRate / 25))\n",
    "        excitation = torch.istft(excitation, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = window, onesided = True, length = length*int(self.vb.sampleRate / 75))\n",
    "        \n",
    "        return excitation[0:length*int(self.vb.sampleRate / 75)]\n",
    "    \n",
    "    def getVoicedExcitation(self):\n",
    "        BatchSize = int(self.vb.sampleRate / 75)\n",
    "        nativePitch = self.vb.phonemeDict[self.phonemeKey].pitch\n",
    "        #nativePitch = self.vb.phonemeDict[self.phonemeKey].pitches[...]\n",
    "        #pitch = nativePitch + self.vb.phonemeDict[phonemeKey].pitches...\n",
    "        premul = self.pitch / nativePitch * self.vb.sampleRate / 75\n",
    "        windowStart = math.floor(self.offset * self.vb.sampleRate / 75)\n",
    "        windowEnd = math.ceil((self.end3 - self.start1) * premul + (self.offset * self.vb.sampleRate / 75))\n",
    "        #windowStart = math.floor(self.offset)\n",
    "        #windowEnd = math.ceil((self.end3 - self.start1) * premul + self.offset)\n",
    "        #voicedExcitation = self.vb.phonemeDict[self.phonemeKey].voicedExcitation[windowStart:windowEnd]\n",
    "        voicedExcitation = self.loopSamplerVoicedExcitation(self.vb.phonemeDict[self.phonemeKey].voicedExcitation, windowEnd, self.repetititionSpacing, math.ceil(nativePitch / 75.))[windowStart:windowEnd]\n",
    "        transform = torchaudio.transforms.Resample(orig_freq = nativePitch,\n",
    "                                                   new_freq = self.pitch,\n",
    "                                                   resampling_method = 'sinc_interpolation')\n",
    "        voicedExcitation = transform(voicedExcitation)\n",
    "        if self.startCap == False:\n",
    "            slope = torch.linspace(0, 1, (self.start3 - self.start1) * int(self.vb.sampleRate / 75))\n",
    "            voicedExcitation[0:(self.start3 - self.start1) * int(self.vb.sampleRate / 75)] *= slope\n",
    "        if self.endCap == False:\n",
    "            slope = torch.linspace(1, 0, (self.end3 - self.end1) * int(self.vb.sampleRate / 75))\n",
    "            voicedExcitation[(self.end1 - self.start1) * int(self.vb.sampleRate / 75):(self.end3 - self.start1) * int(self.vb.sampleRate / 75)] *= slope\n",
    "        return voicedExcitation[0:(self.end3 - self.start1) * int(self.vb.sampleRate / 75)]\n",
    "        #resample segments\n",
    "        #individual fourier transform\n",
    "        #istft\n",
    "        #windowing adaptive to borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "emotional-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocalSequence:\n",
    "    def __init__(self, start, end, vb, borders, phonemes, offsets):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.vb = vb\n",
    "        self.synth = Synthesizer(self.vb.sampleRate)\n",
    "        \n",
    "        self.spectrum = torch.zeros((self.end - self.start, int(self.vb.sampleRate / 25 / 2) + 1))\n",
    "        self.excitation = torch.zeros((self.end - self.start) * int(self.vb.sampleRate / 75))\n",
    "        self.voicedExcitation = torch.zeros((self.end - self.start) * int(self.vb.sampleRate / 75))\n",
    "        \n",
    "        self.segments = []\n",
    "        if len(phonemes)== 1:#rewrite border system to use tensor, implement pitch and steadiness\n",
    "            self.segments.append(VocalSegment(borders[0], borders[1], borders[2], borders[3], borders[4], borders[5],\n",
    "                                             True, True, phonemes[0], vb, offsets[0], 0.5, 386, 0))\n",
    "        else:\n",
    "            self.segments.append(VocalSegment(borders[0], borders[1], borders[2], borders[3], borders[4], borders[5],\n",
    "                                             True, False, phonemes[0], vb, offsets[0], 0.5, 386, 0))\n",
    "            for i in range(1, len(phonemes)-1):\n",
    "                self.segments.append(VocalSegment(borders[3*i], borders[3*i+1], borders[3*i+2], borders[3*i+3], borders[3*i+4], borders[3*i+5],\n",
    "                                                  False, False, phonemes[i], vb, offsets[i], 0.5, 386, 0))\n",
    "            endpoint = len(phonemes)-1\n",
    "            self.segments.append(VocalSegment(borders[3*endpoint], borders[3*endpoint+1], borders[3*endpoint+2], borders[3*endpoint+3], borders[3*endpoint+4], borders[3*endpoint+5],\n",
    "                                             False, True, phonemes[-1], vb, offsets[-1], 0.5, 386, 0))\n",
    "\n",
    "        self.requiresUpdate = np.ones(len(phonemes))\n",
    "        self.update()\n",
    "    def update(self):\n",
    "        for i in range(self.requiresUpdate.size):\n",
    "            if self.requiresUpdate[i] == 1:\n",
    "                print(i)\n",
    "                segment = self.segments[i]\n",
    "                spectrum = torch.zeros((segment.end3 - segment.start1, int(self.vb.sampleRate / 25 / 2) + 1))\n",
    "                excitation = torch.zeros((segment.end3 - segment.start1) * int(self.vb.sampleRate / 75))\n",
    "                voicedExcitation = torch.zeros((segment.end3 - segment.start1) * int(self.vb.sampleRate / 75))\n",
    "                if segment.startCap:\n",
    "                    windowStart = 0\n",
    "                else:\n",
    "                    windowStart = segment.start3 - segment.start1\n",
    "                    previousSpectrum = self.segments[i-1].getSpectrum()[-1]\n",
    "                    previousVoicedExcitation = self.segments[i-1].getVoicedExcitation()[(self.segments[i-1].end1-self.segments[i-1].end3)*int(self.vb.sampleRate/75):]\n",
    "                if segment.endCap:\n",
    "                    windowEnd = segment.end3 - segment.start1\n",
    "                else:\n",
    "                    windowEnd = segment.end1 - segment.start1\n",
    "                    nextSpectrum = self.segments[i+1].getSpectrum()[0]\n",
    "                    nextVoicedExcitation = self.segments[i+1].getVoicedExcitation()[0:(self.segments[i+1].start3-self.segments[i+1].start1)*int(self.vb.sampleRate/75)]\n",
    "                \n",
    "                spectrum[windowStart:windowEnd] = segment.getSpectrum()\n",
    "                voicedExcitation = segment.getVoicedExcitation()\n",
    "                if segment.startCap == False:\n",
    "                    for j in range(segment.start3 - segment.start1):\n",
    "                        spectrum[j] = self.vb.crfAi.processData(previousSpectrum, spectrum[windowStart], j / (segment.start3 - segment.start1))\n",
    "                    voicedExcitation[0:(segment.start3-segment.start1)*int(self.vb.sampleRate/75)] += previousVoicedExcitation\n",
    "                if segment.endCap == False:\n",
    "                    for j in range(segment.end1 - segment.start1, segment.end3 - segment.start1):\n",
    "                        spectrum[j] = self.vb.crfAi.processData(spectrum[windowEnd], nextSpectrum, (j - segment.start1) / (segment.end3 - segment.end1))\n",
    "                    voicedExcitation[(segment.end1-segment.end3)*int(self.vb.sampleRate/75):] += nextVoicedExcitation\n",
    "                if segment.startCap:\n",
    "                    windowStart = 0\n",
    "                else:\n",
    "                    windowStart = (segment.start2 - segment.start1) * int(self.vb.sampleRate / 75)\n",
    "                    previousExcitation = self.segments[i-1].getExcitation()[(segment.start1-segment.start2)*int(self.vb.sampleRate/75):]\n",
    "                    excitation[0:windowStart] = previousExcitation\n",
    "                if segment.endCap:\n",
    "                    windowEnd = (segment.end3 - segment.start1) * int(self.vb.sampleRate / 75)\n",
    "                else:\n",
    "                    windowEnd = (segment.end2 - segment.start1) * int(self.vb.sampleRate / 75)\n",
    "                    nextExcitation = self.segments[i+1].getExcitation()[0:(segment.end3-segment.end2)*int(self.vb.sampleRate/75)]\n",
    "                    excitation[windowEnd:] = nextExcitation\n",
    "                excitation[windowStart:windowEnd] = segment.getExcitation()\n",
    "                \n",
    "                self.spectrum[segment.start1:segment.end3] = spectrum\n",
    "                self.excitation[segment.start1*int(self.vb.sampleRate/75):segment.end3*int(self.vb.sampleRate/75)] = excitation\n",
    "                self.voicedExcitation[segment.start1*int(self.vb.sampleRate/75):segment.end3*int(self.vb.sampleRate/75)] = voicedExcitation\n",
    "                \n",
    "                skipPrevious = True#implement skipPrevious\n",
    "            else:\n",
    "                skipPrevious = False\n",
    "            \n",
    "        self.synth.Synthesize(0, self.spectrum, self.excitation, self.voicedExcitation)\n",
    "    def save(self):\n",
    "        self.synth.save(\"Output_Demo.wav\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "valid-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TempVB:\n",
    "    def __init__(self):\n",
    "        self.sampleRate = 48000\n",
    "        self.phonemeDict = dict([])\n",
    "        phonemeKeys = [\"A\", \"E\", \"I\", \"O\", \"U\", \"G\", \"K\", \"N\", \"S\", \"T\"]\n",
    "        for key in phonemeKeys:\n",
    "            self.phonemeDict[key] = AudioSample(\"Samples_rip/\"+key+\".wav\")\n",
    "            self.sampleRate = self.phonemeDict[key].sampleRate\n",
    "            self.phonemeDict[key].CalculatePitch(249.)\n",
    "            self.phonemeDict[key].CalculateSpectra(iterations = 15)\n",
    "            self.phonemeDict[key].CalculateExcitation()\n",
    "        self.crfAi = SpecCrfAi(learningRate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "expected-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Voicebank:\n",
    "    def __init__(self, vbKey):\n",
    "        self.phonemeDict = dict()\n",
    "        loaded_weights = 0#(vbKey)\n",
    "        self.crfAi = 0#SpecCrfAi(loaded_weights)\n",
    "        #load additional parameters\n",
    "        self.sampleRate = 48000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "polished-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Synthesizer:\n",
    "    def __init__(self, sampleRate):\n",
    "        self.sampleRate = sampleRate\n",
    "        self.returnSignal = torch.tensor([], dtype = float)\n",
    "        \n",
    "    def Synthesize(self, steadiness, spectrum, Excitation, VoicedExcitation):\n",
    "        tripleBatchSize = int(self.sampleRate / 25)\n",
    "        BatchSize = int(self.sampleRate / 75)\n",
    "        Window = torch.hann_window(tripleBatchSize)\n",
    "        \n",
    "        #HERE + VoicedExcitation\n",
    "        \n",
    "        self.returnSignal = torch.stft(Excitation + VoicedExcitation , tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, return_complex = True, onesided = True)\n",
    "        self.returnSignal = torch.transpose(self.returnSignal, 0, 1)[0:-1]\n",
    "        self.returnSignal = self.returnSignal * spectrum\n",
    "        self.returnSignal = torch.transpose(self.returnSignal, 0, 1)\n",
    "        self.returnSignal = torch.istft(self.returnSignal, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, onesided=True)\n",
    "        del Window\n",
    "        \n",
    "    def save(self, filepath):\n",
    "        torchaudio.save(filepath, torch.unsqueeze(self.returnSignal.detach(), 0), self.sampleRate, format=\"wav\", encoding=\"PCM_S\", bits_per_sample=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "latin-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingSamples = dict([])\n",
    "TrainingKeys = [\"A_E\", \"A_G\", \"A_I\", \"A_K\", \"A_N\", \"A_O\", \"A_S\", \"A_T\", \"A_U\",\n",
    "                   \"E_A\", \"E_G\", \"E_I\", \"E_K\", \"E_N\", \"E_O\", \"E_S\", \"E_T\", \"E_U\",\n",
    "                   \"I_A\", \"I_E\", \"I_G\", \"I_K\", \"I_N\", \"I_O\", \"I_S\", \"I_T\", \"I_U\",\n",
    "                   \"O_A\", \"O_E\", \"O_G\", \"O_I\", \"O_K\", \"O_N\", \"O_S\", \"O_T\", \"O_U\",\n",
    "                   \"U_A\", \"U_E\", \"U_G\", \"U_I\", \"U_K\", \"U_N\", \"U_O\", \"U_S\", \"U_T\",\n",
    "                   \"G_A\", \"G_E\", \"G_I\", \"G_O\", \"G_U\",\n",
    "                   \"K_A\", \"K_E\", \"K_I\", \"K_O\", \"K_U\",\n",
    "                   \"N_A\", \"N_E\", \"N_I\", \"N_O\", \"N_U\",\n",
    "                   \"S_A\", \"S_E\", \"S_I\", \"S_O\", \"S_U\",\n",
    "                   \"T_A\", \"T_E\", \"T_I\", \"T_O\", \"T_U\"\n",
    "                  ]\n",
    "\n",
    "for key in TrainingKeys:\n",
    "    TrainingSamples[key] = AudioSample(\"Samples_rip/\"+key+\".wav\")\n",
    "    TrainingSamples[key].CalculatePitch(249.)\n",
    "    TrainingSamples[key].CalculateSpectra(iterations = 25)\n",
    "    TrainingSamples[key].CalculateExcitation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "former-economics",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/5], loss:0.6557\n",
      "epoch [2/5], loss:0.4192\n",
      "epoch [3/5], loss:0.1917\n",
      "epoch [4/5], loss:0.1614\n",
      "epoch [5/5], loss:0.2130\n",
      "epoch [1/5], loss:0.1909\n",
      "epoch [2/5], loss:0.1755\n",
      "epoch [3/5], loss:0.2684\n",
      "epoch [4/5], loss:0.1496\n",
      "epoch [5/5], loss:0.1932\n",
      "epoch [1/5], loss:0.1519\n",
      "epoch [2/5], loss:0.1237\n",
      "epoch [3/5], loss:0.1439\n",
      "epoch [4/5], loss:0.0940\n",
      "epoch [5/5], loss:0.1196\n",
      "epoch [1/5], loss:0.0632\n",
      "epoch [2/5], loss:0.1258\n",
      "epoch [3/5], loss:0.1371\n",
      "epoch [4/5], loss:0.1259\n",
      "epoch [5/5], loss:0.1014\n",
      "epoch [1/5], loss:0.1258\n",
      "epoch [2/5], loss:0.1002\n",
      "epoch [3/5], loss:0.1022\n",
      "epoch [4/5], loss:0.1144\n",
      "epoch [5/5], loss:0.1059\n",
      "epoch [1/5], loss:0.1543\n",
      "epoch [2/5], loss:0.0894\n",
      "epoch [3/5], loss:0.1709\n",
      "epoch [4/5], loss:0.0657\n",
      "epoch [5/5], loss:0.0601\n",
      "epoch [1/5], loss:0.2995\n",
      "epoch [2/5], loss:0.2330\n",
      "epoch [3/5], loss:0.1616\n",
      "epoch [4/5], loss:0.1721\n",
      "epoch [5/5], loss:0.1816\n",
      "epoch [1/5], loss:0.1844\n",
      "epoch [2/5], loss:0.1098\n",
      "epoch [3/5], loss:0.0273\n",
      "epoch [4/5], loss:0.0216\n",
      "epoch [5/5], loss:0.0322\n",
      "epoch [1/5], loss:0.1166\n",
      "epoch [2/5], loss:0.0765\n",
      "epoch [3/5], loss:0.0740\n",
      "epoch [4/5], loss:0.0611\n",
      "epoch [5/5], loss:0.0922\n",
      "epoch [1/5], loss:0.1496\n",
      "epoch [2/5], loss:0.0893\n",
      "epoch [3/5], loss:0.0849\n",
      "epoch [4/5], loss:0.1040\n",
      "epoch [5/5], loss:0.0991\n",
      "epoch [1/5], loss:0.1522\n",
      "epoch [2/5], loss:0.0988\n",
      "epoch [3/5], loss:0.0749\n",
      "epoch [4/5], loss:0.0917\n",
      "epoch [5/5], loss:0.1164\n",
      "epoch [1/5], loss:0.0775\n",
      "epoch [2/5], loss:0.1098\n",
      "epoch [3/5], loss:0.1219\n",
      "epoch [4/5], loss:0.1171\n",
      "epoch [5/5], loss:0.1071\n",
      "epoch [1/5], loss:0.0190\n",
      "epoch [2/5], loss:0.0545\n",
      "epoch [3/5], loss:0.0714\n",
      "epoch [4/5], loss:0.1439\n",
      "epoch [5/5], loss:0.0935\n",
      "epoch [1/5], loss:0.1474\n",
      "epoch [2/5], loss:0.0500\n",
      "epoch [3/5], loss:0.0498\n",
      "epoch [4/5], loss:0.0490\n",
      "epoch [5/5], loss:0.0747\n",
      "epoch [1/5], loss:0.1918\n",
      "epoch [2/5], loss:0.1088\n",
      "epoch [3/5], loss:0.1345\n",
      "epoch [4/5], loss:0.1316\n",
      "epoch [5/5], loss:0.0827\n",
      "epoch [1/5], loss:0.1464\n",
      "epoch [2/5], loss:0.1184\n",
      "epoch [3/5], loss:0.1398\n",
      "epoch [4/5], loss:0.1319\n",
      "epoch [5/5], loss:0.0921\n",
      "epoch [1/5], loss:0.0261\n",
      "epoch [2/5], loss:0.2700\n",
      "epoch [3/5], loss:0.0734\n",
      "epoch [4/5], loss:0.0578\n",
      "epoch [5/5], loss:0.1348\n",
      "epoch [1/5], loss:0.1606\n",
      "epoch [2/5], loss:0.0907\n",
      "epoch [3/5], loss:0.0848\n",
      "epoch [4/5], loss:0.0599\n",
      "epoch [5/5], loss:0.0580\n",
      "epoch [1/5], loss:0.1604\n",
      "epoch [2/5], loss:0.0940\n",
      "epoch [3/5], loss:0.0850\n",
      "epoch [4/5], loss:0.1452\n",
      "epoch [5/5], loss:0.0656\n",
      "epoch [1/5], loss:0.1621\n",
      "epoch [2/5], loss:0.1003\n",
      "epoch [3/5], loss:0.0649\n",
      "epoch [4/5], loss:0.0550\n",
      "epoch [5/5], loss:0.0514\n",
      "epoch [1/5], loss:0.0587\n",
      "epoch [2/5], loss:0.0600\n",
      "epoch [3/5], loss:0.0153\n",
      "epoch [4/5], loss:0.0296\n",
      "epoch [5/5], loss:0.0194\n",
      "epoch [1/5], loss:0.0830\n",
      "epoch [2/5], loss:0.1220\n",
      "epoch [3/5], loss:0.1079\n",
      "epoch [4/5], loss:0.0566\n",
      "epoch [5/5], loss:0.0842\n",
      "epoch [1/5], loss:0.0752\n",
      "epoch [2/5], loss:0.0614\n",
      "epoch [3/5], loss:0.0534\n",
      "epoch [4/5], loss:0.0460\n",
      "epoch [5/5], loss:0.0367\n",
      "epoch [1/5], loss:0.1322\n",
      "epoch [2/5], loss:0.1140\n",
      "epoch [3/5], loss:0.1106\n",
      "epoch [4/5], loss:0.0572\n",
      "epoch [5/5], loss:0.0945\n",
      "epoch [1/5], loss:0.0820\n",
      "epoch [2/5], loss:0.0845\n",
      "epoch [3/5], loss:0.0751\n",
      "epoch [4/5], loss:0.0620\n",
      "epoch [5/5], loss:0.0581\n",
      "epoch [1/5], loss:0.0437\n",
      "epoch [2/5], loss:0.0102\n",
      "epoch [3/5], loss:0.0079\n",
      "epoch [4/5], loss:0.0084\n",
      "epoch [5/5], loss:0.0109\n",
      "epoch [1/5], loss:0.0728\n",
      "epoch [2/5], loss:0.0465\n",
      "epoch [3/5], loss:0.0677\n",
      "epoch [4/5], loss:0.0400\n",
      "epoch [5/5], loss:0.0456\n",
      "epoch [1/5], loss:0.2354\n",
      "epoch [2/5], loss:0.1023\n",
      "epoch [3/5], loss:0.0519\n",
      "epoch [4/5], loss:0.0634\n",
      "epoch [5/5], loss:0.0638\n",
      "epoch [1/5], loss:0.1037\n",
      "epoch [2/5], loss:0.0590\n",
      "epoch [3/5], loss:0.0460\n",
      "epoch [4/5], loss:0.0664\n",
      "epoch [5/5], loss:0.0840\n",
      "epoch [1/5], loss:0.0808\n",
      "epoch [2/5], loss:0.0089\n",
      "epoch [3/5], loss:0.0444\n",
      "epoch [4/5], loss:0.0571\n",
      "epoch [5/5], loss:0.0923\n",
      "epoch [1/5], loss:0.1009\n",
      "epoch [2/5], loss:0.1285\n",
      "epoch [3/5], loss:0.0947\n",
      "epoch [4/5], loss:0.0646\n",
      "epoch [5/5], loss:0.0424\n",
      "epoch [1/5], loss:0.0089\n",
      "epoch [2/5], loss:0.0498\n",
      "epoch [3/5], loss:0.0373\n",
      "epoch [4/5], loss:0.0071\n",
      "epoch [5/5], loss:0.0364\n",
      "epoch [1/5], loss:0.0842\n",
      "epoch [2/5], loss:0.0568\n",
      "epoch [3/5], loss:0.0621\n",
      "epoch [4/5], loss:0.0439\n",
      "epoch [5/5], loss:0.0567\n",
      "epoch [1/5], loss:0.1139\n",
      "epoch [2/5], loss:0.0588\n",
      "epoch [3/5], loss:0.1322\n",
      "epoch [4/5], loss:0.0816\n",
      "epoch [5/5], loss:0.0785\n",
      "epoch [1/5], loss:0.0092\n",
      "epoch [2/5], loss:0.0096\n",
      "epoch [3/5], loss:0.0829\n",
      "epoch [4/5], loss:0.4997\n",
      "epoch [5/5], loss:0.0265\n",
      "epoch [1/5], loss:0.0754\n",
      "epoch [2/5], loss:0.0488\n",
      "epoch [3/5], loss:0.0832\n",
      "epoch [4/5], loss:0.0411\n",
      "epoch [5/5], loss:0.0439\n",
      "epoch [1/5], loss:0.2342\n",
      "epoch [2/5], loss:0.0796\n",
      "epoch [3/5], loss:0.0760\n",
      "epoch [4/5], loss:0.0501\n",
      "epoch [5/5], loss:0.0424\n",
      "epoch [1/5], loss:0.1184\n",
      "epoch [2/5], loss:0.0703\n",
      "epoch [3/5], loss:0.0594\n",
      "epoch [4/5], loss:0.0432\n",
      "epoch [5/5], loss:0.0636\n",
      "epoch [1/5], loss:0.0337\n",
      "epoch [2/5], loss:0.0485\n",
      "epoch [3/5], loss:0.0147\n",
      "epoch [4/5], loss:0.0370\n",
      "epoch [5/5], loss:0.0746\n",
      "epoch [1/5], loss:0.0368\n",
      "epoch [2/5], loss:0.0472\n",
      "epoch [3/5], loss:0.0241\n",
      "epoch [4/5], loss:0.0337\n",
      "epoch [5/5], loss:0.0395\n",
      "epoch [1/5], loss:0.0093\n",
      "epoch [2/5], loss:0.0450\n",
      "epoch [3/5], loss:0.0651\n",
      "epoch [4/5], loss:0.0451\n",
      "epoch [5/5], loss:0.0856\n",
      "epoch [1/5], loss:0.0777\n",
      "epoch [2/5], loss:0.0922\n",
      "epoch [3/5], loss:0.0718\n",
      "epoch [4/5], loss:0.0529\n",
      "epoch [5/5], loss:0.0502\n",
      "epoch [1/5], loss:0.0486\n",
      "epoch [2/5], loss:0.0442\n",
      "epoch [3/5], loss:0.0449\n",
      "epoch [4/5], loss:0.0921\n",
      "epoch [5/5], loss:0.0546\n",
      "epoch [1/5], loss:0.0900\n",
      "epoch [2/5], loss:0.0633\n",
      "epoch [3/5], loss:0.1056\n",
      "epoch [4/5], loss:0.0565\n",
      "epoch [5/5], loss:0.1070\n",
      "epoch [1/5], loss:0.0075\n",
      "epoch [2/5], loss:0.0413\n",
      "epoch [3/5], loss:0.0070\n",
      "epoch [4/5], loss:0.0503\n",
      "epoch [5/5], loss:0.0063\n",
      "epoch [1/5], loss:0.2638\n",
      "epoch [2/5], loss:0.2002\n",
      "epoch [3/5], loss:0.1442\n",
      "epoch [4/5], loss:0.1357\n",
      "epoch [5/5], loss:0.1662\n",
      "epoch [1/5], loss:0.0829\n",
      "epoch [2/5], loss:0.1092\n",
      "epoch [3/5], loss:0.0987\n",
      "epoch [4/5], loss:0.0843\n",
      "epoch [5/5], loss:0.0777\n",
      "epoch [1/5], loss:0.0861\n",
      "epoch [2/5], loss:0.0664\n",
      "epoch [3/5], loss:0.0812\n",
      "epoch [4/5], loss:0.0512\n",
      "epoch [5/5], loss:0.0488\n",
      "epoch [1/5], loss:0.1742\n",
      "epoch [2/5], loss:0.1135\n",
      "epoch [3/5], loss:0.0651\n",
      "epoch [4/5], loss:0.1091\n",
      "epoch [5/5], loss:0.0579\n",
      "epoch [1/5], loss:0.0830\n",
      "epoch [2/5], loss:0.0577\n",
      "epoch [3/5], loss:0.0466\n",
      "epoch [4/5], loss:0.0332\n",
      "epoch [5/5], loss:0.0429\n",
      "epoch [1/5], loss:0.1757\n",
      "epoch [2/5], loss:0.1238\n",
      "epoch [3/5], loss:0.0994\n",
      "epoch [4/5], loss:0.0522\n",
      "epoch [5/5], loss:0.0631\n",
      "epoch [1/5], loss:0.1075\n",
      "epoch [2/5], loss:0.1043\n",
      "epoch [3/5], loss:0.1060\n",
      "epoch [4/5], loss:0.0780\n",
      "epoch [5/5], loss:0.0609\n",
      "epoch [1/5], loss:0.1083\n",
      "epoch [2/5], loss:0.0731\n",
      "epoch [3/5], loss:0.0625\n",
      "epoch [4/5], loss:0.0478\n",
      "epoch [5/5], loss:0.0531\n",
      "epoch [1/5], loss:0.1033\n",
      "epoch [2/5], loss:0.0740\n",
      "epoch [3/5], loss:0.0450\n",
      "epoch [4/5], loss:0.0290\n",
      "epoch [5/5], loss:0.0728\n",
      "epoch [1/5], loss:0.0883\n",
      "epoch [2/5], loss:0.0639\n",
      "epoch [3/5], loss:0.0512\n",
      "epoch [4/5], loss:0.0550\n",
      "epoch [5/5], loss:0.0425\n",
      "epoch [1/5], loss:0.1552\n",
      "epoch [2/5], loss:0.1080\n",
      "epoch [3/5], loss:0.0783\n",
      "epoch [4/5], loss:0.0884\n",
      "epoch [5/5], loss:0.0803\n",
      "epoch [1/5], loss:0.0719\n",
      "epoch [2/5], loss:0.0569\n",
      "epoch [3/5], loss:0.0488\n",
      "epoch [4/5], loss:0.0477\n",
      "epoch [5/5], loss:0.0407\n",
      "epoch [1/5], loss:0.0743\n",
      "epoch [2/5], loss:0.0537\n",
      "epoch [3/5], loss:0.0564\n",
      "epoch [4/5], loss:0.0608\n",
      "epoch [5/5], loss:0.0377\n",
      "epoch [1/5], loss:0.0915\n",
      "epoch [2/5], loss:0.0399\n",
      "epoch [3/5], loss:0.0354\n",
      "epoch [4/5], loss:0.0267\n",
      "epoch [5/5], loss:0.0523\n",
      "epoch [1/5], loss:0.0418\n",
      "epoch [2/5], loss:0.0377\n",
      "epoch [3/5], loss:0.0406\n",
      "epoch [4/5], loss:0.0286\n",
      "epoch [5/5], loss:0.0284\n",
      "epoch [1/5], loss:0.2663\n",
      "epoch [2/5], loss:0.1342\n",
      "epoch [3/5], loss:0.1532\n",
      "epoch [4/5], loss:0.1241\n",
      "epoch [5/5], loss:0.1342\n",
      "epoch [1/5], loss:0.3266\n",
      "epoch [2/5], loss:0.2305\n",
      "epoch [3/5], loss:0.1342\n",
      "epoch [4/5], loss:0.0905\n",
      "epoch [5/5], loss:0.1660\n",
      "epoch [1/5], loss:0.1181\n",
      "epoch [2/5], loss:0.0789\n",
      "epoch [3/5], loss:0.0805\n",
      "epoch [4/5], loss:0.0567\n",
      "epoch [5/5], loss:0.0844\n",
      "epoch [1/5], loss:0.1316\n",
      "epoch [2/5], loss:0.1283\n",
      "epoch [3/5], loss:0.1171\n",
      "epoch [4/5], loss:0.0830\n",
      "epoch [5/5], loss:0.0987\n",
      "epoch [1/5], loss:0.1461\n",
      "epoch [2/5], loss:0.1051\n",
      "epoch [3/5], loss:0.0765\n",
      "epoch [4/5], loss:0.0753\n",
      "epoch [5/5], loss:0.0521\n",
      "epoch [1/5], loss:0.1493\n",
      "epoch [2/5], loss:0.0837\n",
      "epoch [3/5], loss:0.0938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [4/5], loss:0.0763\n",
      "epoch [5/5], loss:0.0926\n",
      "epoch [1/5], loss:0.1364\n",
      "epoch [2/5], loss:0.1170\n",
      "epoch [3/5], loss:0.0668\n",
      "epoch [4/5], loss:0.0574\n",
      "epoch [5/5], loss:0.0524\n",
      "epoch [1/5], loss:0.1423\n",
      "epoch [2/5], loss:0.0767\n",
      "epoch [3/5], loss:0.0704\n",
      "epoch [4/5], loss:0.0498\n",
      "epoch [5/5], loss:0.0380\n",
      "epoch [1/5], loss:0.0999\n",
      "epoch [2/5], loss:0.0839\n",
      "epoch [3/5], loss:0.0767\n",
      "epoch [4/5], loss:0.0424\n",
      "epoch [5/5], loss:0.0482\n",
      "epoch [1/5], loss:0.1578\n",
      "epoch [2/5], loss:0.0705\n",
      "epoch [3/5], loss:0.0791\n",
      "epoch [4/5], loss:0.0562\n",
      "epoch [5/5], loss:0.0477\n"
     ]
    }
   ],
   "source": [
    "trainSpectra = []\n",
    "i = 0\n",
    "for key in TrainingKeys:\n",
    "    trainSpectra.append(torch.empty_like(TrainingSamples[key].spectra))\n",
    "    for j in range(TrainingSamples[key].spectra.size()[0]):\n",
    "        trainSpectra[i][j] = TrainingSamples[key].spectrum + TrainingSamples[key].spectra[j]\n",
    "    i += 1\n",
    "\n",
    "vb = TempVB()\n",
    "for i in range(70):\n",
    "    vb.crfAi.train(trainSpectra[i], epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "directed-nirvana",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "borders = [0, 1, 2,\n",
    "           35, 36, 37,\n",
    "           40, 51, 52,\n",
    "           75, 76, 79,\n",
    "           82, 83, 86,\n",
    "           328,329, 330\n",
    "          ]\n",
    "phonemes = [\"A\", \"N\", \"A\", \"T\", \"A\"]\n",
    "#offsets = [0, 5, 1, 1, 1]\n",
    "offsets = [0, 20, 20, 0, 13]\n",
    "\n",
    "sequence = VocalSequence(0, 400, vb, borders, phonemes, offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "velvet-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "varied-egyptian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22659260f70>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAmUUlEQVR4nO3deXwU5f0H8M83gXDfZ+QKl3IJiDGACHKJQKhYqxSPFlsU8fppbamIFKtWxaNWEStSb63VSj0oBCJQbkEICsgpAaLEAAn3fYQ8vz8ySXaTPWZ3Zmd2Zj7v1yuvzM7OzjzPQr47+xzfR5RSICIi90uwuwBERGQNBnwiIo9gwCci8ggGfCIij2DAJyLyiEp2FyCUhg0bqpSUFLuLQUTkGOvWrTuglGoU6Lm4DvgpKSnIysqyuxhERI4hIj8Ee45NOkREHsGAT0TkEQz4REQewYBPROQRDPhERB7BgE9E5BEM+EREHmFKwBeRt0QkX0Q2BXleRGSaiGSLyEYR6WHGdYnIG9bmHML3+4/bXQzHM+sO/x0AQ0M8PwxAe+1nHIDXTLouEXnATTNWYcjfltldDMczJeArpZYBOBTikJEA3lPFVgOoKyLJZlybiIj0saoNvxmAPT6Pc7V9FYjIOBHJEpGsgoIC0wqwfd9xnL9QZNr5iMh63+UetbsIjmZVwJcA+wKuraiUmqmUSlVKpTZqFDD/T8R+OHgS1760DFPnbTPlfE6SuXkfPlgdNLUGkaO88OV2u4vgaFYF/FwALXweNweQZ9G1ceDEOQDAu1/lWHXJuHHX++sw+fOAfelEQaU9tRApE+fiyKlzdhelgvV7jmDI35bi1LlCu4viOFYF/NkAfq2N1ukF4KhSaq9F1y5VWKSwbd8xqy9L5Dj5x88CAB769wabS1LR0xlb8f3+E1iwZb/dRXEcs4Zl/gvAKgCXiEiuiIwVkfEiMl47JAPALgDZAP4B4B4zrqvXlryydr+hLy238tIUx3YfOImfvbICR0+ft7socUWpstbW/23Lx4WigK2vtvAtyQMfrberGI5lSj58pdTNYZ5XAO4141qROnP+Av70xWY7Lk1xbtqiHfjup6NYtHU/bujR3O7ixI3Nef7fgttOykDO1HSbSkNmcv1M27dX5thdBIpz0xbt8Lur9bqzhRVHsz0zb6sNJSGzuT7gPzvfeyNzSpw8y06tUBZqbcA5B09h5rJdNpcmfhRo7fe+Xl8aH+9P3pHTAT+QSB9XB/z/rMsNuH/N7lBzxNxj3Q+H7S5CXDvu84H4zLxtWLhlP86cv2BjieLDY7NjP6rryKlzUc2Lyc4/gQ17jphfII9wdcD//SeBRxiMen0VPl77o8WloXh3x3tZSJ/m7U797Pzj2H+s4h0+ANz1vjnrSx8+eQ7dn1iAy55YoOt4flM1j6sDfigP/+c7HDoZeIzxQx+vd90ffsrEuZgQ5APQSm+v3I2UiXORMnGureUoDHJ3ubPgpMUlKXOusAhFNo+IuWnGqqDPZW7eb8qInZKJgCd0BvIxb60xfE0q5tmADwCHA0wqKSpS+PTbn7A575jrJnZ8si4XX+08YGsZHv/vltLteG1as6tZ5+LJ89DrmUW2XLvE4VOhh6je/vYaHA1zTDgXIuwgz2LTpGlcG/AfnrUx7DGvLNpRYd9rS3eWbneakmlqmeLBLf/42u4ilBr1+io8Z1OneqiOv7HvrrWwJP7yA3SYxtqKHQcw7OXlOKejM3T5jgPo9sSXhq730sKKf3dkDdcG/I+z9oQ9Ju/IGb/HG/YcwfOZ/rk64mnSiVl2H7Cn2SLQ0Me/L9kZ4MjYKipSeHPF7qDPr8w+aGFpii3fUZYo0OrmromfbsTWvcew7+iZ8AdH4OCJs3h1cTY+//YnFBw/i4EvLAk4AigUdqKby7UBX481Of5NCq8uzq5wTNtJGfhEx4dHPJJAKesADHhhiaXlKLFtX3wsYPHrt9bgxQXfhzxm/qZ9FpWm2M78E36PJ3/+XUzb8/ccOoWUiXOxYkdZE98bK8wdenn5Xxbi+cztePDj9bjiqYXYdeAkrnhqYUTn6PCn+aaWyes8HfD1mjBrIzb95Ly0rKGyZC793rzU03rd+NpXAfdbnbZ6RXb4fozxH6wL+Xz+sTPI1oL0ybOFFQYArNp5EG0nZeBwkIEB5f3Zp28DAD5Y/SN+OHRK12ujkfVD8c3OrHVlNzPvrTInq+rG3CO6h07qfX/IHJ4P+Ho7Zke8siLGJTHXvqNnkLk5eHKpMW+twZY8axPJnTwX+Ov5ku3WffiY1Wmd9vQiDH6xOGNj58cy0ePJBcg9fAo3/H0lLpk8D68t3YkLRQobco9EfY2NuUeCjiYyqmTopYK5zSZf7TyA66avxMhXV+o6/n/b8k27NoXn+YCf9tQi7NHupL50Ufa9wqLwgeLDNfGRJ//O97KwZLv5f/g7C05gwAtLcPBEWbux2Z3Wvh37Vz27GN/8eARnC4sqNNFE44GP1uOx2bHJA1WyNsR/N+SVpg83qqhIRfz+LtpW/Df3h082oNfT9o5Q8gLPB/wTZwvR97nFeOjj9XYXxXKffvOT3UUodfvb5o+MeXbeNuw+cBL/zsrF9n3H8dm3gWdeR0rPHfFPR04DMF6vzM2x7UuItptg79HTFfb5jnDTK+O74vrNWpeLfcf8O40n/if8SDuKjOcDfolPv42f4Fdejk2jaow6evo8lFLYnHcU3++3vsO25Bvbs/O34dqXluF3H5sz8czsuQyhErcdOHEOr9kwkimc7/dX/AZTfoRbtDI378PyHQX4aG34wRILXfSt3AoM+HHmxNlCjHp9VWmQn7txL/q/sASLtpr/H/tUkDb1UA6dPIcb/r4S9334Tcjj9h87g26Pf4m/L9mJ9GkrMORvy6ItZtz57TuRpRgINBTxzRW7sXVvcR/KxjDrtMZjAsB/lEs2t8ekDuafjpzGXe+vw6/e1De79o73zEn34BUM+BGI5ZjgkrvhhVv2Y83uQ5g6bxveX5WDe7XAaseQxhNnC7Gz4ASOnjpfmg6hx5ML8M2PRzBnY+gFy/K0Jo2M7/QvbLbRQAdnLJg1EzhQf8qTc7Zg2MvLtedjP9fjxNlCvLo427R5JeVHOvV9brEp543mb2z/MXPnD7gZA34EJn36XUzOm51/HN0e/xIfrd2DB7W+hPmb9/kt3BKrttzs/OAfJF0ey8Sgvy41NLMyWCKuQMJ9iFht1OsV88pEMza+/Et2FpQ1h/x05HTMm+yOnj6PLo9l4vnM7Zi3ybz3uGSostFvn0YnfHGkj34M+BGI1SieBVuK/8M+EuIDZWPuUTwzbyvOFpr7LWPwi9E3tXy962DQu6uSGHfghP6AbzQnfebmfaa3d/sm+Dpy6hzaTMqI+Bx/Lde2/ZXP3XGfqf8LmtXV18EI3sfy3vsqp3RbT/oEvUqGKo9911izytLvjQVsrl2jHwN+BE6cLcR8E++QSuhto3196S48OWdL+ANNsDYnfHPGL2euRs8gQ+k+XmPt7OQ9h07hrvfX4dn520ydGb3v6GlcKFJ4Y/kudNeZzre8kgEB2/Ydi7rzOtzM4FBi2WS0UscktnDOXygr348HI+8LUGDE18uVAd+sDqRAxn8QurMy1j5Y/SOycg6VtpHHyjcRZCgMNIZ+2Y7oJlNFu9Sgbxvyo5+Zt4DH0xnb0HZSBv4y1/gSf0NfWo4hf1sWVXjaa6DZ42WfJIHLTJ5hfesbxuc1TP687N9Lb8pkio4rA75ZHUjB9H9+MVImzsXzmdGNnpj+vx0h0x6Ec+OMVbhy6v+ifn157R/NQMHxs3jsi01RfZAEGmseJI1PWO9H8L4Mf3l5wNFC50ycnWpW+7DRztJoOrTPXyjC8TP+qYw/X59nqByxFiz/UyhnznPJQ70q2V0AJ8rRvna+ungnJlzbIeLXv/Bl8dfzyZ9vwsRhkb/ebOcvqNKkVu+u+gFXtWuoK9+Mr9PnLqBaUiJOn7uAxIRow334IYpA8Tj4nAOnsGXvMWzZewzTb4n6cpbp9rixlMKRzIZdsGU/GtZMwuiZqz2x/uuTc7Zg7FWt7S6GIzDg26xkins8iTTYA8XpfYd0boqOU+ajXeOaUbeqhmrRyT18CmPfycL2cu3gPZ+OLAOjHXybKmLZybjuh0O4M8zY9G37rM2hRPHDlU06Vgo1rDEQu5ewi5Vsn6GG2QbyyJRMRgpk2MvLKwR7IPDQz9NRTCpzg1+8FnyJwhLnC+P3/6BE3RhIejDgGxTpsMYZy6yZJq8jd5qpnpvvP/Qw2j/bLUECflGRwvEz+jv0PlkXv2sYRNsxHep8i7fl412f4Zchj4/jUS3/3RDffQxOxyYdi8V6dE2Jt1YGX9EpVnw7Fs0OKYP/tjSi46d8EZssk2aIdpWvr7IP4Mp2DSvsfzpjK/6xXP+/dyRzI6y2PMrRXaQP7/Bd6h2dd3tmum56WQ708guCRCLQOPpdBc5MIBdItOvW3hJgCGRWzqGIgj0QeS4gK0k0w3RINwZ8i5WkgzXTibOFcbcil5HRIRNmbUQ+86PoojfJmFNwHH5sMeBbzMidbzB3vLsWI15ZYflSgbF0yxtflybSCpR73avm+SSjO3rqPE5zkW+KANvwHezUuUJUT6qEddqs2CIXJRXJzj/BBawD+HDNjxh2aTLyj59B2lNcIYoiw4DvYIdOnkP1pLJ/Qg5pc7/lOw5g5KsrdS8STuSLTTpEDsNgT9FiwCci8ggGfCIij2DAJyLyCFMCvogMFZHtIpItIhMDPN9fRI6KyHrtZ4oZ1yUiIv0Mj9IRkUQArwK4BkAugLUiMlspVX5ppuVKqRFGr0dERNEx4w4/DUC2UmqXUuocgI8AjDThvEREZCIzAn4zAL7JT3K1feX1FpENIjJPRDoHO5mIjBORLBHJKihgIiUiIrOYEfADzfYpP+XzGwCtlFLdALwC4PNgJ1NKzVRKpSqlUhs1amRC8YiICDAn4OcCaOHzuDkAv6TWSqljSqkT2nYGgMoiUjHPK0Xl/AX3pFQgotgxI+CvBdBeRFqLSBKA0QBm+x4gIk1Fy3sqImnadQ+acG1PYypZIoqE4VE6SqlCEbkPQCaARABvKaU2i8h47fkZAG4EcLeIFAI4DWC0MnvZHw86euo8mtWtZncxiMghTEmepjXTZJTbN8NnezqA6WZci8q8uGA73hhzRelj3vATUSicaetgLl0PnYhihAHfwdgqRkSRYMAnIvIILoDiYIHu78e+sxbLsw9YXhYiin8M+C6yfs8RLNqWb3cxiChOsUnHwXIP+y/ufdOMVTaVhIicgAHfwbLzT9hdBCJyEAZ8IiKPYMA3Qf6xM3YXgYgoLAZ8E6zefci2a+89ejr8QUREYMB3vHOFRXYXgYgcggHfQpwZS0R2YsC30MpsZoQmIvsw4Jvgp8P62tFve/PrGJeEiCg4BnwTPDt/W9hjzpy/YEFJiIiCY8C3SIc/zY/JeU+cLYzJeYnIfRjwYygr5xDSpy1HUQwT1y9m7hwi0onJ02LoRi23zRNztsTsGlzXloj04h1+jOwqKMtz885XOTG7zvurfojZuYnIXRjwTXTybCG+/fEwACDn4ElLrrmPaR2ISCc26Zio82OZAICM/+uL376TZXNpiIj88Q7fJL2eXlS6PXzachtLQkQUGAO+Sdi0QkTxjgGfiMgjGPCJiDyCAZ+IyCMY8ImIPIIBn4gcjyu/6cOAT0SOV3iBiwvpwYBPROQRDPhERB7BgE9E5BEM+EREHsGAT0TkEQz4ROR4i7but7sIjsCAT0SOd/DkObuL4AimBHwRGSoi20UkW0QmBnheRGSa9vxGEelhxnWJiACgMIbrRruJ4QVQRCQRwKsArgGQC2CtiMxWSvku5DoMQHvtpyeA17TfRESG7Tl0CifPFtpdDNOIANWTzF+fyowzpgHIVkrtAgAR+QjASAC+AX8kgPeUUgrAahGpKyLJSqm9JlzfTxE/6Yk8Z87GvZiz0fRwYpuGNasga/Jg089rRsBvBmCPz+NcVLx7D3RMMwAV/oVEZByAcQDQsmXLiAuTkCARv4aInG/S8A52F8E01SonxuS8ZgT8QBG2/G22nmOKdyo1E8BMAEhNTeXtOhGF9cvUFhjXr63dxYh7ZnTa5gJo4fO4OYC8KI4hIoqKCnz/SOWYEfDXAmgvIq1FJAnAaACzyx0zG8CvtdE6vQAcjUX7PRF5E7vu9DHcpKOUKhSR+wBkAkgE8JZSarOIjNeenwEgA8BwANkATgH4jdHrEhGVKFKM+HqYMu5HKZWB4qDuu2+Gz7YCcK8Z1yIiKq921cp2F8ERONOWiBzvZ90usrsIjsCAT0SOl8jh2Low4BOR4zHc68OAT0SOJ4z4ujDgE5HjJTDi68KAT0SOx3ivDwM+ETke7/D1YcAnIserxFE6ujDgE5HjtWtc0+4iOAIDPhE5nrBJRxcGfCIij2DAJyLyCAZ8i3RoWsuyay2bMMCyaxGRczDgW2Tu//W15DoJArRsUN2SaxGRszDgW+DBwe2RmCD4+WXNYn6tWXdfGfNrEJEzMeBboEW94jvuJ6/vYvq5HxjU3u9xj5b1TL8GEbkDA74F0rsmAwBqVjFlvRk/HI1GRHox4FugauXEmJ2bU8qJSC8GfIfr3qKu3UUgIodgwI+xGbf1iOn501rXj+n5icg9zG9UplIb/zyEiysTUdzgHX4MMdgTUTxhwHe4KpX4T0hE+jBaxMjLo7tbch1mCSQivRjwTbBy4kDMf9A/dcLI7rGfVduifrWYX4OI3IOdtiZoVrcaAOuDr1KWX5KIHIx3+CZq3bCGpddjwCeiSPAO30T/vKMnvlifh/FXt7G7KEREFTDgm+iiutVwd/+2ll2vTSP/bxRP/dz85GxE5B5s0nGwcf38v0kkJfKfk4iCY4RwsPo1kvweJ3FMPhGFwAjhIoM7NrG7CEQUxxjwHUzgP+kqMYGTsIgoOAZ8B+MkWyKKBAO+i/ADgIhCMTQsU0TqA/gYQAqAHACjlFKHAxyXA+A4gAsACpVSqUauS8VKAvyqRwZize5DqFIpditrEZHzGb3DnwhgkVKqPYBF2uNgBiilujPYm6ekDT+5TjVLcvcQkbMZDfgjAbyrbb8L4HqD56MIsAmHiCJhNOA3UUrtBQDtd+MgxykAX4rIOhEZF+qEIjJORLJEJKugoMBg8YiIqETYNnwRWQigaYCnHo3gOn2UUnki0hjAAhHZppRaFuhApdRMADMBIDU1lenBQuANPhFFImzAV0oNDvaciOwXkWSl1F4RSQaQH+QcedrvfBH5DEAagIABn/Rjkw4RRcJok85sAGO07TEAvih/gIjUEJFaJdsAhgDYZPC6BKB5vep2F4GIHMRowJ8K4BoR2QHgGu0xROQiEcnQjmkCYIWIbACwBsBcpdR8g9d1rFt7tjTtXFUrcxgmEelnaBy+UuoggEEB9ucBGK5t7wLQzch13OTyVvXwz69/tLsYRORBnGlLRFF75oZL7S4CRYABn4iiNrRzoAF8FK8Y8IkoKpPTO6JejSR8+6dr7C4K6cSAb9Afh14S0fEcSuk8s+/rgyV/6I+mtauid5sGdhcnboy9qjUAoF65hXgofjHgG3RP/3YRHT/80mS/xxumDInquikNQg/JvP3KlKjOSxV1bV4XKQ1rYPWkQfjXuF52FyduiM/dy0d8XxyBAT9KPVvXx+I/9I/4db4ZLd8ck4o61StHfI6Jwzpg1t1XhjxmwrWRffMo8cW9ffDo8I7od3GjqF5P3tS8XrWoX5vaqp6JJaFQGPCj9PyN3dC6YY2oXvv27VdgxcMDMEhbkjDSpQnHX90WDWtWCXlMpDkp7uzbGokJgm4t6uLOfm3w1hj3JDWdNLxD1K/t3qJuhX33DmhroDTuZCQ1d+VEhiGr8J2OUsswTSqhDOjQ2G+W7K96tzKjSIY8mt4JO58eXvq4kov+CJvUrhr1ax+/rnOFfROujfwD5MbLm0ddBidoVKsK3vnNFRiV6u56Op17/qottOOpYaaer3UD/d8UsiYHTW3kRyn99/hsfw2uYa3A36Se+nkXjOx+ke7z9GjprmaLOfdfVWFf/0sa47kbu2H3M8PR/xL9TYJ1o2jWpOgw4EfB7K+gLRtUx3d/Dt95O2t877BNOdHoFWTkye5nhgfcr8fvBl8c9WvNMKJrMt74dXGzVJ92DaM+T7O6gdumb+3ZCi+PviziUVpA8d2w03VpVifocyKCd36ThnWTB2P3M8Ox6fFrgx7bt31DPGjz/xUvYcCP0DcxGnNcq2pl5ExNx3M3dg34/P0D2yE1pb7u85mRV1pEkDM1HeunhK/zwA7+SyHUrmYoa4dhU0Z0wuBOTZAzNT3qD8lx/dqEPeae/u3QKbk2gOIO77v7V2zfn3Hb5X6PvTIyt0HNKhAR1KxSqfR96dbc/4NiSOemqFKJYcgqfKcjVD/GY45HpbbA0gn9Sx/veGoYcqam4/dDoht1Y4a61ZOQMzUdt2iJ35rXq4ZHhvm3Y1/XTX/zhhUaG2i3LzFpeEddxz13Y1f0v6QROibXxsNDO2Byuv/rLm5SE8rnI9iLczEeGNQe9w9sh4ds/H9MDPgRKZloEmutGtRAlUoJaFizStTNRzWS9N1hv/TL7rrPOWVEJ7w/Ng3LJgzAL69o4fdczSqV0LJ+WUe0kVEb/76rN7Y9ORQdtTvneNelWR2885s0JGl3qrf18u+ElziO8BfVMf7BqEfVyon4/ZBL0K+9f/Pa5S3rmfJtlPRhwI9AUxPuGvXa9Pi1WP3IwKhfn5ggqFU1fNC//jL9i59XrZyIvu0bISFBULd62Tedu/u3xaCOjf2ul9Iw+lFMaa3ro2rlRPz3vj5Y8fAAXBqivTgelU9bHW6SXCR2PzPc1E5O39FinS+K/Qds+Q+/ThZck8ow4EcgMcG6O7XKiQmGh0aO6Joc/iADcqamI2dqOh4e2gEigj+N6GTq+SslJqB5veqOH0VUPsj1vzjY0s/6zrU+ytnZgU9YtvnnAENQA3lgUHvzrl9O1+bO+nB3Ggb8CNxi4uIlVjA7AIfTsanxu7UxAeYk1KgSWQdwqA86q1JOPDGyOHi+PLo7AGBk97JvUhc3rWX4/KsMfPvz/cZRObEs4tfU+T4/ONhYwP/L9V38HletXBaGfN8nMh8DfgSctsJUdZ3t+GaJJk1EeUNMSLc79ReBRzoBFXMZxcqve6cgZ2p6aQDzDabRrnqW7lP25DrV8P7YtKjO85+7r8Sn9xSn5ph+cw+/58ZfHXoW8cKHrjbcJ3Frz5Z4dHhHbNSGIifXiT4tA0WGAZ9iol2jmlG9LtiY+Sk6v63Mf7BvyDvVtNb1kTM13dbkctEOQ3z1Vv/g3Ld9dPmOGtSsgh4t6yFnarpfpksR4A9DQo+Jb9c4un9XXyKCO/u1Qe2qnHBlNVcG/A/v6Bnxa3KmpsegJN7VuHZV5ExNL23S0CPUuPdrOoXON/Ty6O5Y+FA/dAjSrDT9lsv8chb5jigKJJJZtJEyc9TOiocHmHauSgkSst9o2s2XmXYtsoe9s2Ni5EoDMyvJXGmt9U8W69Um+LEt6lcv/VBemX0At77xNabfchk6NK2NakmJQWfElhjR9SKM6Ko/iNs9U1gv31E20Vr76GDM3ZiHdo2L+xY6JtfG1r3HKhwXb3MtKHKuDPiR+uye0KmG3eoXPcxPdDX1hkvRyic3UCTts6105hTq064hsiYPNpRmYnDHJnhizpaAz214bAjqVPNOc0OjWlVwe5+yOSZ1AsyStmoqQT3m1YkpVzbpROoyLbFVqBzyVoxRttp1MWi2GJ3WEr3b+ufmeS5EJ6qvthG0+xvNKRQq26lVwT6StNhv3R48XXVaBCk3ZtzWI+wxb465osK+3c9Y0+TZvrHxEUwUnOcD/tYnhpZujy43e9RXJDNSneDqixuhr0VNX2aM3rHKYz+zZijrqkcGYvot+tvEQyUre0U7z8xfXY4btIl0ka6x4CvSYbDkHJ4P+NWSyoZahupMa9/EXXce7/42DQkWTSS7pmOTsKM/yufmsYtVo3eS61RD1cqJWKcz3XWobx1NtA7yIZ2b4q+jumHxH/rjjTGppfMRftvHWEqQaFdPi0YSE6nFFN9dHzWqBB5nzxE8xiQkCO4bGHqyTvncPHYY1qVpTPPeLP/jAL/EeJHQm5tIREpXYps2+jJkPzXML5NppDmOBndsjHsHRLZusxGXRDEpbUAEufe9jgHfR6A/BjcG+ytS4msxjvsHtvPLzWOVm9P8J0D9dVS3mF6vRf3qFTqm68Ww3gnaMMur2jfErPG9cWXbBroXJlnwu34AYFmW1tn39YloCK+vSPJBeR0DfjmdHJKh0Yhx/exZk7VBkNTSdqZ+LrFswgDLZyYDxUF5yxPBFwgBgNWPDDJ8ndSU+vjwzl66v8G0b1ILOVPTLctY2rV5XaZVsAADfjmz7u5tdxFiLtwkplhpZWLWSDNNHNbB0BrFRoX7oGlqUQpjpzLSQe01ng74V7atuLRf9aRKWP7HAXhwcHtDS/zFK99kWVYrnxoAAO4JsEKUVe6+ui06JdfGL1Pt7z8IZtZ499+AlDfjth66O4rvH9iOo4oi4Ol3amKQkSEt6ld37TqbwZZQtEKgSViRzMQ1W8sG1ZHxQF/brh/O6CtaRLSspVsM7VI8uuj5zO02l8R9XHuHX37tzPJm/upydG1e15rCxBG7FxOppA0FXT/lGvz+movRL8oEYF5g5XDIeMTcPeZzbcD/8M7gi2Y0q1vNlDS8TnJFSj28OKpbab4Uu2x+4lpseeJa1K2ehPsHtbdsLkC8C5SWuIHB2cROp2fFthYm5BLyEtc26YRq17ujrzVr08aDyomC8xcUPhkfH/mCjKx162aNa/kHdzcOB46Fm1LNzwflZq69wweKx/YG8qteFVdVcqs1kwZj5cToV0cia/jmH9pg5hKGDlZJx7e/eF4gPh659g4fQNA2eqNrxTpJvRpJiK9pVhRIx+TaqFOtMgZ1bOyo3EOx1Kct05ybzVDkE5GbRGSziBSJSNB0fiIyVES2i0i2iEw0ck0it9rw2BC8OKq73cWIG+zfMZ/RW91NAG4AsCzYASKSCOBVAMMAdAJws4hYu7o2ETmSF+chxJKhJh2l1FYgbDtaGoBspdQu7diPAIwEEHj1CSIiTbB5CC/c1A1HT5+3uDTOZ0VjdjMAe3we52r7AhKRcSKSJSJZBQUFMS8cEcW3J6/v4vc4qVICbry8OcZe5Z3RdmYJG/BFZKGIbArwM1LnNQLd/qtgByulZiqlUpVSqY0aGZ+Us/ZR/3zjc+6/yvA5icg6t/VsGf4g0iVsk45SSt8KDcHlAvBNVtIcQJ7Bc+rWqFYVLJ3QH1c/vwR3Xd0m5MpBRBR/OPTSPFYMy1wLoL2ItAbwE4DRAG6x4LqlWjWogTWTBnl+5iIReZvRYZk/F5FcAL0BzBWRTG3/RSKSAQBKqUIA9wHIBLAVwL+VUpuNFTtyjWtXRSKHeRE5np4JWRSY0VE6nwH4LMD+PADDfR5nAMgwci0iIgD49J74SBPiRN6ZckpEjjf80qbo0NT9q9LFCgM+ETlGmgfXBzATAz4ROcaYK1PsLoKjMeATkWNwiKYxDPhERB7BgE9E5BEM+EREHsGAT0TkEQz4REQewYBPROQRDPhERB7BgE9E5BEM+EREHmFFPnwiIkP+elM3NKtXze5iOB4DPhHFvV9c3tzuIrgCm3SIiDyCAZ+IyCMY8ImIPIIBn4jIIxjwiYg8ggGfiMgjGPCJiDyCAZ+IyCNEKWV3GYISkQIAP0T58oYADphYnHjmpboCrK+beamuQGzq20op1SjQE3Ed8I0QkSylVKrd5bCCl+oKsL5u5qW6AtbXl006REQewYBPROQRbg74M+0ugIW8VFeA9XUzL9UVsLi+rm3DJyIif26+wyciIh8M+EREHuG6gC8iQ0Vku4hki8hEu8sTioi8JSL5IrLJZ199EVkgIju03/V8nntEq9d2EbnWZ//lIvKd9tw0ERFtfxUR+Vjb/7WIpPi8Zox2jR0iMsai+rYQkcUislVENovIA26ts4hUFZE1IrJBq+vjbq2rzzUTReRbEZnjgbrmaOVcLyJZjqmvUso1PwASAewE0AZAEoANADrZXa4Q5e0HoAeATT77ngMwUdueCOBZbbuTVp8qAFpr9UzUnlsDoDcAATAPwDBt/z0AZmjbowF8rG3XB7BL+11P265nQX2TAfTQtmsB+F6rl+vqrJWrprZdGcDXAHq5sa4+dX4IwIcA5njg/3IOgIbl9sV9fS0Lblb8aG9cps/jRwA8Yne5wpQ5Bf4BfzuAZG07GcD2QHUBkKnVNxnANp/9NwN43fcYbbsSimf0ie8x2nOvA7jZhrp/AeAat9cZQHUA3wDo6da6AmgOYBGAgSgL+K6sq3adHFQM+HFfX7c16TQDsMfnca62z0maKKX2AoD2u7G2P1jdmmnb5ff7vUYpVQjgKIAGIc5lGe0r6mUovvN1ZZ21Jo71APIBLFBKubauAF4C8EcART773FpXAFAAvhSRdSIyTtsX9/V12yLmEmCfW8adBqtbqDpH85qYE5GaAP4D4EGl1DGt2TLgoQH2OabOSqkLALqLSF0An4lIlxCHO7auIjICQL5Sap2I9NfzkgD7HFFXH32UUnki0hjAAhHZFuLYuKmv2+7wcwG08HncHECeTWWJ1n4RSQYA7Xe+tj9Y3XK17fL7/V4jIpUA1AFwKMS5Yk5EKqM42P9TKfWpttvVdVZKHQGwBMBQuLOufQBcJyI5AD4CMFBEPoA76woAUErlab/zAXwGIA1OqG+s27qs/EHxN5ZdKO4YKem07Wx3ucKUOQX+bfjPw7/j5zltuzP8O352oazjZy2KOwRLOn6Ga/vvhX/Hz7+17foAdqO406eetl3fgroKgPcAvFRuv+vqDKARgLradjUAywGMcGNdy9W7P8ra8F1ZVwA1ANTy2f4KxR/mcV/fmP8HsPoHwHAUj/7YCeBRu8sTpqz/ArAXwHkUf3KPRXE73SIAO7Tf9X2Of1Sr13Zovfna/lQAm7TnpqNsBnVVAJ8AyEbxaIA2Pq/5rbY/G8BvLKrvVSj++rkRwHrtZ7gb6wygK4BvtbpuAjBF2++6upard3+UBXxX1hXFowA3aD+bocUZJ9SXqRWIiDzCbW34REQUBAM+EZFHMOATEXkEAz4RkUcw4BMReQQDPhGRRzDgExF5xP8D7z019h0YZoIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(sequence.synth.returnSignal.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-crime",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(vb.crfAi.state_dict(), \"CrossfadeWeights.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"model = TheModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-snapshot",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

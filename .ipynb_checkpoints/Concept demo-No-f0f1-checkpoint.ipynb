{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "monetary-bulgaria",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jokla\\anaconda3\\lib\\site-packages\\torchaudio\\extension\\extension.py:13: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "torchaudio.set_audio_backend(\"soundfile\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "occasional-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioSample:\n",
    "    def __init__(self, filepath):\n",
    "        loadedData = torchaudio.load(filepath)\n",
    "        self.waveform = loadedData[0][0]\n",
    "        self.sampleRate = loadedData[1]\n",
    "        del loadedData\n",
    "        self.pitchDeltas = torch.tensor([], dtype = int)\n",
    "        self.pitchBorders = torch.tensor([], dtype = int)\n",
    "        self.pitch = torch.tensor([0], dtype = int)\n",
    "        self.spectra = torch.tensor([[]], dtype = float)\n",
    "        self.spectrum = torch.tensor([], dtype = float)\n",
    "        self.excitation = torch.tensor([], dtype = float)\n",
    "        self.voicedExcitation = torch.tensor([], dtype = float)\n",
    "        self.VoicedExcitations = torch.tensor([], dtype = float)\n",
    "        \n",
    "    def CalculatePitch(self, expectedPitch, searchRange = 0.2):\n",
    "        batchSize = math.floor((1. + searchRange) * self.sampleRate / expectedPitch)\n",
    "        lowerSearchLimit = math.floor((1. - searchRange) * self.sampleRate / expectedPitch)\n",
    "        batchStart = 0\n",
    "        while batchStart + batchSize <= self.waveform.size()[0] - batchSize:\n",
    "            sample = torch.index_select(self.waveform, 0, torch.linspace(batchStart, batchStart + batchSize, batchSize, dtype = int))\n",
    "            zeroTransitions = torch.tensor([], dtype = int)\n",
    "            for i in range(lowerSearchLimit, batchSize):\n",
    "                if (sample[i-1] < 0) and (sample[i] > 0):\n",
    "                    zeroTransitions = torch.cat([zeroTransitions, torch.tensor([i])], 0)\n",
    "            error = math.inf\n",
    "            delta = math.floor(self.sampleRate / expectedPitch)\n",
    "            for i in zeroTransitions:\n",
    "                shiftedSample = torch.index_select(self.waveform, 0, torch.linspace(batchStart + i.item(), batchStart + batchSize + i.item(), batchSize, dtype = int))\n",
    "                newError = torch.sum(torch.pow(sample - shiftedSample, 2))\n",
    "                if error > newError:\n",
    "                    delta = i.item()\n",
    "                    error = newError\n",
    "            self.pitchDeltas = torch.cat([self.pitchDeltas, torch.tensor([delta])])\n",
    "            batchStart += delta\n",
    "        nBatches = self.pitchDeltas.size()[0]\n",
    "        self.pitchBorders = torch.zeros(nBatches + 1, dtype = int)\n",
    "        for i in range(nBatches):\n",
    "            self.pitchBorders[i+1] = self.pitchBorders[i] + self.pitchDeltas[i]\n",
    "        self.pitch = torch.mean(self.pitchDeltas.float()).int()\n",
    "        del batchSize\n",
    "        del lowerSearchLimit\n",
    "        del batchStart\n",
    "        del sample\n",
    "        del zeroTransitions\n",
    "        del error\n",
    "        del delta\n",
    "        del shiftedSample\n",
    "        del newError\n",
    "        del nBatches\n",
    "        \n",
    "    def CalculateSpectra(self, iterations = 10, filterWidth = 10, preIterations = 2):\n",
    "        tripleBatchSize = int(self.sampleRate / 25)\n",
    "        BatchSize = int(self.sampleRate / 75)\n",
    "        Window = torch.hann_window(tripleBatchSize)\n",
    "        signals = torch.stft(self.waveform, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, return_complex = True, onesided = True)\n",
    "        signals = torch.transpose(signals, 0, 1)\n",
    "        signalsAbs = signals.abs()\n",
    "        \n",
    "        workingSpectra = torch.sqrt(signalsAbs)\n",
    "        \n",
    "        workingSpectra = torch.max(workingSpectra, torch.tensor([-100]))\n",
    "        self.spectra = torch.full_like(workingSpectra, -float(\"inf\"), dtype=torch.float)\n",
    "        \n",
    "        for j in range(preIterations):\n",
    "            workingSpectra = torch.max(workingSpectra, self.spectra)\n",
    "            self.spectra = workingSpectra\n",
    "            for i in range(filterWidth):\n",
    "                self.spectra = torch.roll(workingSpectra, -i, dims = 1) + self.spectra + torch.roll(workingSpectra, i, dims = 1)\n",
    "            self.spectra = self.spectra / (2 * filterWidth + 1)\n",
    "        \n",
    "        self.VoicedExcitations = torch.zeros_like(signals)\n",
    "        for i in range(signals.size()[0]):\n",
    "            for j in range(signals.size()[1]):\n",
    "                if torch.sqrt(signalsAbs[i][j]) > self.spectra[i][j]:\n",
    "                    self.VoicedExcitations[i][j] = signals[i][j]\n",
    "                \n",
    "        for j in range(iterations):\n",
    "            workingSpectra = torch.max(workingSpectra, self.spectra)\n",
    "            self.spectra = workingSpectra\n",
    "            for i in range(filterWidth):\n",
    "                self.spectra = torch.roll(workingSpectra, -i, dims = 1) + self.spectra + torch.roll(workingSpectra, i, dims = 1)\n",
    "            self.spectra = self.spectra / (2 * filterWidth + 1)\n",
    "        \n",
    "        self.spectrum = torch.mean(self.spectra, 0)\n",
    "        for i in range(self.spectra.size()[0]):\n",
    "            self.spectra[i] = self.spectra[i] - self.spectrum\n",
    "        #return torch.log(signalsAbs)\n",
    "        del Window\n",
    "        del signals\n",
    "        del workingSpectra\n",
    "        \n",
    "    def CalculateExcitation(self, filterWidth = 10):\n",
    "        tripleBatchSize = int(self.sampleRate / 25)\n",
    "        BatchSize = int(self.sampleRate / 75)\n",
    "        Window = torch.hann_window(tripleBatchSize)\n",
    "        signals = torch.stft(self.waveform, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, return_complex = True, onesided = True)\n",
    "        signals = torch.transpose(signals, 0, 1)\n",
    "        excitations = torch.empty_like(signals)\n",
    "        for i in range(excitations.size()[0]):\n",
    "            excitations[i] = signals[i] / torch.square(self.spectrum + self.spectra[i])\n",
    "            self.VoicedExcitations[i] = self.VoicedExcitations[i] / torch.square(self.spectrum + self.spectra[i])\n",
    "        \n",
    "        VoicedExcitations = torch.transpose(self.VoicedExcitations, 0, 1)\n",
    "        excitations = torch.transpose(excitations, 0, 1)\n",
    "        self.excitation = torch.istft(excitations, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, onesided = True)\n",
    "        self.voicedExcitation = torch.istft(VoicedExcitations, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, onesided = True)\n",
    "        \n",
    "        self.excitation = self.excitation - self.voicedExcitation\n",
    "        self.excitation = torch.stft(self.excitation, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, return_complex = True, onesided = True)\n",
    "        #self.voicedExcitation = torch.stft(self.voicedExcitation, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, return_complex = True, onesided = True)\n",
    "        self.excitation = torch.transpose(self.excitation, 0, 1)\n",
    "        #self.voicedExcitation = torch.transpose(self.voicedExcitation, 0, 1)\n",
    "        \n",
    "        del Window\n",
    "        del signals\n",
    "        del excitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "expired-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(RelLoss, self).__init__()\n",
    " \n",
    "    def forward(self, inputs, targets):    \n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        differences = torch.abs(inputs - targets)\n",
    "        sums = torch.abs(inputs + targets)\n",
    "        out = (differences / sums).sum() / inputs.size()[0]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "configured-craps",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecCrfAi(nn.Module):\n",
    "    def __init__(self, learningRate=1e-4):\n",
    "        super(SpecCrfAi, self).__init__()\n",
    "        \n",
    "        self.layer1 = torch.nn.Linear(3843, 3843)\n",
    "        self.ReLu1 = nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(3843, 5763)\n",
    "        self.ReLu2 = nn.ReLU()\n",
    "        self.layer3 = torch.nn.Linear(5763, 3842)\n",
    "        self.ReLu3 = nn.ReLU()\n",
    "        self.layer4 = torch.nn.Linear(3842, 1921)\n",
    "        \n",
    "        self.learningRate = learningRate\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learningRate, weight_decay=0.)\n",
    "        self.criterion = nn.L1Loss()\n",
    "        #self.criterion = RelLoss()\n",
    "        \n",
    "    def forward(self, spectrum1, spectrum2, factor):\n",
    "        fac = torch.tensor([factor])\n",
    "        x = torch.cat((spectrum1, spectrum2, fac), dim = 0)\n",
    "        x = x.float()#.unsqueeze(0).unsqueeze(0)\n",
    "        x = self.layer1(x)\n",
    "        x = self.ReLu1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.ReLu2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.ReLu3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "    \n",
    "    def processData(self, spectrum1, spectrum2, factor):\n",
    "        output = torch.square(torch.squeeze(self(torch.sqrt(spectrum1), torch.sqrt(spectrum2), factor)))\n",
    "        return output\n",
    "    \n",
    "    def train(self, indata, epochs=1):\n",
    "        for epoch in range(epochs):\n",
    "            for data in self.dataLoader(indata):\n",
    "                spectrum1 = data[0]\n",
    "                spectrum2 = data[-1]\n",
    "                indexList = np.arange(0, data.size()[0], 1)\n",
    "                np.random.shuffle(indexList)\n",
    "                for i in indexList:\n",
    "                    factor = i / float(data.size()[0])\n",
    "                    spectrumTarget = data[i]\n",
    "                    output = torch.squeeze(self(spectrum1, spectrum2, factor))\n",
    "                    loss = self.criterion(output, spectrumTarget)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "            print('epoch [{}/{}], loss:{:.4f}'\n",
    "                  .format(epoch + 1, epochs, loss.data))\n",
    "            \n",
    "    def dataLoader(self, data):\n",
    "        return torch.utils.data.DataLoader(dataset=data, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "black-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocalSegment:\n",
    "    def __init__(self, start1, start2, start3, end1, end2, end3, startCap, endCap, phonemeKey, vb, offset, repetititionSpacing, pitch, steadiness):\n",
    "        self.start1 = start1\n",
    "        self.start2 = start2\n",
    "        self.start3 = start3\n",
    "        self.end1 = end1\n",
    "        self.end2 = end2\n",
    "        self.end3 = end3\n",
    "        self.startCap = startCap\n",
    "        self.endCap = endCap\n",
    "        self.phonemeKey = phonemeKey\n",
    "        self.vb = vb\n",
    "        self.offset = offset\n",
    "        self.repetititionSpacing = repetititionSpacing\n",
    "        self.pitch = pitch\n",
    "        self.steadiness = steadiness\n",
    "        \n",
    "    def phaseShift(inputTensor, pitch, phase):\n",
    "        absolutes = inputTensor.abs()\n",
    "        phases = inputTensor.angle()\n",
    "        phaseOffsets = torch.full(phases.size(), phase / pitch)\n",
    "        phaseOffsets *= torch.arange(phases.size()[0])\n",
    "        phases += phaseOffsets\n",
    "        phases = torch.fmod(phases, 2 * math.pi)\n",
    "        return torch.polar(absolutes, phases)\n",
    "        \n",
    "    def loopSamplerVoicedExcitation(self, inputTensor, targetSize, repetititionSpacing, pitch = None):\n",
    "        batchRS = math.ceil(repetititionSpacing/BatchSize)\n",
    "        BatchSize = int(self.vb.sampleRate / 75)\n",
    "        tripleBatchSize = int(self.vb.sampleRate / 25)\n",
    "        window = torch.hann_window(tripleBatchSize)\n",
    "        if pitch == None:\n",
    "            pitch = torch.argmax(inputTensor[int(inputTensor.size()[0] * 0.5)])\n",
    "        alignPhase = inputTensor[batchRS][pitch].angle()\n",
    "        finalPhase = inputTensor[1][pitch].angle()\n",
    "        phaseShift = finalPhase - alignPhase\n",
    "        requiredTensors = math.ceil((targetSize + batchRS) / inputTensor.size()[0])\n",
    "        \n",
    "        if requiredTensors == 1:\n",
    "            outputTensor = inputTensor\n",
    "            outputTensor = torch.transpose(outputTensor, 0, 1)\n",
    "            outputTensor = torch.istft(outputTensor, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = window, onesided = True, length = length*BatchSize)\n",
    "        else:\n",
    "            outputTensor = torch.zeros(requiredTensors * (inputTensor.size()[0] - repetititionSpacing) + repetititionSpacing)\n",
    "            \n",
    "            workingTensor = inputTensor\n",
    "            workingTensor = torch.transpose(workingTensor, 0, 1)\n",
    "            workingTensor = torch.istft(workingTensor, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = window, onesided = True, length = length*BatchSize)\n",
    "            workingTensor[-repetititionSpacing:] *= torch.linspace(1, 0, repetititionSpacing)\n",
    "            outputTensor[0:inputTensor.size()[0]] += workingTensor\n",
    "            \n",
    "            for i in range(1, requiredTensors - 1):\n",
    "                workingTensor = torch.transpose(inputTensor, 0, 1)\n",
    "                workingTensor = phaseShift(workingTensor, pitch, i * phaseShift)\n",
    "                workingTensor = torch.istft(workingTensor, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = window, onesided = True, length = length*BatchSize)\n",
    "                workingTensor[0:repetititionSpacing] *= torch.linspace(0, 1, repetititionSpacing)\n",
    "                workingTensor[-repetititionSpacing:] *= torch.linspace(1, 0, repetititionSpacing)\n",
    "                outputTensor[i * (inputTensor.size()[0] - repetititionSpacing):i * (inputTensor.size()[0] - repetititionSpacing) + inputTensor.size()[0]] += workingTensor\n",
    "            \n",
    "            workingTensor = torch.transpose(inputTensor, 0, 1)\n",
    "            workingTensor = phaseShift(inputTensor, pitch, (requiredTensors - 1) * phaseShift)\n",
    "            workingTensor = torch.istft(workingTensor, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = window, onesided = True, length = length*BatchSize)\n",
    "            workingTensor[0:repetititionSpacing] *= torch.linspace(0, 1, repetititionSpacing)\n",
    "            outputTensor[(requiredTensors - 1) * (inputTensor.size()[0] - repetititionSpacing):] += workingTensor\n",
    "        return outputTensor[0:targetSize]\n",
    "    \n",
    "    def loopSamplerSpectrum(self, inputTensor, targetSize, repetititionSpacing):\n",
    "        repetititionSpacing = math.ceil(repetititionSpacing / int(self.vb.sampleRate / 75))\n",
    "        requiredTensors = math.ceil((targetSize + repetititionSpacing) / inputTensor.size()[0])\n",
    "        if requiredTensors == 1:\n",
    "            outputTensor = inputTensor\n",
    "        else:\n",
    "            outputTensor = torch.zeros(requiredTensors * (inputTensor.size()[0] - repetititionSpacing) + repetititionSpacing)\n",
    "            \n",
    "            workingTensor = inputTensor\n",
    "            workingTensor[-repetititionSpacing:] *= torch.linspace(1, 0, repetititionSpacing)\n",
    "            outputTensor[0:inputTensor.size()[0]] += workingTensor\n",
    "            \n",
    "            for i in range(1, requiredTensors - 1):\n",
    "                workingTensor = inputTensor\n",
    "                workingTensor[0:repetititionSpacing] *= torch.linspace(0, 1, repetititionSpacing)\n",
    "                workingTensor[-repetititionSpacing:] *= torch.linspace(1, 0, repetititionSpacing)\n",
    "                outputTensor[i * (inputTensor.size()[0] - repetititionSpacing):i * (inputTensor.size()[0] - repetititionSpacing) + inputTensor.size()[0]] += workingTensor\n",
    "            \n",
    "            workingTensor = inputTensor\n",
    "            workingTensor[0:repetititionSpacing] *= torch.linspace(0, 1, repetititionSpacing)\n",
    "            outputTensor[(requiredTensors - 1) * (inputTensor.size()[0] - repetititionSpacing):] += workingTensor\n",
    "        return outputTensor[0:targetSize]\n",
    "    \n",
    "    def getSpectrum(self):\n",
    "        if self.startCap:\n",
    "            windowStart = self.offset\n",
    "        else:\n",
    "            windowStart = self.start3 - self.start1 + self.offset\n",
    "        if self.endCap:\n",
    "            windowEnd = self.end3 - self.start1 + self.offset\n",
    "        else:\n",
    "            windowEnd = self.end1 - self.start1 + self.offset\n",
    "        spectrum =  self.vb.phonemeDict[self.phonemeKey].spectrum#implement looping\n",
    "        spectra =  self.vb.phonemeDict[self.phonemeKey].spectra[windowStart:windowEnd]\n",
    "        #spectra = self.loopSamplerSpectrum(self.vb.phonemeDict[self.phonemeKey].spectra, windowEnd, self.repetititionSpacing)[windowStart:windowEnd]\n",
    "        return torch.square(spectrum + (math.pow(1 - self.steadiness, 2) * spectra))\n",
    "    \n",
    "    def getExcitation(self):\n",
    "        BatchSize = int(self.vb.sampleRate / 75)\n",
    "        tripleBatchSize = int(self.vb.sampleRate / 25)\n",
    "        premul = self.vb.phonemeDict[self.phonemeKey].excitation.size()[0] / (self.end3 - self.start1 + 1)\n",
    "        #premul = 1\n",
    "        if self.startCap:\n",
    "            windowStart = 0\n",
    "            length = -self.start1\n",
    "        else:\n",
    "            windowStart = math.floor((self.start2 - self.start1) * premul)\n",
    "            length = -self.start2\n",
    "        if self.endCap:\n",
    "            windowEnd = math.ceil((self.end3 - self.start1) * premul)\n",
    "            length += self.end3\n",
    "        else:\n",
    "            windowEnd = math.ceil((self.end2 - self.start1) * premul)\n",
    "            length += self.end2\n",
    "        excitation = self.vb.phonemeDict[self.phonemeKey].excitation[windowStart:windowEnd]\n",
    "        excitation = torch.transpose(excitation, 0, 1)\n",
    "        transform = torchaudio.transforms.TimeStretch(hop_length = int(self.vb.sampleRate / 75),\n",
    "                                                      n_freq = int(self.vb.sampleRate / 25 / 2) + 1, \n",
    "                                                      fixed_rate = premul)\n",
    "        excitation = transform(torch.view_as_real(excitation))\n",
    "        excitation = torch.view_as_complex(excitation)\n",
    "        window = torch.hann_window(int(self.vb.sampleRate / 25))\n",
    "        excitation = torch.istft(excitation, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = window, onesided = True, length = length*int(self.vb.sampleRate / 75))\n",
    "        \n",
    "        return excitation[0:length*int(self.vb.sampleRate / 75)]\n",
    "    \n",
    "    def getVoicedExcitation(self):\n",
    "        nativePitch = self.vb.phonemeDict[self.phonemeKey].pitch\n",
    "        #nativePitch = self.vb.phonemeDict[self.phonemeKey].pitches[...]\n",
    "        #pitch = nativePitch + self.vb.phonemeDict[phonemeKey].pitches...\n",
    "        premul = self.pitch / nativePitch * self.vb.sampleRate / 75\n",
    "        windowStart = math.floor(self.offset * self.vb.sampleRate / 75)\n",
    "        windowEnd = math.ceil((self.end3 - self.start1) * premul + (self.offset * self.vb.sampleRate / 75))\n",
    "        #windowStart = math.floor(self.offset)\n",
    "        #windowEnd = math.ceil((self.end3 - self.start1) * premul + self.offset)\n",
    "        voicedExcitation = self.vb.phonemeDict[self.phonemeKey].voicedExcitation[windowStart:windowEnd]\n",
    "        #voicedExcitation = self.loopSamplerVoicedExcitation(self.vb.phonemeDict[self.phonemeKey].voicedExcitation, windowEnd, self.repetititionSpacing)[windowStart:windowEnd]\n",
    "        transform = torchaudio.transforms.Resample(orig_freq = nativePitch,\n",
    "                                                   new_freq = self.pitch,\n",
    "                                                   resampling_method = 'sinc_interpolation')\n",
    "        voicedExcitation = transform(voicedExcitation)\n",
    "        if self.startCap == False:\n",
    "            slope = torch.linspace(0, 1, (self.start3 - self.start1) * int(self.vb.sampleRate / 75))\n",
    "            voicedExcitation[0:(self.start3 - self.start1) * int(self.vb.sampleRate / 75)] *= slope\n",
    "        if self.endCap == False:\n",
    "            slope = torch.linspace(1, 0, (self.end3 - self.end1) * int(self.vb.sampleRate / 75))\n",
    "            voicedExcitation[(self.end1 - self.start1) * int(self.vb.sampleRate / 75):(self.end3 - self.start1) * int(self.vb.sampleRate / 75)] *= slope\n",
    "        print(windowStart, windowEnd, self.start1, self.end3)\n",
    "        return voicedExcitation[0:(self.end3 - self.start1) * int(self.vb.sampleRate / 75)]\n",
    "        #resample segments\n",
    "        #individual fourier transform\n",
    "        #istft\n",
    "        #windowing adaptive to borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "emotional-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocalSequence:\n",
    "    def __init__(self, start, end, vb, borders, phonemes, offsets):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.vb = vb\n",
    "        self.synth = Synthesizer(self.vb.sampleRate)\n",
    "        \n",
    "        self.spectrum = torch.zeros((self.end - self.start, int(self.vb.sampleRate / 25 / 2) + 1))\n",
    "        self.excitation = torch.zeros((self.end - self.start) * int(self.vb.sampleRate / 75))\n",
    "        self.voicedExcitation = torch.zeros((self.end - self.start) * int(self.vb.sampleRate / 75))\n",
    "        \n",
    "        self.segments = []\n",
    "        if len(phonemes)== 1:#rewrite border system to use tensor, implement pitch and steadiness\n",
    "            self.segments.append(VocalSegment(borders[0], borders[1], borders[2], borders[3], borders[4], borders[5],\n",
    "                                             True, True, phonemes[0], vb, offsets[0], None, 386, 0))\n",
    "        else:\n",
    "            self.segments.append(VocalSegment(borders[0], borders[1], borders[2], borders[3], borders[4], borders[5],\n",
    "                                             True, False, phonemes[0], vb, offsets[0], None, 386, 0))\n",
    "            for i in range(1, len(phonemes)-1):\n",
    "                self.segments.append(VocalSegment(borders[3*i], borders[3*i+1], borders[3*i+2], borders[3*i+3], borders[3*i+4], borders[3*i+5],\n",
    "                                                  False, False, phonemes[i], vb, offsets[i], None, 386, 0))\n",
    "            endpoint = len(phonemes)-1\n",
    "            self.segments.append(VocalSegment(borders[3*endpoint], borders[3*endpoint+1], borders[3*endpoint+2], borders[3*endpoint+3], borders[3*endpoint+4], borders[3*endpoint+5],\n",
    "                                             False, True, phonemes[-1], vb, offsets[-1], None, 386, 0))\n",
    "\n",
    "        self.requiresUpdate = np.ones(len(phonemes))\n",
    "        self.update()\n",
    "    def update(self):\n",
    "        for i in range(self.requiresUpdate.size):\n",
    "            if self.requiresUpdate[i] == 1:\n",
    "                print(i)\n",
    "                segment = self.segments[i]\n",
    "                spectrum = torch.zeros((segment.end3 - segment.start1, int(self.vb.sampleRate / 25 / 2) + 1))\n",
    "                excitation = torch.zeros((segment.end3 - segment.start1) * int(self.vb.sampleRate / 75))\n",
    "                voicedExcitation = torch.zeros((segment.end3 - segment.start1) * int(self.vb.sampleRate / 75))\n",
    "                if segment.startCap:\n",
    "                    windowStart = 0\n",
    "                else:\n",
    "                    windowStart = segment.start3 - segment.start1\n",
    "                    previousSpectrum = self.segments[i-1].getSpectrum()[-1]\n",
    "                    previousVoicedExcitation = self.segments[i-1].getVoicedExcitation()[(self.segments[i-1].end1-self.segments[i-1].end3)*int(self.vb.sampleRate/75):]\n",
    "                if segment.endCap:\n",
    "                    windowEnd = segment.end3 - segment.start1\n",
    "                else:\n",
    "                    windowEnd = segment.end1 - segment.start1\n",
    "                    nextSpectrum = self.segments[i+1].getSpectrum()[0]\n",
    "                    nextVoicedExcitation = self.segments[i+1].getVoicedExcitation()[0:(self.segments[i+1].start3-self.segments[i+1].start1)*int(self.vb.sampleRate/75)]\n",
    "                \n",
    "                spectrum[windowStart:windowEnd] = segment.getSpectrum()\n",
    "                voicedExcitation = segment.getVoicedExcitation()\n",
    "                if segment.startCap == False:\n",
    "                    for j in range(segment.start3 - segment.start1):\n",
    "                        spectrum[j] = self.vb.crfAi.processData(previousSpectrum, spectrum[windowStart], j / (segment.start3 - segment.start1))\n",
    "                    voicedExcitation[0:(segment.start3-segment.start1)*int(self.vb.sampleRate/75)] += previousVoicedExcitation\n",
    "                if segment.endCap == False:\n",
    "                    for j in range(segment.end1 - segment.start1, segment.end3 - segment.start1):\n",
    "                        spectrum[j] = self.vb.crfAi.processData(spectrum[windowEnd], nextSpectrum, (j - segment.start1) / (segment.end3 - segment.end1))\n",
    "                    voicedExcitation[(segment.end1-segment.end3)*int(self.vb.sampleRate/75):] += nextVoicedExcitation\n",
    "                if segment.startCap:\n",
    "                    windowStart = 0\n",
    "                else:\n",
    "                    windowStart = (segment.start2 - segment.start1) * int(self.vb.sampleRate / 75)\n",
    "                    previousExcitation = self.segments[i-1].getExcitation()[(segment.start1-segment.start2)*int(self.vb.sampleRate/75):]\n",
    "                    excitation[0:windowStart] = previousExcitation\n",
    "                if segment.endCap:\n",
    "                    windowEnd = (segment.end3 - segment.start1) * int(self.vb.sampleRate / 75)\n",
    "                else:\n",
    "                    windowEnd = (segment.end2 - segment.start1) * int(self.vb.sampleRate / 75)\n",
    "                    nextExcitation = self.segments[i+1].getExcitation()[0:(segment.end3-segment.end2)*int(self.vb.sampleRate/75)]\n",
    "                    excitation[windowEnd:] = nextExcitation\n",
    "                excitation[windowStart:windowEnd] = segment.getExcitation()\n",
    "                \n",
    "                self.spectrum[segment.start1:segment.end3] = spectrum\n",
    "                self.excitation[segment.start1*int(self.vb.sampleRate/75):segment.end3*int(self.vb.sampleRate/75)] = excitation\n",
    "                self.voicedExcitation[segment.start1*int(self.vb.sampleRate/75):segment.end3*int(self.vb.sampleRate/75)] = voicedExcitation\n",
    "                \n",
    "                skipPrevious = True#implement skipPrevious\n",
    "            else:\n",
    "                skipPrevious = False\n",
    "            \n",
    "        self.synth.Synthesize(0, self.spectrum, self.excitation, self.voicedExcitation)\n",
    "    def save(self):\n",
    "        self.synth.save(\"Output_Demo.wav\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "valid-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TempVB:\n",
    "    def __init__(self):\n",
    "        self.sampleRate = 48000\n",
    "        self.phonemeDict = dict([])\n",
    "        phonemeKeys = [\"A\", \"E\", \"I\", \"O\", \"U\", \"G\", \"K\", \"N\", \"S\", \"T\"]\n",
    "        for key in phonemeKeys:\n",
    "            self.phonemeDict[key] = AudioSample(\"Samples_rip/\"+key+\".wav\")\n",
    "            self.sampleRate = self.phonemeDict[key].sampleRate\n",
    "            self.phonemeDict[key].CalculatePitch(249.)\n",
    "            self.phonemeDict[key].CalculateSpectra(iterations = 15)\n",
    "            self.phonemeDict[key].CalculateExcitation()\n",
    "        self.crfAi = SpecCrfAi(learningRate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "expected-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Voicebank:\n",
    "    def __init__(self, vbKey):\n",
    "        self.phonemeDict = dict()\n",
    "        loaded_weights = 0#(vbKey)\n",
    "        self.crfAi = 0#SpecCrfAi(loaded_weights)\n",
    "        #load additional parameters\n",
    "        self.sampleRate = 48000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "polished-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Synthesizer:\n",
    "    def __init__(self, sampleRate):\n",
    "        self.sampleRate = sampleRate\n",
    "        self.returnSignal = torch.tensor([], dtype = float)\n",
    "        \n",
    "    def Synthesize(self, steadiness, spectrum, Excitation, VoicedExcitation):\n",
    "        tripleBatchSize = int(self.sampleRate / 25)\n",
    "        BatchSize = int(self.sampleRate / 75)\n",
    "        Window = torch.hann_window(tripleBatchSize)\n",
    "        \n",
    "        #HERE + VoicedExcitation\n",
    "        \n",
    "        self.returnSignal = torch.stft(Excitation + VoicedExcitation , tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, return_complex = True, onesided = True)\n",
    "        self.returnSignal = torch.transpose(self.returnSignal, 0, 1)[0:-1]\n",
    "        self.returnSignal = self.returnSignal * spectrum\n",
    "        self.returnSignal = torch.transpose(self.returnSignal, 0, 1)\n",
    "        self.returnSignal = torch.istft(self.returnSignal, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, onesided=True)\n",
    "        del Window\n",
    "        \n",
    "    def save(self, filepath):\n",
    "        torchaudio.save(filepath, torch.unsqueeze(self.returnSignal.detach(), 0), self.sampleRate, format=\"wav\", encoding=\"PCM_S\", bits_per_sample=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "latin-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingSamples = dict([])\n",
    "TrainingKeys = [\"A_E\", \"A_G\", \"A_I\", \"A_K\", \"A_N\", \"A_O\", \"A_S\", \"A_T\", \"A_U\",\n",
    "                   \"E_A\", \"E_G\", \"E_I\", \"E_K\", \"E_N\", \"E_O\", \"E_S\", \"E_T\", \"E_U\",\n",
    "                   \"I_A\", \"I_E\", \"I_G\", \"I_K\", \"I_N\", \"I_O\", \"I_S\", \"I_T\", \"I_U\",\n",
    "                   \"O_A\", \"O_E\", \"O_G\", \"O_I\", \"O_K\", \"O_N\", \"O_S\", \"O_T\", \"O_U\",\n",
    "                   \"U_A\", \"U_E\", \"U_G\", \"U_I\", \"U_K\", \"U_N\", \"U_O\", \"U_S\", \"U_T\",\n",
    "                   \"G_A\", \"G_E\", \"G_I\", \"G_O\", \"G_U\",\n",
    "                   \"K_A\", \"K_E\", \"K_I\", \"K_O\", \"K_U\",\n",
    "                   \"N_A\", \"N_E\", \"N_I\", \"N_O\", \"N_U\",\n",
    "                   \"S_A\", \"S_E\", \"S_I\", \"S_O\", \"S_U\",\n",
    "                   \"T_A\", \"T_E\", \"T_I\", \"T_O\", \"T_U\"\n",
    "                  ]\n",
    "\n",
    "for key in TrainingKeys:\n",
    "    TrainingSamples[key] = AudioSample(\"Samples_rip/\"+key+\".wav\")\n",
    "    TrainingSamples[key].CalculatePitch(249.)\n",
    "    TrainingSamples[key].CalculateSpectra(iterations = 25)\n",
    "    TrainingSamples[key].CalculateExcitation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "former-economics",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/5], loss:0.6331\n",
      "epoch [2/5], loss:0.3591\n",
      "epoch [3/5], loss:0.2202\n",
      "epoch [4/5], loss:0.1821\n",
      "epoch [5/5], loss:0.1805\n",
      "epoch [1/5], loss:0.4621\n",
      "epoch [2/5], loss:0.1461\n",
      "epoch [3/5], loss:0.0769\n",
      "epoch [4/5], loss:0.1363\n",
      "epoch [5/5], loss:0.0812\n",
      "epoch [1/5], loss:0.1463\n",
      "epoch [2/5], loss:0.1423\n",
      "epoch [3/5], loss:0.1999\n",
      "epoch [4/5], loss:0.1834\n",
      "epoch [5/5], loss:0.2425\n",
      "epoch [1/5], loss:0.0225\n",
      "epoch [2/5], loss:0.1649\n",
      "epoch [3/5], loss:0.1426\n",
      "epoch [4/5], loss:0.2499\n",
      "epoch [5/5], loss:0.0178\n",
      "epoch [1/5], loss:0.1227\n",
      "epoch [2/5], loss:0.0886\n",
      "epoch [3/5], loss:0.1006\n",
      "epoch [4/5], loss:0.1218\n",
      "epoch [5/5], loss:0.1535\n",
      "epoch [1/5], loss:0.1352\n",
      "epoch [2/5], loss:0.0868\n",
      "epoch [3/5], loss:0.0766\n",
      "epoch [4/5], loss:0.0810\n",
      "epoch [5/5], loss:0.0758\n",
      "epoch [1/5], loss:0.2426\n",
      "epoch [2/5], loss:0.1723\n",
      "epoch [3/5], loss:0.2169\n",
      "epoch [4/5], loss:0.1881\n",
      "epoch [5/5], loss:0.1783\n",
      "epoch [1/5], loss:0.0188\n",
      "epoch [2/5], loss:0.0726\n",
      "epoch [3/5], loss:0.0166\n",
      "epoch [4/5], loss:0.1352\n",
      "epoch [5/5], loss:0.0325\n",
      "epoch [1/5], loss:0.1241\n",
      "epoch [2/5], loss:0.0803\n",
      "epoch [3/5], loss:0.0867\n",
      "epoch [4/5], loss:0.1055\n",
      "epoch [5/5], loss:0.0694\n",
      "epoch [1/5], loss:0.1575\n",
      "epoch [2/5], loss:0.0805\n",
      "epoch [3/5], loss:0.0868\n",
      "epoch [4/5], loss:0.0924\n",
      "epoch [5/5], loss:0.1182\n",
      "epoch [1/5], loss:0.1547\n",
      "epoch [2/5], loss:0.0786\n",
      "epoch [3/5], loss:0.0676\n",
      "epoch [4/5], loss:0.0726\n",
      "epoch [5/5], loss:0.1374\n",
      "epoch [1/5], loss:0.1004\n",
      "epoch [2/5], loss:0.0906\n",
      "epoch [3/5], loss:0.0763\n",
      "epoch [4/5], loss:0.1402\n",
      "epoch [5/5], loss:0.1638\n",
      "epoch [1/5], loss:0.0437\n",
      "epoch [2/5], loss:0.0300\n",
      "epoch [3/5], loss:0.0757\n",
      "epoch [4/5], loss:0.0803\n",
      "epoch [5/5], loss:0.0835\n",
      "epoch [1/5], loss:0.0669\n",
      "epoch [2/5], loss:0.1026\n",
      "epoch [3/5], loss:0.0642\n",
      "epoch [4/5], loss:0.0761\n",
      "epoch [5/5], loss:0.0582\n",
      "epoch [1/5], loss:0.1847\n",
      "epoch [2/5], loss:0.0764\n",
      "epoch [3/5], loss:0.1743\n",
      "epoch [4/5], loss:0.0677\n",
      "epoch [5/5], loss:0.0879\n",
      "epoch [1/5], loss:0.1152\n",
      "epoch [2/5], loss:0.1660\n",
      "epoch [3/5], loss:0.1095\n",
      "epoch [4/5], loss:0.1131\n",
      "epoch [5/5], loss:0.1111\n",
      "epoch [1/5], loss:0.1062\n",
      "epoch [2/5], loss:0.0517\n",
      "epoch [3/5], loss:0.0588\n",
      "epoch [4/5], loss:0.0222\n",
      "epoch [5/5], loss:0.0118\n",
      "epoch [1/5], loss:0.0731\n",
      "epoch [2/5], loss:0.0844\n",
      "epoch [3/5], loss:0.0861\n",
      "epoch [4/5], loss:0.0608\n",
      "epoch [5/5], loss:0.0551\n",
      "epoch [1/5], loss:0.1262\n",
      "epoch [2/5], loss:0.0731\n",
      "epoch [3/5], loss:0.0819\n",
      "epoch [4/5], loss:0.0758\n",
      "epoch [5/5], loss:0.0635\n",
      "epoch [1/5], loss:0.1125\n",
      "epoch [2/5], loss:0.0953\n",
      "epoch [3/5], loss:0.0766\n",
      "epoch [4/5], loss:0.0626\n",
      "epoch [5/5], loss:0.0818\n",
      "epoch [1/5], loss:0.0444\n",
      "epoch [2/5], loss:0.2948\n",
      "epoch [3/5], loss:0.0187\n",
      "epoch [4/5], loss:0.0191\n",
      "epoch [5/5], loss:0.0338\n",
      "epoch [1/5], loss:0.0485\n",
      "epoch [2/5], loss:0.0466\n",
      "epoch [3/5], loss:0.0374\n",
      "epoch [4/5], loss:0.0894\n",
      "epoch [5/5], loss:0.0076\n",
      "epoch [1/5], loss:0.1321\n",
      "epoch [2/5], loss:0.0634\n",
      "epoch [3/5], loss:0.0406\n",
      "epoch [4/5], loss:0.0714\n",
      "epoch [5/5], loss:0.0366\n",
      "epoch [1/5], loss:0.1536\n",
      "epoch [2/5], loss:0.1090\n",
      "epoch [3/5], loss:0.0470\n",
      "epoch [4/5], loss:0.1041\n",
      "epoch [5/5], loss:0.0567\n",
      "epoch [1/5], loss:0.1086\n",
      "epoch [2/5], loss:0.0526\n",
      "epoch [3/5], loss:0.0577\n",
      "epoch [4/5], loss:0.0464\n",
      "epoch [5/5], loss:0.0834\n",
      "epoch [1/5], loss:0.0110\n",
      "epoch [2/5], loss:0.0139\n",
      "epoch [3/5], loss:0.0644\n",
      "epoch [4/5], loss:0.0320\n",
      "epoch [5/5], loss:0.0715\n",
      "epoch [1/5], loss:0.0679\n",
      "epoch [2/5], loss:0.0507\n",
      "epoch [3/5], loss:0.0524\n",
      "epoch [4/5], loss:0.0352\n",
      "epoch [5/5], loss:0.0410\n",
      "epoch [1/5], loss:0.2043\n",
      "epoch [2/5], loss:0.0788\n",
      "epoch [3/5], loss:0.0576\n",
      "epoch [4/5], loss:0.0448\n",
      "epoch [5/5], loss:0.0505\n",
      "epoch [1/5], loss:0.0770\n",
      "epoch [2/5], loss:0.0928\n",
      "epoch [3/5], loss:0.0536\n",
      "epoch [4/5], loss:0.0463\n",
      "epoch [5/5], loss:0.0466\n",
      "epoch [1/5], loss:0.0767\n",
      "epoch [2/5], loss:0.0077\n",
      "epoch [3/5], loss:0.0269\n",
      "epoch [4/5], loss:0.0950\n",
      "epoch [5/5], loss:0.0090\n",
      "epoch [1/5], loss:0.0983\n",
      "epoch [2/5], loss:0.0637\n",
      "epoch [3/5], loss:0.0715\n",
      "epoch [4/5], loss:0.0742\n",
      "epoch [5/5], loss:0.0629\n",
      "epoch [1/5], loss:0.0068\n",
      "epoch [2/5], loss:0.0500\n",
      "epoch [3/5], loss:0.0456\n",
      "epoch [4/5], loss:0.0394\n",
      "epoch [5/5], loss:0.0101\n",
      "epoch [1/5], loss:0.0803\n",
      "epoch [2/5], loss:0.0819\n",
      "epoch [3/5], loss:0.0719\n",
      "epoch [4/5], loss:0.0618\n",
      "epoch [5/5], loss:0.0390\n",
      "epoch [1/5], loss:0.0807\n",
      "epoch [2/5], loss:0.1106\n",
      "epoch [3/5], loss:0.0708\n",
      "epoch [4/5], loss:0.0490\n",
      "epoch [5/5], loss:0.0782\n",
      "epoch [1/5], loss:0.1015\n",
      "epoch [2/5], loss:0.0716\n",
      "epoch [3/5], loss:0.0663\n",
      "epoch [4/5], loss:0.0453\n",
      "epoch [5/5], loss:0.0085\n",
      "epoch [1/5], loss:0.0641\n",
      "epoch [2/5], loss:0.0477\n",
      "epoch [3/5], loss:0.0469\n",
      "epoch [4/5], loss:0.0511\n",
      "epoch [5/5], loss:0.0449\n",
      "epoch [1/5], loss:0.0726\n",
      "epoch [2/5], loss:0.0543\n",
      "epoch [3/5], loss:0.0703\n",
      "epoch [4/5], loss:0.0456\n",
      "epoch [5/5], loss:0.0715\n",
      "epoch [1/5], loss:0.0750\n",
      "epoch [2/5], loss:0.0666\n",
      "epoch [3/5], loss:0.0810\n",
      "epoch [4/5], loss:0.0683\n",
      "epoch [5/5], loss:0.1030\n",
      "epoch [1/5], loss:0.0696\n",
      "epoch [2/5], loss:0.0455\n",
      "epoch [3/5], loss:0.0508\n",
      "epoch [4/5], loss:0.0139\n",
      "epoch [5/5], loss:0.0347\n",
      "epoch [1/5], loss:0.0463\n",
      "epoch [2/5], loss:0.0545\n",
      "epoch [3/5], loss:0.0405\n",
      "epoch [4/5], loss:0.0231\n",
      "epoch [5/5], loss:0.0419\n",
      "epoch [1/5], loss:0.0079\n",
      "epoch [2/5], loss:0.0549\n",
      "epoch [3/5], loss:0.0484\n",
      "epoch [4/5], loss:0.0279\n",
      "epoch [5/5], loss:0.0049\n",
      "epoch [1/5], loss:0.0676\n",
      "epoch [2/5], loss:0.0668\n",
      "epoch [3/5], loss:0.0976\n",
      "epoch [4/5], loss:0.0520\n",
      "epoch [5/5], loss:0.0483\n",
      "epoch [1/5], loss:0.0685\n",
      "epoch [2/5], loss:0.0304\n",
      "epoch [3/5], loss:0.0525\n",
      "epoch [4/5], loss:0.0274\n",
      "epoch [5/5], loss:0.0446\n",
      "epoch [1/5], loss:0.1174\n",
      "epoch [2/5], loss:0.1079\n",
      "epoch [3/5], loss:0.1008\n",
      "epoch [4/5], loss:0.0571\n",
      "epoch [5/5], loss:0.1350\n",
      "epoch [1/5], loss:0.0600\n",
      "epoch [2/5], loss:0.0264\n",
      "epoch [3/5], loss:0.0417\n",
      "epoch [4/5], loss:0.0594\n",
      "epoch [5/5], loss:0.0611\n",
      "epoch [1/5], loss:0.1376\n",
      "epoch [2/5], loss:0.2311\n",
      "epoch [3/5], loss:0.1915\n",
      "epoch [4/5], loss:0.1777\n",
      "epoch [5/5], loss:0.1893\n",
      "epoch [1/5], loss:0.1387\n",
      "epoch [2/5], loss:0.0936\n",
      "epoch [3/5], loss:0.0730\n",
      "epoch [4/5], loss:0.0747\n",
      "epoch [5/5], loss:0.0778\n",
      "epoch [1/5], loss:0.1155\n",
      "epoch [2/5], loss:0.0771\n",
      "epoch [3/5], loss:0.0699\n",
      "epoch [4/5], loss:0.0500\n",
      "epoch [5/5], loss:0.0459\n",
      "epoch [1/5], loss:0.1456\n",
      "epoch [2/5], loss:0.0836\n",
      "epoch [3/5], loss:0.0857\n",
      "epoch [4/5], loss:0.0586\n",
      "epoch [5/5], loss:0.0821\n",
      "epoch [1/5], loss:0.0998\n",
      "epoch [2/5], loss:0.0771\n",
      "epoch [3/5], loss:0.0714\n",
      "epoch [4/5], loss:0.0629\n",
      "epoch [5/5], loss:0.0359\n",
      "epoch [1/5], loss:0.1019\n",
      "epoch [2/5], loss:0.1205\n",
      "epoch [3/5], loss:0.0863\n",
      "epoch [4/5], loss:0.0767\n",
      "epoch [5/5], loss:0.0707\n",
      "epoch [1/5], loss:0.1192\n",
      "epoch [2/5], loss:0.1540\n",
      "epoch [3/5], loss:0.0681\n",
      "epoch [4/5], loss:0.0858\n",
      "epoch [5/5], loss:0.0643\n",
      "epoch [1/5], loss:0.0869\n",
      "epoch [2/5], loss:0.0861\n",
      "epoch [3/5], loss:0.0665\n",
      "epoch [4/5], loss:0.0883\n",
      "epoch [5/5], loss:0.0534\n",
      "epoch [1/5], loss:0.0970\n",
      "epoch [2/5], loss:0.0793\n",
      "epoch [3/5], loss:0.0630\n",
      "epoch [4/5], loss:0.0610\n",
      "epoch [5/5], loss:0.0649\n",
      "epoch [1/5], loss:0.0846\n",
      "epoch [2/5], loss:0.0597\n",
      "epoch [3/5], loss:0.0682\n",
      "epoch [4/5], loss:0.0667\n",
      "epoch [5/5], loss:0.0458\n",
      "epoch [1/5], loss:0.1296\n",
      "epoch [2/5], loss:0.0888\n",
      "epoch [3/5], loss:0.0726\n",
      "epoch [4/5], loss:0.0673\n",
      "epoch [5/5], loss:0.0732\n",
      "epoch [1/5], loss:0.0631\n",
      "epoch [2/5], loss:0.0384\n",
      "epoch [3/5], loss:0.0684\n",
      "epoch [4/5], loss:0.0277\n",
      "epoch [5/5], loss:0.0461\n",
      "epoch [1/5], loss:0.0547\n",
      "epoch [2/5], loss:0.0427\n",
      "epoch [3/5], loss:0.0353\n",
      "epoch [4/5], loss:0.0590\n",
      "epoch [5/5], loss:0.0438\n",
      "epoch [1/5], loss:0.0414\n",
      "epoch [2/5], loss:0.0759\n",
      "epoch [3/5], loss:0.0541\n",
      "epoch [4/5], loss:0.0606\n",
      "epoch [5/5], loss:0.0257\n",
      "epoch [1/5], loss:0.0336\n",
      "epoch [2/5], loss:0.0308\n",
      "epoch [3/5], loss:0.0276\n",
      "epoch [4/5], loss:0.0515\n",
      "epoch [5/5], loss:0.0300\n",
      "epoch [1/5], loss:0.3054\n",
      "epoch [2/5], loss:0.3379\n",
      "epoch [3/5], loss:0.0813\n",
      "epoch [4/5], loss:0.1840\n",
      "epoch [5/5], loss:0.1562\n",
      "epoch [1/5], loss:0.1366\n",
      "epoch [2/5], loss:0.1897\n",
      "epoch [3/5], loss:0.1102\n",
      "epoch [4/5], loss:0.0990\n",
      "epoch [5/5], loss:0.0783\n",
      "epoch [1/5], loss:0.0933\n",
      "epoch [2/5], loss:0.0816\n",
      "epoch [3/5], loss:0.0544\n",
      "epoch [4/5], loss:0.0634\n",
      "epoch [5/5], loss:0.1238\n",
      "epoch [1/5], loss:0.1824\n",
      "epoch [2/5], loss:0.1030\n",
      "epoch [3/5], loss:0.1317\n",
      "epoch [4/5], loss:0.0661\n",
      "epoch [5/5], loss:0.0967\n",
      "epoch [1/5], loss:0.1642\n",
      "epoch [2/5], loss:0.0810\n",
      "epoch [3/5], loss:0.0935\n",
      "epoch [4/5], loss:0.0729\n",
      "epoch [5/5], loss:0.0701\n",
      "epoch [1/5], loss:0.1975\n",
      "epoch [2/5], loss:0.1290\n",
      "epoch [3/5], loss:0.1114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [4/5], loss:0.1064\n",
      "epoch [5/5], loss:0.0868\n",
      "epoch [1/5], loss:0.0735\n",
      "epoch [2/5], loss:0.1209\n",
      "epoch [3/5], loss:0.0520\n",
      "epoch [4/5], loss:0.0585\n",
      "epoch [5/5], loss:0.0540\n",
      "epoch [1/5], loss:0.0799\n",
      "epoch [2/5], loss:0.0609\n",
      "epoch [3/5], loss:0.0504\n",
      "epoch [4/5], loss:0.0486\n",
      "epoch [5/5], loss:0.0353\n",
      "epoch [1/5], loss:0.1426\n",
      "epoch [2/5], loss:0.1087\n",
      "epoch [3/5], loss:0.0666\n",
      "epoch [4/5], loss:0.0582\n",
      "epoch [5/5], loss:0.0587\n",
      "epoch [1/5], loss:0.0997\n",
      "epoch [2/5], loss:0.0693\n",
      "epoch [3/5], loss:0.0737\n",
      "epoch [4/5], loss:0.0550\n",
      "epoch [5/5], loss:0.0583\n"
     ]
    }
   ],
   "source": [
    "trainSpectra = []\n",
    "i = 0\n",
    "for key in TrainingKeys:\n",
    "    trainSpectra.append(torch.empty_like(TrainingSamples[key].spectra))\n",
    "    for j in range(TrainingSamples[key].spectra.size()[0]):\n",
    "        trainSpectra[i][j] = TrainingSamples[key].spectrum + TrainingSamples[key].spectra[j]\n",
    "    i += 1\n",
    "\n",
    "vb = TempVB()\n",
    "for i in range(70):\n",
    "    vb.crfAi.train(trainSpectra[i], epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "directed-nirvana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "25600 48363 35 52\n",
      "0 47484 0 37\n",
      "1\n",
      "0 47484 0 37\n",
      "25600 75650 40 79\n",
      "25600 48363 35 52\n",
      "2\n",
      "25600 48363 35 52\n",
      "0 14417 75 86\n",
      "25600 75650 40 79\n",
      "3\n",
      "25600 75650 40 79\n",
      "16640 78240 82 130\n",
      "0 14417 75 86\n",
      "4\n",
      "0 14417 75 86\n",
      "16640 78240 82 130\n"
     ]
    }
   ],
   "source": [
    "borders = [0, 1, 2,\n",
    "           35, 36, 37,\n",
    "           40, 51, 52,\n",
    "           75, 76, 79,\n",
    "           82, 83, 86,\n",
    "           128, 129, 130\n",
    "          ]\n",
    "phonemes = [\"A\", \"N\", \"A\", \"T\", \"A\"]\n",
    "#offsets = [0, 5, 1, 1, 1]\n",
    "offsets = [0, 20, 20, 0, 13]\n",
    "\n",
    "sequence = VocalSequence(0, 400, vb, borders, phonemes, offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "velvet-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "secret-working",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1dd42d57550>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(sequence.spectrum[31].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "strange-notification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1dd7d467f40>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(sequence.synth.returnSignal.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "residential-crime",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vb.crfAi.state_dict(), \"CrossfadeWeights.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "divine-middle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model = TheModelClass(*args, **kwargs)\\nmodel.load_state_dict(torch.load(PATH))\\nmodel.eval()'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"model = TheModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

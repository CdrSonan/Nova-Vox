{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "monetary-bulgaria",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jokla\\anaconda3\\lib\\site-packages\\torchaudio\\extension\\extension.py:13: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "torchaudio.set_audio_backend(\"soundfile\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "occasional-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioSample:\n",
    "    def __init__(self, filepath):\n",
    "        loadedData = torchaudio.load(filepath)\n",
    "        self.waveform = loadedData[0][0]\n",
    "        self.sampleRate = loadedData[1]\n",
    "        del loadedData\n",
    "        self.pitchDeltas = torch.tensor([], dtype = int)\n",
    "        self.pitchBorders = torch.tensor([], dtype = int)\n",
    "        self.pitch = torch.tensor([0], dtype = int)\n",
    "        self.spectra = torch.tensor([[]], dtype = float)\n",
    "        self.spectrum = torch.tensor([], dtype = float)\n",
    "        self.excitation = torch.tensor([], dtype = float)\n",
    "        self.voicedExcitation = torch.tensor([], dtype = float)\n",
    "        self.VoicedExcitations = torch.tensor([], dtype = float)\n",
    "        \n",
    "    def CalculatePitch(self, expectedPitch, searchRange = 0.2):\n",
    "        batchSize = math.floor((1. + searchRange) * self.sampleRate / expectedPitch)\n",
    "        lowerSearchLimit = math.floor((1. - searchRange) * self.sampleRate / expectedPitch)\n",
    "        batchStart = 0\n",
    "        while batchStart + batchSize <= self.waveform.size()[0] - batchSize:\n",
    "            sample = torch.index_select(self.waveform, 0, torch.linspace(batchStart, batchStart + batchSize, batchSize, dtype = int))\n",
    "            zeroTransitions = torch.tensor([], dtype = int)\n",
    "            for i in range(lowerSearchLimit, batchSize):\n",
    "                if (sample[i-1] < 0) and (sample[i] > 0):\n",
    "                    zeroTransitions = torch.cat([zeroTransitions, torch.tensor([i])], 0)\n",
    "            error = math.inf\n",
    "            delta = math.floor(self.sampleRate / expectedPitch)\n",
    "            for i in zeroTransitions:\n",
    "                shiftedSample = torch.index_select(self.waveform, 0, torch.linspace(batchStart + i.item(), batchStart + batchSize + i.item(), batchSize, dtype = int))\n",
    "                newError = torch.sum(torch.pow(sample - shiftedSample, 2))\n",
    "                if error > newError:\n",
    "                    delta = i.item()\n",
    "                    error = newError\n",
    "            self.pitchDeltas = torch.cat([self.pitchDeltas, torch.tensor([delta])])\n",
    "            batchStart += delta\n",
    "        nBatches = self.pitchDeltas.size()[0]\n",
    "        self.pitchBorders = torch.zeros(nBatches + 1, dtype = int)\n",
    "        for i in range(nBatches):\n",
    "            self.pitchBorders[i+1] = self.pitchBorders[i] + self.pitchDeltas[i]\n",
    "        self.pitch = torch.mean(self.pitchDeltas.float()).int()\n",
    "        del batchSize\n",
    "        del lowerSearchLimit\n",
    "        del batchStart\n",
    "        del sample\n",
    "        del zeroTransitions\n",
    "        del error\n",
    "        del delta\n",
    "        del shiftedSample\n",
    "        del newError\n",
    "        del nBatches\n",
    "        \n",
    "    def CalculateSpectra(self, iterations = 10, filterWidth = 10, preIterations = 2):\n",
    "        tripleBatchSize = int(self.sampleRate / 25)\n",
    "        BatchSize = int(self.sampleRate / 75)\n",
    "        Window = torch.hann_window(tripleBatchSize)\n",
    "        signals = torch.stft(self.waveform, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, return_complex = True, onesided = True)\n",
    "        signals = torch.transpose(signals, 0, 1)\n",
    "        signalsAbs = signals.abs()\n",
    "        \n",
    "        workingSpectra = torch.sqrt(signalsAbs)\n",
    "        \n",
    "        workingSpectra = torch.max(workingSpectra, torch.tensor([-100]))\n",
    "        self.spectra = torch.full_like(workingSpectra, -float(\"inf\"), dtype=torch.float)\n",
    "        \n",
    "        for j in range(preIterations):\n",
    "            workingSpectra = torch.max(workingSpectra, self.spectra)\n",
    "            self.spectra = workingSpectra\n",
    "            for i in range(filterWidth):\n",
    "                self.spectra = torch.roll(workingSpectra, -i, dims = 1) + self.spectra + torch.roll(workingSpectra, i, dims = 1)\n",
    "            self.spectra = self.spectra / (2 * filterWidth + 1)\n",
    "        \n",
    "        self.VoicedExcitations = torch.zeros_like(signals)\n",
    "        for i in range(signals.size()[0]):\n",
    "            for j in range(signals.size()[1]):\n",
    "                if torch.sqrt(signalsAbs[i][j]) > self.spectra[i][j]:\n",
    "                    self.VoicedExcitations[i][j] = signals[i][j]\n",
    "                \n",
    "        for j in range(iterations):\n",
    "            workingSpectra = torch.max(workingSpectra, self.spectra)\n",
    "            self.spectra = workingSpectra\n",
    "            for i in range(filterWidth):\n",
    "                self.spectra = torch.roll(workingSpectra, -i, dims = 1) + self.spectra + torch.roll(workingSpectra, i, dims = 1)\n",
    "            self.spectra = self.spectra / (2 * filterWidth + 1)\n",
    "        \n",
    "        self.spectrum = torch.mean(self.spectra, 0)\n",
    "        for i in range(self.spectra.size()[0]):\n",
    "            self.spectra[i] = self.spectra[i] - self.spectrum\n",
    "        #return torch.log(signalsAbs)\n",
    "        del Window\n",
    "        del signals\n",
    "        del workingSpectra\n",
    "        \n",
    "    def CalculateExcitation(self, filterWidth = 10):\n",
    "        tripleBatchSize = int(self.sampleRate / 25)\n",
    "        BatchSize = int(self.sampleRate / 75)\n",
    "        Window = torch.hann_window(tripleBatchSize)\n",
    "        signals = torch.stft(self.waveform, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, return_complex = True, onesided = True)\n",
    "        signals = torch.transpose(signals, 0, 1)\n",
    "        excitations = torch.empty_like(signals)\n",
    "        for i in range(excitations.size()[0]):\n",
    "            excitations[i] = signals[i] / torch.square(self.spectrum + self.spectra[i])\n",
    "            self.VoicedExcitations[i] = self.VoicedExcitations[i] / torch.square(self.spectrum + self.spectra[i])\n",
    "        \n",
    "        VoicedExcitations = torch.transpose(self.VoicedExcitations, 0, 1)\n",
    "        excitations = torch.transpose(excitations, 0, 1)\n",
    "        self.excitation = torch.istft(excitations, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, onesided = True)\n",
    "        self.voicedExcitation = torch.istft(VoicedExcitations, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, onesided = True)\n",
    "        \n",
    "        self.excitation = self.excitation - self.voicedExcitation\n",
    "        self.excitation = torch.stft(self.excitation, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, return_complex = True, onesided = True)\n",
    "        self.voicedExcitation = torch.stft(self.voicedExcitation, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, return_complex = True, onesided = True)\n",
    "        self.excitation = torch.transpose(self.excitation, 0, 1)\n",
    "        #self.voicedExcitation = torch.transpose(self.voicedExcitation, 0, 1)\n",
    "        \n",
    "        del Window\n",
    "        del signals\n",
    "        del excitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "expired-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(RelLoss, self).__init__()\n",
    " \n",
    "    def forward(self, inputs, targets):    \n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        differences = torch.abs(inputs - targets)\n",
    "        sums = torch.abs(inputs + targets)\n",
    "        out = (differences / sums).sum() / inputs.size()[0]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "configured-craps",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecCrfAi(nn.Module):\n",
    "    def __init__(self, learningRate=1e-4):\n",
    "        super(SpecCrfAi, self).__init__()\n",
    "        \n",
    "        self.layer1 = torch.nn.Linear(3843, 3843)\n",
    "        self.ReLu1 = nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(3843, 5763)\n",
    "        self.ReLu2 = nn.ReLU()\n",
    "        self.layer3 = torch.nn.Linear(5763, 3842)\n",
    "        self.ReLu3 = nn.ReLU()\n",
    "        self.layer4 = torch.nn.Linear(3842, 1921)\n",
    "        \n",
    "        self.learningRate = learningRate\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learningRate, weight_decay=0.)\n",
    "        self.criterion = nn.L1Loss()\n",
    "        #self.criterion = RelLoss()\n",
    "        \n",
    "    def forward(self, spectrum1, spectrum2, factor):\n",
    "        fac = torch.tensor([factor])\n",
    "        x = torch.cat((spectrum1, spectrum2, fac), dim = 0)\n",
    "        x = x.float()#.unsqueeze(0).unsqueeze(0)\n",
    "        x = self.layer1(x)\n",
    "        x = self.ReLu1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.ReLu2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.ReLu3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "    \n",
    "    def processData(self, spectrum1, spectrum2, factor):\n",
    "        output = torch.square(torch.squeeze(self(torch.sqrt(spectrum1), torch.sqrt(spectrum2), factor)))\n",
    "        return output\n",
    "    \n",
    "    def train(self, indata, epochs=1):\n",
    "        for epoch in range(epochs):\n",
    "            for data in self.dataLoader(indata):\n",
    "                spectrum1 = data[0]\n",
    "                spectrum2 = data[-1]\n",
    "                indexList = np.arange(0, data.size()[0], 1)\n",
    "                np.random.shuffle(indexList)\n",
    "                for i in indexList:\n",
    "                    factor = i / float(data.size()[0])\n",
    "                    spectrumTarget = data[i]\n",
    "                    output = torch.squeeze(self(spectrum1, spectrum2, factor))\n",
    "                    loss = self.criterion(output, spectrumTarget)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "            print('epoch [{}/{}], loss:{:.4f}'\n",
    "                  .format(epoch + 1, epochs, loss.data))\n",
    "            \n",
    "    def dataLoader(self, data):\n",
    "        return torch.utils.data.DataLoader(dataset=data, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "black-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocalSegment:\n",
    "    def __init__(self, start1, start2, start3, end1, end2, end3, startCap, endCap, phonemeKey, vb, offset, repetititionSpacing, pitch, steadiness):\n",
    "        self.start1 = start1\n",
    "        self.start2 = start2\n",
    "        self.start3 = start3\n",
    "        self.end1 = end1\n",
    "        self.end2 = end2\n",
    "        self.end3 = end3\n",
    "        self.startCap = startCap\n",
    "        self.endCap = endCap\n",
    "        self.phonemeKey = phonemeKey\n",
    "        self.vb = vb\n",
    "        self.offset = offset\n",
    "        self.repetititionSpacing = repetititionSpacing\n",
    "        self.pitch = pitch\n",
    "        self.steadiness = steadiness\n",
    "        \n",
    "    def phaseShift(inputTensor, pitch, phase):\n",
    "        absolutes = inputTensor.abs()\n",
    "        phases = inputTensor.angle()\n",
    "        phaseOffsets = torch.full(phases.size(), phase / pitch)\n",
    "        phaseOffsets *= torch.arange(phases.size()[0])\n",
    "        phases += phaseOffsets\n",
    "        phases = torch.fmod(phases, 2 * math.pi)\n",
    "        return torch.transpose(torch.polar(absolutes, phases), 0, 1)\n",
    "        \n",
    "    def loopSamplerVoicedExcitation(self, inputTensor, targetSize, repetititionSpacing, pitch):\n",
    "        batchRS = repetititionSpacing\n",
    "        BatchSize = int(self.vb.sampleRate / 75)\n",
    "        tripleBatchSize = int(self.vb.sampleRate / 25)\n",
    "        repetititionSpacing *= BatchSize\n",
    "        window = torch.hann_window(tripleBatchSize)\n",
    "        alignPhase = inputTensor[batchRS][pitch].angle()\n",
    "        finalPhase = inputTensor[1][pitch].angle()\n",
    "        phaseShift = finalPhase - alignPhase\n",
    "        requiredTensors = math.ceil((targetSize + batchRS) / inputTensor.size()[0])\n",
    "        \n",
    "        if requiredTensors == 1:\n",
    "            outputTensor = inputTensor\n",
    "            outputTensor = torch.transpose(outputTensor, 0, 1)\n",
    "            outputTensor = torch.istft(outputTensor, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = window, onesided = True, length = inputTensor.size()[0]*BatchSize)\n",
    "        else:\n",
    "            outputTensor = torch.zeros(requiredTensors * (inputTensor.size()[0] * BatchSize - repetititionSpacing) + repetititionSpacing)\n",
    "            \n",
    "            workingTensor = inputTensor\n",
    "            workingTensor = torch.istft(workingTensor, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = window, onesided = True, length = inputTensor.size()[0]*BatchSize)\n",
    "            workingTensor[-repetititionSpacing:] *= torch.linspace(1, 0, repetititionSpacing)\n",
    "            outputTensor[0:inputTensor.size()[0]] += workingTensor\n",
    "            \n",
    "            for i in range(1, requiredTensors - 1):\n",
    "                workingTensor = torch.transpose(inputTensor, 0, 1)\n",
    "                workingTensor = phaseShift(workingTensor, pitch, i * phaseShift)\n",
    "                workingTensor = torch.istft(workingTensor, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = window, onesided = True, length = inputTensor.size()[0]*BatchSize)\n",
    "                workingTensor[0:repetititionSpacing] *= torch.linspace(0, 1, repetititionSpacing)\n",
    "                workingTensor[-repetititionSpacing:] *= torch.linspace(1, 0, repetititionSpacing)\n",
    "                outputTensor[i * (inputTensor.size()[0] - repetititionSpacing):i * (inputTensor.size()[0] - repetititionSpacing) + inputTensor.size()[0]] += workingTensor\n",
    "            \n",
    "            workingTensor = torch.transpose(inputTensor, 0, 1)\n",
    "            workingTensor = phaseShift(inputTensor, pitch, (requiredTensors - 1) * phaseShift)\n",
    "            workingTensor = torch.istft(workingTensor, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = window, onesided = True, length = inputTensor.size()[0]*BatchSize)\n",
    "            workingTensor[0:repetititionSpacing] *= torch.linspace(0, 1, repetititionSpacing)\n",
    "            outputTensor[(requiredTensors - 1) * (inputTensor.size()[0] - repetititionSpacing):] += workingTensor\n",
    "        return outputTensor[0:targetSize]\n",
    "    \n",
    "    def loopSamplerSpectrum(self, inputTensor, targetSize, repetititionSpacing):\n",
    "        repetititionSpacing = math.ceil(repetititionSpacing / int(self.vb.sampleRate / 75))\n",
    "        requiredTensors = math.ceil((targetSize + repetititionSpacing) / inputTensor.size()[0])\n",
    "        if requiredTensors == 1:\n",
    "            outputTensor = inputTensor\n",
    "        else:\n",
    "            outputTensor = torch.zeros(requiredTensors * (inputTensor.size()[0] - repetititionSpacing) + repetititionSpacing)\n",
    "            \n",
    "            workingTensor = inputTensor\n",
    "            workingTensor[-repetititionSpacing:] *= torch.linspace(1, 0, repetititionSpacing)\n",
    "            outputTensor[0:inputTensor.size()[0]] += workingTensor\n",
    "            \n",
    "            for i in range(1, requiredTensors - 1):\n",
    "                workingTensor = inputTensor\n",
    "                workingTensor[0:repetititionSpacing] *= torch.linspace(0, 1, repetititionSpacing)\n",
    "                workingTensor[-repetititionSpacing:] *= torch.linspace(1, 0, repetititionSpacing)\n",
    "                outputTensor[i * (inputTensor.size()[0] - repetititionSpacing):i * (inputTensor.size()[0] - repetititionSpacing) + inputTensor.size()[0]] += workingTensor\n",
    "            \n",
    "            workingTensor = inputTensor\n",
    "            workingTensor[0:repetititionSpacing] *= torch.linspace(0, 1, repetititionSpacing)\n",
    "            outputTensor[(requiredTensors - 1) * (inputTensor.size()[0] - repetititionSpacing):] += workingTensor\n",
    "        return outputTensor[0:targetSize]\n",
    "    \n",
    "    def getSpectrum(self):\n",
    "        if self.startCap:\n",
    "            windowStart = self.offset\n",
    "        else:\n",
    "            windowStart = self.start3 - self.start1 + self.offset\n",
    "        if self.endCap:\n",
    "            windowEnd = self.end3 - self.start1 + self.offset\n",
    "        else:\n",
    "            windowEnd = self.end1 - self.start1 + self.offset\n",
    "        spectrum =  self.vb.phonemeDict[self.phonemeKey].spectrum#implement looping\n",
    "        #spectra =  self.vb.phonemeDict[self.phonemeKey].spectra[windowStart:windowEnd]\n",
    "        spectra = self.loopSamplerSpectrum(self.vb.phonemeDict[self.phonemeKey].spectra, windowEnd, self.repetititionSpacing)[windowStart:windowEnd]\n",
    "        return torch.square(spectrum + (math.pow(1 - self.steadiness, 2) * spectra))\n",
    "    \n",
    "    def getExcitation(self):\n",
    "        BatchSize = int(self.vb.sampleRate / 75)\n",
    "        tripleBatchSize = int(self.vb.sampleRate / 25)\n",
    "        premul = self.vb.phonemeDict[self.phonemeKey].excitation.size()[0] / (self.end3 - self.start1 + 1)\n",
    "        #premul = 1\n",
    "        if self.startCap:\n",
    "            windowStart = 0\n",
    "            length = -self.start1\n",
    "        else:\n",
    "            windowStart = math.floor((self.start2 - self.start1) * premul)\n",
    "            length = -self.start2\n",
    "        if self.endCap:\n",
    "            windowEnd = math.ceil((self.end3 - self.start1) * premul)\n",
    "            length += self.end3\n",
    "        else:\n",
    "            windowEnd = math.ceil((self.end2 - self.start1) * premul)\n",
    "            length += self.end2\n",
    "        excitation = self.vb.phonemeDict[self.phonemeKey].excitation[windowStart:windowEnd]\n",
    "        excitation = torch.transpose(excitation, 0, 1)\n",
    "        transform = torchaudio.transforms.TimeStretch(hop_length = int(self.vb.sampleRate / 75),\n",
    "                                                      n_freq = int(self.vb.sampleRate / 25 / 2) + 1, \n",
    "                                                      fixed_rate = premul)\n",
    "        excitation = transform(torch.view_as_real(excitation))\n",
    "        excitation = torch.view_as_complex(excitation)\n",
    "        window = torch.hann_window(int(self.vb.sampleRate / 25))\n",
    "        excitation = torch.istft(excitation, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = window, onesided = True, length = length*int(self.vb.sampleRate / 75))\n",
    "        \n",
    "        return excitation[0:length*int(self.vb.sampleRate / 75)]\n",
    "    \n",
    "    def getVoicedExcitation(self):\n",
    "        nativePitch = self.vb.phonemeDict[self.phonemeKey].pitch\n",
    "        #nativePitch = self.vb.phonemeDict[self.phonemeKey].pitches[...]\n",
    "        #pitch = nativePitch + self.vb.phonemeDict[phonemeKey].pitches...\n",
    "        premul = self.pitch / nativePitch * self.vb.sampleRate / 75\n",
    "        #windowStart = math.floor(self.offset * self.vb.sampleRate / 75)\n",
    "        #windowEnd = math.ceil((self.end3 - self.start1) * premul + (self.offset * self.vb.sampleRate / 75))\n",
    "        windowStart = math.floor(self.offset)\n",
    "        windowEnd = math.ceil((self.end3 - self.start1) * premul + self.offset)\n",
    "        #voicedExcitation = self.vb.phonemeDict[self.phonemeKey].voicedExcitation[windowStart:windowEnd]\n",
    "        voicedExcitation = self.loopSamplerVoicedExcitation(self.vb.phonemeDict[self.phonemeKey].voicedExcitation, windowEnd, self.repetititionSpacing, math.ceil(nativePitch / 75.))[windowStart * self.vb.sampleRate / 75:windowEnd * self.vb.sampleRate / 75]\n",
    "        transform = torchaudio.transforms.Resample(orig_freq = nativePitch,\n",
    "                                                   new_freq = self.pitch,\n",
    "                                                   resampling_method = 'sinc_interpolation')\n",
    "        voicedExcitation = transform(voicedExcitation)\n",
    "        if self.startCap == False:\n",
    "            slope = torch.linspace(0, 1, (self.start3 - self.start1) * int(self.vb.sampleRate / 75))\n",
    "            voicedExcitation[0:(self.start3 - self.start1) * int(self.vb.sampleRate / 75)] *= slope\n",
    "        if self.endCap == False:\n",
    "            slope = torch.linspace(1, 0, (self.end3 - self.end1) * int(self.vb.sampleRate / 75))\n",
    "            voicedExcitation[(self.end1 - self.start1) * int(self.vb.sampleRate / 75):(self.end3 - self.start1) * int(self.vb.sampleRate / 75)] *= slope\n",
    "        print(windowStart, windowEnd, self.start1, self.end3)\n",
    "        return voicedExcitation[0:(self.end3 - self.start1) * int(self.vb.sampleRate / 75)]\n",
    "        #resample segments\n",
    "        #individual fourier transform\n",
    "        #istft\n",
    "        #windowing adaptive to borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "emotional-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocalSequence:\n",
    "    def __init__(self, start, end, vb, borders, phonemes, offsets):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.vb = vb\n",
    "        self.synth = Synthesizer(self.vb.sampleRate)\n",
    "        \n",
    "        self.spectrum = torch.zeros((self.end - self.start, int(self.vb.sampleRate / 25 / 2) + 1))\n",
    "        self.excitation = torch.zeros((self.end - self.start) * int(self.vb.sampleRate / 75))\n",
    "        self.voicedExcitation = torch.zeros((self.end - self.start) * int(self.vb.sampleRate / 75))\n",
    "        \n",
    "        self.segments = []\n",
    "        if len(phonemes)== 1:#rewrite border system to use tensor, implement pitch and steadiness\n",
    "            self.segments.append(VocalSegment(borders[0], borders[1], borders[2], borders[3], borders[4], borders[5],\n",
    "                                             True, True, phonemes[0], vb, offsets[0], 5, 386, 0))\n",
    "        else:\n",
    "            self.segments.append(VocalSegment(borders[0], borders[1], borders[2], borders[3], borders[4], borders[5],\n",
    "                                             True, False, phonemes[0], vb, offsets[0], 5, 386, 0))\n",
    "            for i in range(1, len(phonemes)-1):\n",
    "                self.segments.append(VocalSegment(borders[3*i], borders[3*i+1], borders[3*i+2], borders[3*i+3], borders[3*i+4], borders[3*i+5],\n",
    "                                                  False, False, phonemes[i], vb, offsets[i], 5, 386, 0))\n",
    "            endpoint = len(phonemes)-1\n",
    "            self.segments.append(VocalSegment(borders[3*endpoint], borders[3*endpoint+1], borders[3*endpoint+2], borders[3*endpoint+3], borders[3*endpoint+4], borders[3*endpoint+5],\n",
    "                                             False, True, phonemes[-1], vb, offsets[-1], 5, 386, 0))\n",
    "\n",
    "        self.requiresUpdate = np.ones(len(phonemes))\n",
    "        self.update()\n",
    "    def update(self):\n",
    "        for i in range(self.requiresUpdate.size):\n",
    "            if self.requiresUpdate[i] == 1:\n",
    "                print(i)\n",
    "                segment = self.segments[i]\n",
    "                spectrum = torch.zeros((segment.end3 - segment.start1, int(self.vb.sampleRate / 25 / 2) + 1))\n",
    "                excitation = torch.zeros((segment.end3 - segment.start1) * int(self.vb.sampleRate / 75))\n",
    "                voicedExcitation = torch.zeros((segment.end3 - segment.start1) * int(self.vb.sampleRate / 75))\n",
    "                if segment.startCap:\n",
    "                    windowStart = 0\n",
    "                else:\n",
    "                    windowStart = segment.start3 - segment.start1\n",
    "                    previousSpectrum = self.segments[i-1].getSpectrum()[-1]\n",
    "                    previousVoicedExcitation = self.segments[i-1].getVoicedExcitation()[(self.segments[i-1].end1-self.segments[i-1].end3)*int(self.vb.sampleRate/75):]\n",
    "                if segment.endCap:\n",
    "                    windowEnd = segment.end3 - segment.start1\n",
    "                else:\n",
    "                    windowEnd = segment.end1 - segment.start1\n",
    "                    nextSpectrum = self.segments[i+1].getSpectrum()[0]\n",
    "                    nextVoicedExcitation = self.segments[i+1].getVoicedExcitation()[0:(self.segments[i+1].start3-self.segments[i+1].start1)*int(self.vb.sampleRate/75)]\n",
    "                \n",
    "                spectrum[windowStart:windowEnd] = segment.getSpectrum()\n",
    "                voicedExcitation = segment.getVoicedExcitation()\n",
    "                if segment.startCap == False:\n",
    "                    for j in range(segment.start3 - segment.start1):\n",
    "                        spectrum[j] = self.vb.crfAi.processData(previousSpectrum, spectrum[windowStart], j / (segment.start3 - segment.start1))\n",
    "                    voicedExcitation[0:(segment.start3-segment.start1)*int(self.vb.sampleRate/75)] += previousVoicedExcitation\n",
    "                if segment.endCap == False:\n",
    "                    for j in range(segment.end1 - segment.start1, segment.end3 - segment.start1):\n",
    "                        spectrum[j] = self.vb.crfAi.processData(spectrum[windowEnd], nextSpectrum, (j - segment.start1) / (segment.end3 - segment.end1))\n",
    "                    voicedExcitation[(segment.end1-segment.end3)*int(self.vb.sampleRate/75):] += nextVoicedExcitation\n",
    "                if segment.startCap:\n",
    "                    windowStart = 0\n",
    "                else:\n",
    "                    windowStart = (segment.start2 - segment.start1) * int(self.vb.sampleRate / 75)\n",
    "                    previousExcitation = self.segments[i-1].getExcitation()[(segment.start1-segment.start2)*int(self.vb.sampleRate/75):]\n",
    "                    excitation[0:windowStart] = previousExcitation\n",
    "                if segment.endCap:\n",
    "                    windowEnd = (segment.end3 - segment.start1) * int(self.vb.sampleRate / 75)\n",
    "                else:\n",
    "                    windowEnd = (segment.end2 - segment.start1) * int(self.vb.sampleRate / 75)\n",
    "                    nextExcitation = self.segments[i+1].getExcitation()[0:(segment.end3-segment.end2)*int(self.vb.sampleRate/75)]\n",
    "                    excitation[windowEnd:] = nextExcitation\n",
    "                excitation[windowStart:windowEnd] = segment.getExcitation()\n",
    "                \n",
    "                self.spectrum[segment.start1:segment.end3] = spectrum\n",
    "                self.excitation[segment.start1*int(self.vb.sampleRate/75):segment.end3*int(self.vb.sampleRate/75)] = excitation\n",
    "                self.voicedExcitation[segment.start1*int(self.vb.sampleRate/75):segment.end3*int(self.vb.sampleRate/75)] = voicedExcitation\n",
    "                \n",
    "                skipPrevious = True#implement skipPrevious\n",
    "            else:\n",
    "                skipPrevious = False\n",
    "            \n",
    "        self.synth.Synthesize(0, self.spectrum, self.excitation, self.voicedExcitation)\n",
    "    def save(self):\n",
    "        self.synth.save(\"Output_Demo.wav\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "valid-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TempVB:\n",
    "    def __init__(self):\n",
    "        self.sampleRate = 48000\n",
    "        self.phonemeDict = dict([])\n",
    "        phonemeKeys = [\"A\", \"E\", \"I\", \"O\", \"U\", \"G\", \"K\", \"N\", \"S\", \"T\"]\n",
    "        for key in phonemeKeys:\n",
    "            self.phonemeDict[key] = AudioSample(\"Samples_rip/\"+key+\".wav\")\n",
    "            self.sampleRate = self.phonemeDict[key].sampleRate\n",
    "            self.phonemeDict[key].CalculatePitch(249.)\n",
    "            self.phonemeDict[key].CalculateSpectra(iterations = 15)\n",
    "            self.phonemeDict[key].CalculateExcitation()\n",
    "        self.crfAi = SpecCrfAi(learningRate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "expected-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Voicebank:\n",
    "    def __init__(self, vbKey):\n",
    "        self.phonemeDict = dict()\n",
    "        loaded_weights = 0#(vbKey)\n",
    "        self.crfAi = 0#SpecCrfAi(loaded_weights)\n",
    "        #load additional parameters\n",
    "        self.sampleRate = 48000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "polished-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Synthesizer:\n",
    "    def __init__(self, sampleRate):\n",
    "        self.sampleRate = sampleRate\n",
    "        self.returnSignal = torch.tensor([], dtype = float)\n",
    "        \n",
    "    def Synthesize(self, steadiness, spectrum, Excitation, VoicedExcitation):\n",
    "        tripleBatchSize = int(self.sampleRate / 25)\n",
    "        BatchSize = int(self.sampleRate / 75)\n",
    "        Window = torch.hann_window(tripleBatchSize)\n",
    "        \n",
    "        #HERE + VoicedExcitation\n",
    "        \n",
    "        self.returnSignal = torch.stft(Excitation + VoicedExcitation , tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, return_complex = True, onesided = True)\n",
    "        self.returnSignal = torch.transpose(self.returnSignal, 0, 1)[0:-1]\n",
    "        self.returnSignal = self.returnSignal * spectrum\n",
    "        self.returnSignal = torch.transpose(self.returnSignal, 0, 1)\n",
    "        self.returnSignal = torch.istft(self.returnSignal, tripleBatchSize, hop_length = BatchSize, win_length = tripleBatchSize, window = Window, onesided=True)\n",
    "        del Window\n",
    "        \n",
    "    def save(self, filepath):\n",
    "        torchaudio.save(filepath, torch.unsqueeze(self.returnSignal.detach(), 0), self.sampleRate, format=\"wav\", encoding=\"PCM_S\", bits_per_sample=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "latin-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingSamples = dict([])\n",
    "TrainingKeys = [\"A_E\", \"A_G\", \"A_I\", \"A_K\", \"A_N\", \"A_O\", \"A_S\", \"A_T\", \"A_U\",\n",
    "                   \"E_A\", \"E_G\", \"E_I\", \"E_K\", \"E_N\", \"E_O\", \"E_S\", \"E_T\", \"E_U\",\n",
    "                   \"I_A\", \"I_E\", \"I_G\", \"I_K\", \"I_N\", \"I_O\", \"I_S\", \"I_T\", \"I_U\",\n",
    "                   \"O_A\", \"O_E\", \"O_G\", \"O_I\", \"O_K\", \"O_N\", \"O_S\", \"O_T\", \"O_U\",\n",
    "                   \"U_A\", \"U_E\", \"U_G\", \"U_I\", \"U_K\", \"U_N\", \"U_O\", \"U_S\", \"U_T\",\n",
    "                   \"G_A\", \"G_E\", \"G_I\", \"G_O\", \"G_U\",\n",
    "                   \"K_A\", \"K_E\", \"K_I\", \"K_O\", \"K_U\",\n",
    "                   \"N_A\", \"N_E\", \"N_I\", \"N_O\", \"N_U\",\n",
    "                   \"S_A\", \"S_E\", \"S_I\", \"S_O\", \"S_U\",\n",
    "                   \"T_A\", \"T_E\", \"T_I\", \"T_O\", \"T_U\"\n",
    "                  ]\n",
    "\n",
    "for key in TrainingKeys:\n",
    "    TrainingSamples[key] = AudioSample(\"Samples_rip/\"+key+\".wav\")\n",
    "    TrainingSamples[key].CalculatePitch(249.)\n",
    "    TrainingSamples[key].CalculateSpectra(iterations = 25)\n",
    "    TrainingSamples[key].CalculateExcitation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "former-economics",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/5], loss:0.6763\n",
      "epoch [2/5], loss:0.3630\n",
      "epoch [3/5], loss:0.2082\n",
      "epoch [4/5], loss:0.1566\n",
      "epoch [5/5], loss:0.1041\n",
      "epoch [1/5], loss:0.1576\n",
      "epoch [2/5], loss:0.0982\n",
      "epoch [3/5], loss:0.1820\n",
      "epoch [4/5], loss:0.1365\n",
      "epoch [5/5], loss:0.1282\n",
      "epoch [1/5], loss:0.5060\n",
      "epoch [2/5], loss:0.1596\n",
      "epoch [3/5], loss:0.1408\n",
      "epoch [4/5], loss:0.1472\n",
      "epoch [5/5], loss:0.1675\n",
      "epoch [1/5], loss:0.1882\n",
      "epoch [2/5], loss:0.1343\n",
      "epoch [3/5], loss:0.1085\n",
      "epoch [4/5], loss:0.1929\n",
      "epoch [5/5], loss:0.0419\n",
      "epoch [1/5], loss:0.1333\n",
      "epoch [2/5], loss:0.1420\n",
      "epoch [3/5], loss:0.1236\n",
      "epoch [4/5], loss:0.1064\n",
      "epoch [5/5], loss:0.0791\n",
      "epoch [1/5], loss:0.1519\n",
      "epoch [2/5], loss:0.0801\n",
      "epoch [3/5], loss:0.0682\n",
      "epoch [4/5], loss:0.0605\n",
      "epoch [5/5], loss:0.0580\n",
      "epoch [1/5], loss:0.3872\n",
      "epoch [2/5], loss:0.1792\n",
      "epoch [3/5], loss:0.2550\n",
      "epoch [4/5], loss:0.1628\n",
      "epoch [5/5], loss:0.2420\n",
      "epoch [1/5], loss:0.0294\n",
      "epoch [2/5], loss:0.1711\n",
      "epoch [3/5], loss:0.1577\n",
      "epoch [4/5], loss:0.1151\n",
      "epoch [5/5], loss:0.1277\n",
      "epoch [1/5], loss:0.1112\n",
      "epoch [2/5], loss:0.0725\n",
      "epoch [3/5], loss:0.1257\n",
      "epoch [4/5], loss:0.1249\n",
      "epoch [5/5], loss:0.0579\n",
      "epoch [1/5], loss:0.1693\n",
      "epoch [2/5], loss:0.1015\n",
      "epoch [3/5], loss:0.0797\n",
      "epoch [4/5], loss:0.0776\n",
      "epoch [5/5], loss:0.1137\n",
      "epoch [1/5], loss:0.0861\n",
      "epoch [2/5], loss:0.1122\n",
      "epoch [3/5], loss:0.0727\n",
      "epoch [4/5], loss:0.0719\n",
      "epoch [5/5], loss:0.1194\n",
      "epoch [1/5], loss:0.1072\n",
      "epoch [2/5], loss:0.0780\n",
      "epoch [3/5], loss:0.1180\n",
      "epoch [4/5], loss:0.0864\n",
      "epoch [5/5], loss:0.1050\n",
      "epoch [1/5], loss:0.0231\n",
      "epoch [2/5], loss:0.1249\n",
      "epoch [3/5], loss:0.1211\n",
      "epoch [4/5], loss:0.0292\n",
      "epoch [5/5], loss:0.1111\n",
      "epoch [1/5], loss:0.0705\n",
      "epoch [2/5], loss:0.1046\n",
      "epoch [3/5], loss:0.0899\n",
      "epoch [4/5], loss:0.0452\n",
      "epoch [5/5], loss:0.0761\n",
      "epoch [1/5], loss:0.0755\n",
      "epoch [2/5], loss:0.0742\n",
      "epoch [3/5], loss:0.1722\n",
      "epoch [4/5], loss:0.0764\n",
      "epoch [5/5], loss:0.1732\n",
      "epoch [1/5], loss:0.1473\n",
      "epoch [2/5], loss:0.1390\n",
      "epoch [3/5], loss:0.1439\n",
      "epoch [4/5], loss:0.1424\n",
      "epoch [5/5], loss:0.0958\n",
      "epoch [1/5], loss:0.0772\n",
      "epoch [2/5], loss:0.0792\n",
      "epoch [3/5], loss:0.1013\n",
      "epoch [4/5], loss:0.0204\n",
      "epoch [5/5], loss:0.0141\n",
      "epoch [1/5], loss:0.1646\n",
      "epoch [2/5], loss:0.1069\n",
      "epoch [3/5], loss:0.0550\n",
      "epoch [4/5], loss:0.0688\n",
      "epoch [5/5], loss:0.0839\n",
      "epoch [1/5], loss:0.1069\n",
      "epoch [2/5], loss:0.0930\n",
      "epoch [3/5], loss:0.0788\n",
      "epoch [4/5], loss:0.0871\n",
      "epoch [5/5], loss:0.0648\n",
      "epoch [1/5], loss:0.1308\n",
      "epoch [2/5], loss:0.0734\n",
      "epoch [3/5], loss:0.0912\n",
      "epoch [4/5], loss:0.0758\n",
      "epoch [5/5], loss:0.0677\n",
      "epoch [1/5], loss:0.0654\n",
      "epoch [2/5], loss:0.0153\n",
      "epoch [3/5], loss:0.1376\n",
      "epoch [4/5], loss:0.0972\n",
      "epoch [5/5], loss:0.0147\n",
      "epoch [1/5], loss:0.0314\n",
      "epoch [2/5], loss:0.0435\n",
      "epoch [3/5], loss:0.0373\n",
      "epoch [4/5], loss:0.0473\n",
      "epoch [5/5], loss:0.0075\n",
      "epoch [1/5], loss:0.0624\n",
      "epoch [2/5], loss:0.0364\n",
      "epoch [3/5], loss:0.0319\n",
      "epoch [4/5], loss:0.0368\n",
      "epoch [5/5], loss:0.0396\n",
      "epoch [1/5], loss:0.1143\n",
      "epoch [2/5], loss:0.1345\n",
      "epoch [3/5], loss:0.0644\n",
      "epoch [4/5], loss:0.0611\n",
      "epoch [5/5], loss:0.0513\n",
      "epoch [1/5], loss:0.0761\n",
      "epoch [2/5], loss:0.0798\n",
      "epoch [3/5], loss:0.1934\n",
      "epoch [4/5], loss:0.0638\n",
      "epoch [5/5], loss:0.0430\n",
      "epoch [1/5], loss:0.0269\n",
      "epoch [2/5], loss:0.0543\n",
      "epoch [3/5], loss:0.0069\n",
      "epoch [4/5], loss:0.0455\n",
      "epoch [5/5], loss:0.0404\n",
      "epoch [1/5], loss:0.0460\n",
      "epoch [2/5], loss:0.0458\n",
      "epoch [3/5], loss:0.0527\n",
      "epoch [4/5], loss:0.0402\n",
      "epoch [5/5], loss:0.0426\n",
      "epoch [1/5], loss:0.1402\n",
      "epoch [2/5], loss:0.0729\n",
      "epoch [3/5], loss:0.0511\n",
      "epoch [4/5], loss:0.0670\n",
      "epoch [5/5], loss:0.0620\n",
      "epoch [1/5], loss:0.1254\n",
      "epoch [2/5], loss:0.0704\n",
      "epoch [3/5], loss:0.0438\n",
      "epoch [4/5], loss:0.1105\n",
      "epoch [5/5], loss:0.0433\n",
      "epoch [1/5], loss:0.0659\n",
      "epoch [2/5], loss:0.0827\n",
      "epoch [3/5], loss:0.1193\n",
      "epoch [4/5], loss:0.0489\n",
      "epoch [5/5], loss:0.0077\n",
      "epoch [1/5], loss:0.0448\n",
      "epoch [2/5], loss:0.0789\n",
      "epoch [3/5], loss:0.0480\n",
      "epoch [4/5], loss:0.0520\n",
      "epoch [5/5], loss:0.0406\n",
      "epoch [1/5], loss:0.0548\n",
      "epoch [2/5], loss:0.0378\n",
      "epoch [3/5], loss:0.0081\n",
      "epoch [4/5], loss:0.0472\n",
      "epoch [5/5], loss:0.0375\n",
      "epoch [1/5], loss:0.0562\n",
      "epoch [2/5], loss:0.0884\n",
      "epoch [3/5], loss:0.0850\n",
      "epoch [4/5], loss:0.0700\n",
      "epoch [5/5], loss:0.0915\n",
      "epoch [1/5], loss:0.0668\n",
      "epoch [2/5], loss:0.0620\n",
      "epoch [3/5], loss:0.0535\n",
      "epoch [4/5], loss:0.0844\n",
      "epoch [5/5], loss:0.0754\n",
      "epoch [1/5], loss:0.0324\n",
      "epoch [2/5], loss:0.0737\n",
      "epoch [3/5], loss:0.0104\n",
      "epoch [4/5], loss:0.1163\n",
      "epoch [5/5], loss:0.0600\n",
      "epoch [1/5], loss:0.0879\n",
      "epoch [2/5], loss:0.0812\n",
      "epoch [3/5], loss:0.0860\n",
      "epoch [4/5], loss:0.0481\n",
      "epoch [5/5], loss:0.0459\n",
      "epoch [1/5], loss:0.0662\n",
      "epoch [2/5], loss:0.2277\n",
      "epoch [3/5], loss:0.0765\n",
      "epoch [4/5], loss:0.0808\n",
      "epoch [5/5], loss:0.0505\n",
      "epoch [1/5], loss:0.0580\n",
      "epoch [2/5], loss:0.0486\n",
      "epoch [3/5], loss:0.0525\n",
      "epoch [4/5], loss:0.0564\n",
      "epoch [5/5], loss:0.0522\n",
      "epoch [1/5], loss:0.0474\n",
      "epoch [2/5], loss:0.0484\n",
      "epoch [3/5], loss:0.0312\n",
      "epoch [4/5], loss:0.1502\n",
      "epoch [5/5], loss:0.0382\n",
      "epoch [1/5], loss:0.0501\n",
      "epoch [2/5], loss:0.0541\n",
      "epoch [3/5], loss:0.0663\n",
      "epoch [4/5], loss:0.0651\n",
      "epoch [5/5], loss:0.0246\n",
      "epoch [1/5], loss:0.0873\n",
      "epoch [2/5], loss:0.0383\n",
      "epoch [3/5], loss:0.0090\n",
      "epoch [4/5], loss:0.0055\n",
      "epoch [5/5], loss:0.0072\n",
      "epoch [1/5], loss:0.0596\n",
      "epoch [2/5], loss:0.0579\n",
      "epoch [3/5], loss:0.0594\n",
      "epoch [4/5], loss:0.0438\n",
      "epoch [5/5], loss:0.0890\n",
      "epoch [1/5], loss:0.1078\n",
      "epoch [2/5], loss:0.0363\n",
      "epoch [3/5], loss:0.0664\n",
      "epoch [4/5], loss:0.1024\n",
      "epoch [5/5], loss:0.0815\n",
      "epoch [1/5], loss:0.0693\n",
      "epoch [2/5], loss:0.0602\n",
      "epoch [3/5], loss:0.0614\n",
      "epoch [4/5], loss:0.0493\n",
      "epoch [5/5], loss:0.0583\n",
      "epoch [1/5], loss:0.0618\n",
      "epoch [2/5], loss:0.0912\n",
      "epoch [3/5], loss:0.0798\n",
      "epoch [4/5], loss:0.0061\n",
      "epoch [5/5], loss:0.0191\n",
      "epoch [1/5], loss:0.1948\n",
      "epoch [2/5], loss:0.1287\n",
      "epoch [3/5], loss:0.1006\n",
      "epoch [4/5], loss:0.1696\n",
      "epoch [5/5], loss:0.1128\n",
      "epoch [1/5], loss:0.1426\n",
      "epoch [2/5], loss:0.1245\n",
      "epoch [3/5], loss:0.0848\n",
      "epoch [4/5], loss:0.0890\n",
      "epoch [5/5], loss:0.0644\n",
      "epoch [1/5], loss:0.1190\n",
      "epoch [2/5], loss:0.0613\n",
      "epoch [3/5], loss:0.0753\n",
      "epoch [4/5], loss:0.0764\n",
      "epoch [5/5], loss:0.0618\n",
      "epoch [1/5], loss:0.1430\n",
      "epoch [2/5], loss:0.1214\n",
      "epoch [3/5], loss:0.1026\n",
      "epoch [4/5], loss:0.0666\n",
      "epoch [5/5], loss:0.1108\n",
      "epoch [1/5], loss:0.0858\n",
      "epoch [2/5], loss:0.0601\n",
      "epoch [3/5], loss:0.0504\n",
      "epoch [4/5], loss:0.0424\n",
      "epoch [5/5], loss:0.0479\n",
      "epoch [1/5], loss:0.1042\n",
      "epoch [2/5], loss:0.1158\n",
      "epoch [3/5], loss:0.0956\n",
      "epoch [4/5], loss:0.0510\n",
      "epoch [5/5], loss:0.0564\n",
      "epoch [1/5], loss:0.0962\n",
      "epoch [2/5], loss:0.1174\n",
      "epoch [3/5], loss:0.1394\n",
      "epoch [4/5], loss:0.0777\n",
      "epoch [5/5], loss:0.0460\n",
      "epoch [1/5], loss:0.1168\n",
      "epoch [2/5], loss:0.0616\n",
      "epoch [3/5], loss:0.0795\n",
      "epoch [4/5], loss:0.0624\n",
      "epoch [5/5], loss:0.0462\n",
      "epoch [1/5], loss:0.0637\n",
      "epoch [2/5], loss:0.0485\n",
      "epoch [3/5], loss:0.0459\n",
      "epoch [4/5], loss:0.0416\n",
      "epoch [5/5], loss:0.0453\n",
      "epoch [1/5], loss:0.0696\n",
      "epoch [2/5], loss:0.0830\n",
      "epoch [3/5], loss:0.0574\n",
      "epoch [4/5], loss:0.0512\n",
      "epoch [5/5], loss:0.0458\n",
      "epoch [1/5], loss:0.1890\n",
      "epoch [2/5], loss:0.0975\n",
      "epoch [3/5], loss:0.0620\n",
      "epoch [4/5], loss:0.0696\n",
      "epoch [5/5], loss:0.0500\n",
      "epoch [1/5], loss:0.0653\n",
      "epoch [2/5], loss:0.0530\n",
      "epoch [3/5], loss:0.0576\n",
      "epoch [4/5], loss:0.0524\n",
      "epoch [5/5], loss:0.0396\n",
      "epoch [1/5], loss:0.0905\n",
      "epoch [2/5], loss:0.0492\n",
      "epoch [3/5], loss:0.0514\n",
      "epoch [4/5], loss:0.0299\n",
      "epoch [5/5], loss:0.0571\n",
      "epoch [1/5], loss:0.0633\n",
      "epoch [2/5], loss:0.0383\n",
      "epoch [3/5], loss:0.0693\n",
      "epoch [4/5], loss:0.0312\n",
      "epoch [5/5], loss:0.0573\n",
      "epoch [1/5], loss:0.0762\n",
      "epoch [2/5], loss:0.0569\n",
      "epoch [3/5], loss:0.0322\n",
      "epoch [4/5], loss:0.0547\n",
      "epoch [5/5], loss:0.0362\n",
      "epoch [1/5], loss:0.2890\n",
      "epoch [2/5], loss:0.3277\n",
      "epoch [3/5], loss:0.2096\n",
      "epoch [4/5], loss:0.1215\n",
      "epoch [5/5], loss:0.1346\n",
      "epoch [1/5], loss:0.2899\n",
      "epoch [2/5], loss:0.1994\n",
      "epoch [3/5], loss:0.1132\n",
      "epoch [4/5], loss:0.1234\n",
      "epoch [5/5], loss:0.0809\n",
      "epoch [1/5], loss:0.1949\n",
      "epoch [2/5], loss:0.0850\n",
      "epoch [3/5], loss:0.1017\n",
      "epoch [4/5], loss:0.0843\n",
      "epoch [5/5], loss:0.1046\n",
      "epoch [1/5], loss:0.1297\n",
      "epoch [2/5], loss:0.1031\n",
      "epoch [3/5], loss:0.1105\n",
      "epoch [4/5], loss:0.0694\n",
      "epoch [5/5], loss:0.1163\n",
      "epoch [1/5], loss:0.1598\n",
      "epoch [2/5], loss:0.0935\n",
      "epoch [3/5], loss:0.0846\n",
      "epoch [4/5], loss:0.0805\n",
      "epoch [5/5], loss:0.0657\n",
      "epoch [1/5], loss:0.2103\n",
      "epoch [2/5], loss:0.1000\n",
      "epoch [3/5], loss:0.1166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [4/5], loss:0.0782\n",
      "epoch [5/5], loss:0.0639\n",
      "epoch [1/5], loss:0.0845\n",
      "epoch [2/5], loss:0.1248\n",
      "epoch [3/5], loss:0.0909\n",
      "epoch [4/5], loss:0.0562\n",
      "epoch [5/5], loss:0.0764\n",
      "epoch [1/5], loss:0.0903\n",
      "epoch [2/5], loss:0.0586\n",
      "epoch [3/5], loss:0.0764\n",
      "epoch [4/5], loss:0.0406\n",
      "epoch [5/5], loss:0.0419\n",
      "epoch [1/5], loss:0.1077\n",
      "epoch [2/5], loss:0.0997\n",
      "epoch [3/5], loss:0.0717\n",
      "epoch [4/5], loss:0.0472\n",
      "epoch [5/5], loss:0.0370\n",
      "epoch [1/5], loss:0.1295\n",
      "epoch [2/5], loss:0.0980\n",
      "epoch [3/5], loss:0.0679\n",
      "epoch [4/5], loss:0.0491\n",
      "epoch [5/5], loss:0.0576\n"
     ]
    }
   ],
   "source": [
    "trainSpectra = []\n",
    "i = 0\n",
    "for key in TrainingKeys:\n",
    "    trainSpectra.append(torch.empty_like(TrainingSamples[key].spectra))\n",
    "    for j in range(TrainingSamples[key].spectra.size()[0]):\n",
    "        trainSpectra[i][j] = TrainingSamples[key].spectrum + TrainingSamples[key].spectra[j]\n",
    "    i += 1\n",
    "\n",
    "vb = TempVB()\n",
    "for i in range(70):\n",
    "    vb.crfAi.train(trainSpectra[i], epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "directed-nirvana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "istft(CPUComplexFloatType[1921, 48], n_fft=3840, hop_length=1280, win_length=3840, window=torch.FloatTensor{[3840]}, center=1, normalized=0, onesided=1, length=2458880, return_complex=0) window overlap add min: 4.29878e-13",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-c47a80eae756>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0moffsets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m13\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0msequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVocalSequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mborders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphonemes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-1284a6cfa30b>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, start, end, vb, borders, phonemes, offsets)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequiresUpdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphonemes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequiresUpdate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-1284a6cfa30b>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m                     \u001b[0mwindowEnd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msegment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msegment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                     \u001b[0mnextSpectrum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msegments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetSpectrum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m                     \u001b[0mnextVoicedExcitation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msegments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetVoicedExcitation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msegments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart3\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msegments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampleRate\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mspectrum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwindowStart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mwindowEnd\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msegment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetSpectrum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-a00f4811104f>\u001b[0m in \u001b[0;36mgetVoicedExcitation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mwindowEnd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend3\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpremul\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;31m#voicedExcitation = self.vb.phonemeDict[self.phonemeKey].voicedExcitation[windowStart:windowEnd]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[0mvoicedExcitation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloopSamplerVoicedExcitation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphonemeDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphonemeKey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvoicedExcitation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindowEnd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepetititionSpacing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnativePitch\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m75.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwindowStart\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampleRate\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mwindowEnd\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampleRate\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         transform = torchaudio.transforms.Resample(orig_freq = nativePitch,\n\u001b[0;32m    143\u001b[0m                                                    \u001b[0mnew_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpitch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-a00f4811104f>\u001b[0m in \u001b[0;36mloopSamplerVoicedExcitation\u001b[1;34m(self, inputTensor, targetSize, repetititionSpacing, pitch)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mworkingTensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mworkingTensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mistft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworkingTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtripleBatchSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhop_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwin_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtripleBatchSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monesided\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mBatchSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mworkingTensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mrepetititionSpacing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepetititionSpacing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0moutputTensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0minputTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mworkingTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36mistft\u001b[1;34m(input, n_fft, hop_length, win_length, window, center, normalized, onesided, length, return_complex)\u001b[0m\n\u001b[0;32m    652\u001b[0m             length=length, return_complex=return_complex)\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 654\u001b[1;33m     return _VF.istft(input, n_fft, hop_length, win_length, window, center,  # type: ignore\n\u001b[0m\u001b[0;32m    655\u001b[0m                      normalized, onesided, length, return_complex)\n\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: istft(CPUComplexFloatType[1921, 48], n_fft=3840, hop_length=1280, win_length=3840, window=torch.FloatTensor{[3840]}, center=1, normalized=0, onesided=1, length=2458880, return_complex=0) window overlap add min: 4.29878e-13"
     ]
    }
   ],
   "source": [
    "borders = [0, 1, 2,\n",
    "           35, 36, 37,\n",
    "           40, 51, 52,\n",
    "           75, 76, 79,\n",
    "           82, 83, 86,\n",
    "           128, 129, 130\n",
    "          ]\n",
    "phonemes = [\"A\", \"N\", \"A\", \"T\", \"A\"]\n",
    "#offsets = [0, 5, 1, 1, 1]\n",
    "offsets = [0, 20, 20, 0, 13]\n",
    "\n",
    "sequence = VocalSequence(0, 400, vb, borders, phonemes, offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sequence.spectrum[31].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-notification",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sequence.synth.returnSignal.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-crime",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(vb.crfAi.state_dict(), \"CrossfadeWeights.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"model = TheModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
